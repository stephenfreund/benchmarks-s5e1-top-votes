{"metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}, "language_info": {"name": "python", "version": "3.10.12", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}, "kaggle": {"accelerator": "none", "dataSources": [{"sourceId": 85723, "databundleVersionId": 10652996, "sourceType": "competition"}], "dockerImageVersionId": 30839, "isInternetEnabled": true, "language": "python", "sourceType": "notebook", "isGpuEnabled": false}}, "nbformat_minor": 4, "nbformat": 4, "cells": [{"cell_type": "markdown", "source": "This solution is largely based on [Konstantin Dmitriev's starter model](https://www.kaggle.com/code/kdmitrie/pg501-model-2-decomposition-country-doy-factor?scriptVersionId=220159752). I recommend reading through his notebook first as it provides wonderful explanations for many of the factors included below.", "metadata": {}}, {"cell_type": "code", "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport datetime as dt\nimport holidays\nfrom sklearn.linear_model import Ridge", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-02-01T09:05:24.621084Z", "iopub.execute_input": "2025-02-01T09:05:24.621516Z", "iopub.status.idle": "2025-02-01T09:05:24.627155Z", "shell.execute_reply.started": "2025-02-01T09:05:24.62148Z", "shell.execute_reply": "2025-02-01T09:05:24.625697Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "from scipy.optimize import minimize\nfrom sklearn.linear_model import LinearRegression\n\ndef fit_mape_linear_model(X, y):\n    # Ensure X is a 2D array\n    X = np.asarray(X)\n    y = np.asarray(y).squeeze()\n    \n    # Add bias term to X\n    X = np.column_stack((np.ones(X.shape[0]), X))\n    \n    # Define the MAPE loss function\n    def mape_loss(beta, X, y):\n        y_pred = X @ beta\n        return np.mean(np.abs((y - y_pred) / y)) * 100\n    \n    # Initial guess for parameters\n    init_params = np.zeros(X.shape[1])\n    \n    # Minimize the MAPE loss\n    result = minimize(mape_loss, init_params, args=(X, y), method='L-BFGS-B')\n    \n    # Extract optimized parameters\n    beta_opt = result.x\n    \n    # Create and return an sklearn LinearRegression model\n    model = LinearRegression()\n    model.coef_ = beta_opt[1:]\n    model.intercept_ = beta_opt[0]\n    return model", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-02-01T09:05:24.657383Z", "iopub.execute_input": "2025-02-01T09:05:24.657909Z", "iopub.status.idle": "2025-02-01T09:05:24.665858Z", "shell.execute_reply.started": "2025-02-01T09:05:24.657873Z", "shell.execute_reply": "2025-02-01T09:05:24.664601Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "train = pd.read_csv('./input/playground-series-s5e1/train.csv', parse_dates=['date'])\ntest = pd.read_csv('./input/playground-series-s5e1/test.csv', parse_dates=['date'])\ntrain = train.dropna().reset_index(drop=True)\ndf = pd.concat([train, test], sort=False).reset_index(drop=True)", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-02-01T09:05:24.667245Z", "iopub.execute_input": "2025-02-01T09:05:24.667603Z", "iopub.status.idle": "2025-02-01T09:05:25.105018Z", "shell.execute_reply.started": "2025-02-01T09:05:24.667575Z", "shell.execute_reply": "2025-02-01T09:05:25.103884Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "##### Useful Columns", "metadata": {}}, {"cell_type": "code", "source": "df['year'] = df['date'].dt.year\ndf['n_day'] = (df['date'] - df['date'].min()).dt.days\ndf['weekday'] = df['date'].dt.weekday\ndf['day_of_year'] = df['date'].dt.dayofyear\n\n# Generate Wave Columns\nwave_columns = []\n# subtract leap dates\ndf.loc[df['date'] > dt.datetime(2012, 2, 29), 'n_day'] -= 1\ndf.loc[df['date'] > dt.datetime(2016, 2, 29), 'n_day'] -= 1\n\nfor i in range(1, 10):\n\n    df[f'wave_sin{i}'] = np.sin(np.pi * i * df['n_day'] / 365)\n    df[f'wave_cos{i}'] = np.cos(np.pi * i * df['n_day'] / 365)\n    wave_columns.append(f'wave_sin{i}')\n    wave_columns.append(f'wave_cos{i}')\n\n# Near Holiday\ndf['near_holiday'] = 0\nfor country in df['country'].unique():\n    days = [day for day in holidays.CountryHoliday(country, years=df['year'].unique())] \n    for day in days:\n        df.loc[(df.country == country) & (df['date'].dt.date < day + dt.timedelta(days=10)) & (df['date'].dt.date > day - dt.timedelta(days=10)), 'near_holiday'] = 1", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-02-01T09:05:25.107175Z", "iopub.execute_input": "2025-02-01T09:05:25.107543Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "### Decomposition", "metadata": {}}, {"cell_type": "markdown", "source": "##### GDP Factor", "metadata": {}}, {"cell_type": "code", "source": "import requests\ndef get_gdp_per_capita(country,year):\n    alpha3 = {'Canada': 'CAN', 'Finland': 'FIN',\n              'Italy': 'ITA', 'Kenya': 'KEN', \n              'Norway': 'NOR', 'Singapore': 'SGP'}\n    url=\"https://api.worldbank.org/v2/country/{0}/indicator/NY.GDP.PCAP.CD?date={1}&format=json\".format(alpha3[country],year)\n    response = requests.get(url).json()\n    return response[1][0]['value']\n\ngdp = np.array([[get_gdp_per_capita(country, year) for year in df['year'].unique()] for country in df['country'].unique()])\ngdp_df = pd.DataFrame(gdp, columns=df['year'].unique(), index=df['country'].unique())\nfor year in df['year'].unique():\n    for country in df['country'].unique():\n        df.loc[(df['year'] == year) & (df['country'] == country), 'gdp'] = gdp_df.loc[country, year]", "metadata": {"trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "From broccoli beef's [discussion post comment](https://www.kaggle.com/competitions/playground-series-s5e1/discussion/555500#3091632). Using the following least MAPE linear fit improves predictions for kenya.", "metadata": {}}, {"cell_type": "code", "source": "df['gdp_factor'] =  (-17643.346899+85.42355636*df['gdp']) / 365", "metadata": {"trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "fig, axs = plt.subplots(1, 2, figsize=(20, 5))\ngrouped_data = df.groupby(['date', 'year', 'country'])['num_sold'].sum().reset_index()\ntotal_per_day = df.groupby('year')['num_sold'].sum().reset_index()\ngrouped_data = grouped_data.merge(total_per_day, on=['year'], suffixes=['', '_total']).reset_index()\ngrouped_data = grouped_data.merge(df[['date', 'country', 'gdp_factor']], on=['date', 'country'])\n\nfor country in df['country'].unique():\n    country_data = grouped_data[((grouped_data['country'] == country) & (grouped_data['date'] < dt.datetime(2017, 1, 1)))]\n    axs[0].plot(country_data['date'], country_data['num_sold'] / country_data['num_sold_total'], '-', label=country)\n    axs[0].plot(country_data['date'], country_data['gdp_factor'] / country_data['num_sold_total'], 'b--')\naxs[0].set_title('Amt Sold Per Country')\naxs[0].legend()\n\nfor country in df['country'].unique():\n    country_data = grouped_data[((grouped_data['country'] == country) & (grouped_data['date'] < dt.datetime(2017, 1, 1)))]\n    axs[1].plot(country_data['date'], country_data['num_sold'] / country_data['gdp_factor'], '-', label=country)\naxs[1].set_title('Amt Sold Per Country normalized by GDP factor')\naxs[1].legend()\n\ndf['ratio'] = df['gdp_factor']\ndf['total'] = df['num_sold'] / df['ratio']\n\nplt.tight_layout()\nplt.show()", "metadata": {"trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "##### Store Factor", "metadata": {}}, {"cell_type": "code", "source": "df_no_can_ken = df[~df['country'].isin(['Canada', 'Kenya'])]\n\nfig, axs = plt.subplots(1, 2, figsize=(10, 6))\nstore_data = df_no_can_ken.groupby(['date', 'store'])['num_sold'].sum().reset_index()\ntotal_per_day = df_no_can_ken.groupby('date')['num_sold'].sum().reset_index()\nstore_data = store_data.merge(total_per_day, on=['date'], suffixes=['', '_total'])\n\n# Calculate store factor\nstore_data['store_factor'] = store_data['num_sold'] / store_data['num_sold_total']\nstore_df = store_data.groupby('store')['store_factor'].mean().reset_index()\nstore_data.drop('store_factor', axis=1, inplace=True)\nstore_data = store_data.merge(store_df, on=['store'])\nprint(f\"Store factor sum is {store_df['store_factor'].sum()}\")\n\n# Merge store factor into df\ndf = df.drop('store_factor', axis=1, errors='ignore')\ndf = df.merge(store_df, on=['store'])\ndf['ratio'] = df['store_factor']\n\nfor store in df['store'].unique():\n    data = store_data[store_data['store'] == store]\n    axs[0].plot(data['date'], data['num_sold'] / data['num_sold_total'], '.', label=f'Store {store}')\naxs[0].set_title('Relative Amt Sold Per Store')\naxs[0].legend()\n\n# Normalize by current ratio\nfor store in df['store'].unique():\n    data = store_data[store_data['store'] == store]\n    axs[1].plot(data['date'], data['num_sold'] /  data['num_sold_total'] /  data['store_factor'], '.', label=f'Store {store}')\naxs[1].set_title('Rel Amt Sold Per Store normalized by store factor') \naxs[1].legend()\n\nplt.tight_layout()\nplt.show()", "metadata": {"trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "##### Product Factor", "metadata": {}}, {"cell_type": "code", "source": "df['ratio'] = df['store_factor'] * df['gdp_factor']\ndf['total'] = df['num_sold'] / df['ratio']\n\ndf_no_can_ken = df[~df['country'].isin(['Canada', 'Kenya'])]\ntotal_per_day = df_no_can_ken.groupby('date')['total'].sum().reset_index()\ndf_no_can_ken = df_no_can_ken.merge(total_per_day, on=['date'], suffixes=['', '_per_day'])\ndf_no_can_ken['total_perc_per_day'] = df_no_can_ken['total'] / df_no_can_ken['total_per_day'] \nfig, axs = plt.subplots(1, 2, figsize=(20, 5))\n\n# fit wave columns to each product\ndf['product_factor'] = None\nfor product in df['product'].unique():\n\n    df_product = df_no_can_ken[((df_no_can_ken['product'] == product) & (df_no_can_ken['date'] < dt.datetime(2017, 1, 1)))].groupby('date')\n    X = df_product[wave_columns].mean()\n    y = df_product['total_perc_per_day'].sum()\n    \n    model = fit_mape_linear_model(X, y)\n    df.loc[df['product'] == product, 'product_factor'] = model.predict(df[df['product'] == product][wave_columns])\n\n    axs[0].plot(df_product['date'].unique().index, y, '-', label=product)\n    axs[0].plot(df_product['date'].unique().index, model.predict(X), 'b--')\naxs[0].set_title('Amt Sold Per Product')\naxs[0].legend()\n\n# Visualize the result\ndf_no_can_ken = df[~df['country'].isin(['Canada', 'Kenya'])]\ntotal_per_day = df_no_can_ken.groupby('date')['total'].sum().reset_index()\ndf_no_can_ken = df_no_can_ken.merge(total_per_day, on=['date'], suffixes=['', '_per_day'])\ndf_no_can_ken['total_perc_per_day'] = df_no_can_ken['total'] / df_no_can_ken['total_per_day'] \nfor product in df['product'].unique():\n    df_product = df_no_can_ken[((df_no_can_ken['product'] == product) & (df_no_can_ken['date'] < dt.datetime(2017, 1, 1)))].groupby('date')\n    y = df_product['total_perc_per_day'].sum()\n    product_factor = df_product['product_factor'].mean()\n    axs[1].plot(df_product['date'].unique().index, y / product_factor, '-', label=product)\naxs[1].set_title('Amt Sold Per Product normalized by Product factor')\naxs[1].legend()\n\ndf['ratio'] = df['store_factor'] * df['gdp_factor'] * df['product_factor']\ndf['total'] = df['num_sold'] / df['ratio']", "metadata": {"trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "##### Day of Week Factor", "metadata": {}}, {"cell_type": "code", "source": "df['ratio'] = df['store_factor'] * df['gdp_factor'] * df['product_factor']\ndf['total'] = df['num_sold'] / df['ratio']\n\ndf_no_can_ken_hol = df[~((df['country'].isin(['Canada', 'Kenya'])) | (df['near_holiday']))]\n\nmean_per_weekday = df_no_can_ken_hol.groupby('weekday')['total'].mean().reset_index()\nmean_mon_thur = mean_per_weekday[mean_per_weekday['weekday'] < 4]['total'].mean()\nratio_per_weekday = mean_per_weekday.copy()\nratio_per_weekday['day_of_week_factor'] = ratio_per_weekday['total'] / mean_mon_thur\nratio_per_weekday = ratio_per_weekday.drop('total', axis=1)\n\ndf = df.drop('day_of_week_factor', axis=1, errors='ignore')\ndf = df.merge(ratio_per_weekday, on='weekday')\n\ngrouped_data = df_no_can_ken_hol.groupby(['date'])['total'].mean().reset_index()\ngrouped_data = grouped_data[grouped_data['date'] < dt.datetime(2017, 1, 1)]\n\nfig, axs = plt.subplots(1, 2, figsize=(20, 5))\n\naxs[0].plot(grouped_data['date'], grouped_data['total'], '-')\naxs[0].set_title('Mean Total Per Day')\n\ndf['ratio'] = df['store_factor'] * df['gdp_factor'] * df['product_factor'] * df['day_of_week_factor']\ndf['total'] = df['num_sold'] / df['ratio']\n\n# Visualize the result\ndf_no_can_ken_hol = df[~((df['country'].isin(['Canada', 'Kenya'])) | (df['near_holiday']))]\n\ngrouped_data = df_no_can_ken_hol.groupby(['date'])['total'].mean().reset_index()\ngrouped_data = grouped_data[grouped_data['date'] < dt.datetime(2017, 1, 1)]\n\naxs[1].plot(grouped_data['date'], grouped_data['total'], '-')\naxs[1].set_title('Mean Total Per Day normalized by weekday factor')\n", "metadata": {"trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "##### Sincos factor", "metadata": {}}, {"cell_type": "code", "source": "df['ratio'] = df['store_factor'] * df['gdp_factor'] * df['product_factor'] * df['day_of_week_factor']\ndf['total'] = df['num_sold'] / df['ratio']\n\ndf_no_can_ken_hol = df[~((df['country'].isin(['Canada', 'Kenya'])) | (df['near_holiday']))]\ngrouped_data = df_no_can_ken_hol[df_no_can_ken_hol['date'] < dt.datetime(2017, 1, 1)].groupby(['date'])\nX = grouped_data[wave_columns].mean()\ny = grouped_data['total'].mean()\n\nmodel = fit_mape_linear_model(X, y)\n\ndf['sincos_factor'] = model.predict(df[wave_columns])\n\nfig, axs = plt.subplots(1, 2, figsize=(20, 5))\n\naxs[0].plot(grouped_data['date'].unique().index, y, '-')\naxs[0].set_title('Mean Total Per Day')\naxs[0].plot(grouped_data['date'].unique().index, model.predict(X), 'r--')\n\n\ndf['ratio'] = df['store_factor'] * df['gdp_factor'] * df['product_factor'] * df['day_of_week_factor'] * df['sincos_factor']\ndf['total'] = df['num_sold'] / df['ratio']\n\n# Visualize the result\ndf_no_can_ken_hol = df[~((df['country'].isin(['Canada', 'Kenya'])) | (df['near_holiday']))]\ngrouped_data = df_no_can_ken_hol.groupby(['date'])['total'].mean().reset_index()\n\naxs[1].plot(grouped_data['date'], grouped_data['total'], '-')\naxs[1].set_title('Mean Total Per Day normalized by sincos factor')\n\n", "metadata": {"trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "##### Trend Factor", "metadata": {}}, {"cell_type": "markdown", "source": "This factor is only meant to make calculating the subsequent factors easier.\n\n\nI attempted to include this factor in the final product for one of my submissions. Doing this while also removing the 1.06 factor (see const_factor under the \"Prediction and Submission\" section) resulted in my best public LB score (0.04422). However, as expected, this was very overfit (private LB  ~0.054).", "metadata": {}}, {"cell_type": "code", "source": "df['ratio'] = df['store_factor'] * df['gdp_factor'] * df['product_factor'] * df['day_of_week_factor'] * df['sincos_factor']\ndf['total'] = df['num_sold'] / df['ratio']\n\ngrouped_data = df.groupby(['date', 'n_day'])['total'].mean().reset_index()\n\n\n\nfig, axs = plt.subplots(1, 2, figsize=(20, 5))\naxs[0].plot(grouped_data['date'], grouped_data['total'], '-')\n\ntrain = grouped_data[(grouped_data['date'] < dt.datetime(2017, 1, 1)) & (grouped_data['date'] > dt.datetime(2012, 12, 31))]\nX = train['n_day'].to_numpy().reshape(-1, 1)\ny = train['total']\n\nmodel = Ridge(alpha=0.1)\nmodel.fit(X, y)\n\ndf['trend_factor'] = model.predict(df['n_day'].to_numpy().reshape(-1, 1))\ndf.loc[df['date'] < dt.datetime(2013, 1, 1), 'trend_factor'] = 1\naxs[0].plot(grouped_data['date'], model.predict(grouped_data['n_day'].to_numpy().reshape(-1, 1)), 'r--')\naxs[0].set_title('Mean Total Over Time Uncorrected')\n\ndf['ratio'] = df['store_factor'] * df['gdp_factor'] * df['product_factor'] * df['day_of_week_factor'] * df['sincos_factor'] * df['trend_factor']\ndf['total'] = df['num_sold'] / df['ratio']\n\n# Visualize the result\ngrouped_data = df.groupby(['date', 'n_day'])['total'].mean().reset_index()\naxs[1].plot(grouped_data['date'], grouped_data['total'], '-')\naxs[1].set_title('Mean Total Over Time Corrected by Trend Factor')\n", "metadata": {"trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "##### Country Factor", "metadata": {}}, {"cell_type": "code", "source": "\ndf['ratio'] = df['store_factor'] * df['gdp_factor'] * df['product_factor'] * df['day_of_week_factor'] * df['sincos_factor'] * df['trend_factor']\ndf['total'] = df['num_sold'] / df['ratio']\n\ngrouped_data = df[df['product'] == \"Kaggle\"].groupby(['date', 'country'])['total'].sum().reset_index()\ntotal_per_day = df[df['product'] == \"Kaggle\"].groupby('date')['total'].sum().reset_index()\ngrouped_data = grouped_data.merge(total_per_day, on=['date'], suffixes=['', '_per_day'])\ngrouped_data = grouped_data[grouped_data['date'] < dt.datetime(2017, 1, 1)]\n\ndf['ratio'] = df['store_factor'] * df['gdp_factor'] * df['product_factor'] * df['day_of_week_factor'] * df['sincos_factor'] * df['trend_factor']\ndf['total'] = df['num_sold'] / df['ratio']\n\nfig, axs = plt.subplots(1, 2, figsize=(20, 5))\n\nfor country in df['country'].unique():\n    country_data = grouped_data[grouped_data['country'] == country]\n    axs[0].plot(country_data['date'], country_data['total'] / country_data['total_per_day'], '-', label=country)\naxs[0].set_title('Mean Total Per Day Per Country')\naxs[0].legend()\n\ncountry_factor = df[(df['product'] == 'Kaggle')].groupby('country').total.sum().rename('country_factor')\ncountry_factor = country_factor / country_factor.median()\ndf = df.drop('country_factor', axis=1, errors='ignore')\ndf = df.merge(country_factor, on='country')\ndf['ratio'] = df['store_factor'] * df['gdp_factor'] * df['product_factor'] * df['day_of_week_factor'] * df['sincos_factor'] * df['country_factor']\ndf['total'] = df['num_sold'] / df['ratio']\n\n# Visualize the result\ngrouped_data = df[df['product'] == \"Kaggle\"].groupby(['date', 'country'])['total'].sum().reset_index()\ntotal_per_day = df[df['product'] == \"Kaggle\"].groupby('date')['total'].sum().reset_index()\ngrouped_data = grouped_data.merge(total_per_day, on=['date'], suffixes=['', '_per_day'])\ngrouped_data = grouped_data[grouped_data['date'] < dt.datetime(2017, 1, 1)]\n\nfor country in df['country'].unique():\n    country_data = grouped_data[grouped_data['country'] == country]\n    axs[1].plot(country_data['date'], country_data['total'] / country_data['total_per_day'], '-', label=country)\naxs[1].set_title('Mean Total Per Day Per Country normalized by country factor')\naxs[1].legend()", "metadata": {"trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "##### Holiday factor", "metadata": {}}, {"cell_type": "markdown", "source": "My handling of the following two factors (holiday factor and New Years factor) is inspired by JZ's [first place solution](https://www.kaggle.com/code/ivyzang/1st-place-solution-less-is-more/notebook) to a previous competition.", "metadata": {}}, {"cell_type": "code", "source": "# Define the years and countries\nyears = [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]\ncountries = df['country'].unique()\n# Initialize an empty list to hold DataFrames\ndfs = []\n# Generate holidays for each country and year\nfor year in years:\n    for country in countries:\n        for date, holiday_name in sorted(holidays.CountryHoliday(country, years=year).items()):\n\n            df_0 = pd.DataFrame({\"date\": [date], \"country\": [\n                country]})\n            dfs.append(df_0)\n\n# Concatenate all the DataFrames\ndf_holidays = pd.concat(dfs, ignore_index=True)\n# Convert 'date' column to datetime\ndf_holidays['date'] = pd.to_datetime(df_holidays['date'])\ndf_holidays['tmp'] = 1", "metadata": {"trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "for column in df.columns:\n    if 'holiday_' in column:\n        df = df.drop(column, axis=1)", "metadata": {"trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "# holidays\nholidays_columns = []\nfor i in range(0, 10):\n    column = 'holiday_{}'.format(i)\n    shifted = df_holidays.rename(columns={'tmp': column})\n    shifted['date'] = shifted['date'] + dt.timedelta(days=i)\n    df = pd.merge(df, shifted, on=['country', 'date'], how='left')\n    df[column].fillna(0, inplace=True)\n    df[column] = df[column]\n    holidays_columns.append(column)", "metadata": {"trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "fig, axs = plt.subplots(1, 2, figsize=(20, 5))\n\ndf['ratio'] = df['store_factor'] * df['gdp_factor'] * df['product_factor'] * df['day_of_week_factor'] * df['sincos_factor'] * df['country_factor'] * df['trend_factor']\ndf['total'] = df['num_sold'] / df['ratio']\n\naxs[0].plot(df['date'], df['total'], '-')\naxs[0].set_title('Total Over Time')\n\n# fit linear model to total using holidays\n\ntrain = df[(df['date'] > dt.datetime(2012, 12, 31)) & (df['date'] < dt.datetime(2017, 1, 1))]\nX = train[holidays_columns]\ny = train['total']\nmodel = fit_mape_linear_model(X, y)\n\ndf['holiday_factor'] = model.predict(df[holidays_columns])\n\ndf['ratio'] = df['store_factor'] * df['gdp_factor'] * df['product_factor'] * df['day_of_week_factor'] * df['sincos_factor'] * df['country_factor'] * df['holiday_factor'] * df['trend_factor']\ndf['total'] = df['num_sold'] / df['ratio']\n\naxs[1].plot(df['date'], df['total'], '-')\naxs[1].set_title('Total Over Time normalized by holiday factor')", "metadata": {"trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "##### New Years Factor", "metadata": {}}, {"cell_type": "code", "source": "new_years_columns = []\nfor day in range(25, 32):\n    column = 'day_12_{}'.format(day)\n    df[column] = ((df['date'].dt.month == 12) & (df['date'].dt.day == day)).astype(float)\n    new_years_columns.append(column)\nfor day in range(1, 11):\n    column = 'day_1_{}'.format(day)\n    df[column] = ((df['date'].dt.month == 1) & (df['date'].dt.day  == day)).astype(float)\n    new_years_columns.append(column)", "metadata": {"trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "fig, axs = plt.subplots(1, 2, figsize=(20, 5))\n\ndf['ratio'] = df['store_factor'] * df['gdp_factor'] * df['product_factor'] * df['day_of_week_factor'] * df['sincos_factor'] * df['country_factor'] * df['holiday_factor'] * df['trend_factor']\ndf['total'] = df['num_sold'] / df['ratio']\n\naxs[0].plot(df['date'], df['total'], '-')\naxs[0].set_title('Total Over Time')\n\ntrain = df[(df['date'] > dt.datetime(2012, 12, 31)) & (df['date'] < dt.datetime(2017, 1, 1))]\nX = train[new_years_columns]\ny = train['total']\nmodel = fit_mape_linear_model(X, y)\n\ndf['new_years_factor'] = model.predict(df[new_years_columns])\n\ndf['ratio'] = df['store_factor'] * df['gdp_factor'] * df['product_factor'] * df['day_of_week_factor'] * df['sincos_factor'] * df['country_factor'] * df['holiday_factor'] * df['new_years_factor'] * df['trend_factor']\ndf['total'] = df['num_sold'] / df['ratio']\n\naxs[1].plot(df['date'], df['total'], '-')\naxs[1].set_title('Total Over Time normalized by new years factor')", "metadata": {"trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "### Prediction and Submission", "metadata": {}}, {"cell_type": "code", "source": "df['ratio'] = df['country_factor'] * df['store_factor'] * df['gdp_factor'] * df['product_factor'] * df['day_of_week_factor'] * df['sincos_factor'] * df['holiday_factor'] * df['new_years_factor']\ndf['total'] = df['num_sold'] / df['ratio']\n\n# Multiplying the predictions by 1.06 seems to improve the public LB score.\n# I'm not entirely sure why, but I suspect it has to do with the fact that the model is off by ~6% by 2017 (as shown in the right plot of the sincos section above).\nconst_factor = df['total'].median() * 1.06\n\ndf['prediction'] = df['ratio'] * const_factor \n\nfig, ax = plt.subplots(1, 1, figsize=(20, 5))\nax.plot(df['date'], df['total'])", "metadata": {"trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "from sklearn.metrics import mean_absolute_percentage_error\n\nmape_train = mean_absolute_percentage_error(df[(df['date'] < dt.datetime(2017, 1, 1)) & (~pd.isna(df.num_sold))].num_sold, df[(df['date'] < dt.datetime(2017, 1, 1)) & (~pd.isna(df.num_sold))].prediction)\n\nprint(f'{mape_train=}')", "metadata": {"trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "df['prediction'] = np.round(df['prediction']).astype(float).astype(int)", "metadata": {"trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "submission = df[df['date'] >= dt.datetime(2017, 1, 1)][['id', 'prediction']].rename(columns={'prediction': 'num_sold'})\n\n# timestampt submission filename\nsubmission_filename = dt.datetime.now().strftime('%Y_%m_%d_%H_%M_%S') + '_submission.csv'\n\nsubmission.to_csv(f\"{submission_filename}\", index=False)\n", "metadata": {"trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "", "metadata": {"trusted": true}, "outputs": [], "execution_count": null}]}