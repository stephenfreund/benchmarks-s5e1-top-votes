{"metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "language_info": {"name": "python", "version": "3.10.13", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}, "kaggle": {"accelerator": "none", "dataSources": [{"sourceId": 85723, "databundleVersionId": 10652996, "sourceType": "competition"}], "dockerImageVersionId": 30746, "isInternetEnabled": true, "language": "python", "sourceType": "notebook", "isGpuEnabled": false}}, "nbformat_minor": 4, "nbformat": 4, "cells": [{"cell_type": "markdown", "source": "# Kaggle Sticker Sales Forecasting Competition\n## Playground Series S5E1 - January 2025\n\n### Overview\nThis notebook presents a solution for the Kaggle Playground Series competition on forecasting sticker sales. The goal is to predict sticker sales across different countries using various store and time-based features.\n\n### Import Libraries", "metadata": {}}, {"cell_type": "code", "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('./input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session", "metadata": {"trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_absolute_percentage_error\nimport lightgbm as lgb\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set styling for visualizations\nplt.style.use('seaborn')\nsns.set_palette(\"husl\")", "metadata": {"_uuid": "051d70d956493feee0c6d64651c6a088724dca2a", "_execution_state": "idle", "trusted": true, "execution": {"iopub.status.busy": "2025-01-01T06:13:53.070185Z", "iopub.execute_input": "2025-01-01T06:13:53.070569Z", "iopub.status.idle": "2025-01-01T06:13:55.297703Z", "shell.execute_reply.started": "2025-01-01T06:13:53.07054Z", "shell.execute_reply": "2025-01-01T06:13:55.296619Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "### Load Data", "metadata": {}}, {"cell_type": "code", "source": "# Read the training and test datasets\ntrain = pd.read_csv('./input/playground-series-s5e1/train.csv')\ntest = pd.read_csv('./input/playground-series-s5e1/test.csv')\n\nprint(\"Training set shape:\", train.shape)\nprint(\"Test set shape:\", test.shape)", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-01T06:13:56.756566Z", "iopub.execute_input": "2025-01-01T06:13:56.757213Z", "iopub.status.idle": "2025-01-01T06:13:57.228814Z", "shell.execute_reply.started": "2025-01-01T06:13:56.757183Z", "shell.execute_reply": "2025-01-01T06:13:57.22774Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "### Exploratory Data Analysis", "metadata": {}}, {"cell_type": "code", "source": "# Display basic information about the training data\nprint(\"\\nTraining data info:\")\nprint(train.info())\n\n# Show first few rows\nprint(\"\\nFirst few rows of training data:\")\ndisplay(train.head())\n\n# Check for missing values\nprint(\"\\nMissing values in training data:\")\nprint(train.isnull().sum())\n\n# Basic statistics of numerical columns\nprint(\"\\nDescriptive statistics:\")\ndisplay(train.describe())", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-01T06:13:58.729119Z", "iopub.execute_input": "2025-01-01T06:13:58.729501Z", "iopub.status.idle": "2025-01-01T06:13:58.903311Z", "shell.execute_reply.started": "2025-01-01T06:13:58.72947Z", "shell.execute_reply": "2025-01-01T06:13:58.902228Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "### Data Visualization", "metadata": {}}, {"cell_type": "code", "source": "# Create a figure with multiple subplots\nplt.figure(figsize=(15, 10))\n\n# Plot 1: Average sales by day of week\nplt.subplot(2, 2, 1)\ntrain['date'] = pd.to_datetime(train['date'])\ntrain['dayofweek'] = train['date'].dt.dayofweek\navg_sales_by_day = train.groupby('dayofweek')['num_sold'].mean()\nsns.barplot(x=avg_sales_by_day.index, y=avg_sales_by_day.values)\nplt.title('Average Sales by Day of Week')\nplt.xlabel('Day of Week')\nplt.ylabel('Average Sales')\n\n# Plot 2: Sales distribution\nplt.subplot(2, 2, 2)\nsns.histplot(train['num_sold'], bins=50)\nplt.title('Distribution of Sales')\nplt.xlabel('Number Sold')\nplt.ylabel('Count')\n\n# Plot 3: Average sales by country\nplt.subplot(2, 2, 3)\navg_sales_by_country = train.groupby('country')['num_sold'].mean().sort_values(ascending=False)\nsns.barplot(x=avg_sales_by_country.index, y=avg_sales_by_country.values)\nplt.title('Average Sales by Country')\nplt.xticks(rotation=45)\nplt.xlabel('Country')\nplt.ylabel('Average Sales')\n\nplt.tight_layout()\nplt.show()", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-01T06:14:07.511815Z", "iopub.execute_input": "2025-01-01T06:14:07.51218Z", "iopub.status.idle": "2025-01-01T06:14:08.650357Z", "shell.execute_reply.started": "2025-01-01T06:14:07.512154Z", "shell.execute_reply": "2025-01-01T06:14:08.649311Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "### Feature Engineering", "metadata": {}}, {"cell_type": "code", "source": "def create_features(df):\n    # Create a copy to avoid modifying original dataframe\n    df = df.copy()\n    \n    # Convert date to datetime if it's not already\n    df['date'] = pd.to_datetime(df['date'])\n    \n    # Time-based features\n    df['year'] = df['date'].dt.year\n    df['month'] = df['date'].dt.month\n    df['day'] = df['date'].dt.day\n    df['dayofweek'] = df['date'].dt.dayofweek\n    df['is_weekend'] = df['dayofweek'].isin([5, 6]).astype(int)\n    \n    # Seasonal features\n    df['season'] = pd.cut(df['month'], \n                         bins=[0, 3, 6, 9, 12], \n                         labels=['Winter', 'Spring', 'Summer', 'Fall'])\n    \n    # Encode categorical variables\n    le = LabelEncoder()\n    categorical_cols = ['country', 'store', 'item', 'season']\n    \n    for col in categorical_cols:\n        if col in df.columns:\n            df[f'{col}_encoded'] = le.fit_transform(df[col])\n    \n    return df\n\n# Apply feature engineering\ntrain_processed = create_features(train)\ntest_processed = create_features(test)\n\n# Display new features\nprint(\"New features created:\")\nprint(train_processed.columns.tolist())", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-01T06:14:11.58999Z", "iopub.execute_input": "2025-01-01T06:14:11.590948Z", "iopub.status.idle": "2025-01-01T06:14:11.880923Z", "shell.execute_reply.started": "2025-01-01T06:14:11.590903Z", "shell.execute_reply": "2025-01-01T06:14:11.879736Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "### Model Training", "metadata": {}}, {"cell_type": "code", "source": "\"\"\"# Prepare features for modeling\nfeature_cols = ['year', 'month', 'day', 'dayofweek', 'is_weekend',\n               'country_encoded', 'store_encoded', 'item_encoded']\n\nX = train_processed[feature_cols]\ny = train_processed['num_sold']\n\n# Split data\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Initialize and train model\nmodel = lgb.LGBMRegressor(\n    n_estimators=1000,\n    learning_rate=0.1,\n    max_depth=8,\n    num_leaves=31,\n    random_state=42\n)\n\nmodel.fit(\n    X_train, y_train,\n    eval_set=[(X_val, y_val)],\n    eval_metric='mape',\n    early_stopping_rounds=50,\n    verbose=100\n)\n\n# Calculate validation score\nval_predictions = model.predict(X_val)\nval_mape = mean_absolute_percentage_error(y_val, val_predictions)\nprint(f\"\\nValidation MAPE: {val_mape:.4f}\")\"\"\"\n\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_percentage_error\nfrom sklearn.preprocessing import LabelEncoder\nimport lightgbm as lgb\n\ntrain_data = pd.read_csv('./input/playground-series-s5e1/train.csv')\ntest_data = pd.read_csv('./input/playground-series-s5e1/test.csv')\n\nprint(\"Training set shape:\", train_data.shape)\nprint(\"Test set shape:\", test_data.shape)\n\ntrain_data = train_data.dropna(subset=['num_sold'])\n\ntrain_data['date'] = pd.to_datetime(train_data['date'])\ntrain_data['year'] = train_data['date'].dt.year\ntrain_data['month'] = train_data['date'].dt.month\ntrain_data['day'] = train_data['date'].dt.day\ntrain_data['dayofweek'] = train_data['date'].dt.dayofweek\ntrain_data['is_weekend'] = (train_data['dayofweek'] >= 5).astype(int)\n\nlabel_encoders = {}\nfor col in ['country', 'store', 'product']:\n    le = LabelEncoder()\n    train_data[col + '_encoded'] = le.fit_transform(train_data[col])\n    label_encoders[col] = le\n\nfeature_cols = ['year', 'month', 'day', 'dayofweek', 'is_weekend',\n                'country_encoded', 'store_encoded', 'product_encoded']\nX = train_data[feature_cols]\ny = train_data['num_sold']\nmin_gain_to_split=0.1 \nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = lgb.LGBMRegressor(\n    n_estimators=500,\n    learning_rate=0.05,\n    num_leaves=64,\n    max_depth=8,\n    min_gain_to_split=0.1,\n    random_state=42\n)\n\n\nmodel.fit(\n    X_train, y_train,\n    eval_set=[(X_val, y_val)],\n    eval_metric='mape',\n    callbacks=[\n        lgb.early_stopping(stopping_rounds=50),\n        lgb.log_evaluation(period=100)\n    ]\n)\n\nval_predictions = model.predict(X_val)\nval_mape = mean_absolute_percentage_error(y_val, val_predictions)\nprint(f\"Validation MAPE: {val_mape:.4f}\")\n", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-01T06:17:30.242544Z", "iopub.execute_input": "2025-01-01T06:17:30.242995Z", "iopub.status.idle": "2025-01-01T06:17:35.573413Z", "shell.execute_reply.started": "2025-01-01T06:17:30.242968Z", "shell.execute_reply": "2025-01-01T06:17:35.572432Z"}, "_kg_hide-input": true}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "### Feature Importance Analysis", "metadata": {}}, {"cell_type": "code", "source": "# Plot feature importance\nfeature_importance = pd.DataFrame({\n    'feature': feature_cols,\n    'importance': model.feature_importances_\n})\nfeature_importance = feature_importance.sort_values('importance', ascending=False)\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x='importance', y='feature', data=feature_importance)\nplt.title('Feature Importance')\nplt.show()", "metadata": {"trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "### Make Predictions and Create Submission", "metadata": {}}, {"cell_type": "code", "source": "# Generate predictions for test set\ntest_predictions = model.predict(test_processed[feature_cols])\n\n# Create submission file\nsubmission = pd.DataFrame({\n    'id': test['id'],\n    'num_sold': test_predictions\n})\n\n# Save submission file\nsubmission.to_csv('submission.csv', index=False)\nprint(\"Submission file created!\")", "metadata": {"trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "### Next Steps for Improvement\n1. Feature Engineering:\n   - Add lag features for time series aspects\n   - Create rolling statistics\n   - Add holiday indicators\n   - Include price-related features if available\n\n2. Modeling:\n   - Experiment with other algorithms (XGBoost, CatBoost)\n   - Implement cross-validation\n   - Add hyperparameter tuning\n   - Consider ensemble methods\n\n3. Analysis:\n   - Analyze prediction errors\n   - Investigate seasonal patterns\n   - Study country-specific trends", "metadata": {}}]}