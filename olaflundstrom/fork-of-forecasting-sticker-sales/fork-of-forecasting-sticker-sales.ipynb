{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0075",
   "metadata": {},
   "source": [
    "# Kaggle Sticker Sales Forecasting Competition\n",
    "## Playground Series S5E1 - January 2025\n",
    "\n",
    "### Overview\n",
    "This notebook presents a solution for the Kaggle Playground Series competition on forecasting sticker sales. The goal is to predict sticker sales across different countries using various store and time-based features.\n",
    "\n",
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('./input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d4",
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "051d70d956493feee0c6d64651c6a088724dca2a",
    "execution": {
     "iopub.execute_input": "2025-01-01T06:13:53.070569Z",
     "iopub.status.busy": "2025-01-01T06:13:53.070185Z",
     "iopub.status.idle": "2025-01-01T06:13:55.297703Z",
     "shell.execute_reply": "2025-01-01T06:13:55.296619Z",
     "shell.execute_reply.started": "2025-01-01T06:13:53.07054Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import lightgbm as lgb\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set styling for visualizations\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825d",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T06:13:56.757213Z",
     "iopub.status.busy": "2025-01-01T06:13:56.756566Z",
     "iopub.status.idle": "2025-01-01T06:13:57.228814Z",
     "shell.execute_reply": "2025-01-01T06:13:57.22774Z",
     "shell.execute_reply.started": "2025-01-01T06:13:56.757183Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read the training and test datasets\n",
    "train = pd.read_csv('./input/playground-series-s5e1/train.csv')\n",
    "test = pd.read_csv('./input/playground-series-s5e1/test.csv')\n",
    "\n",
    "print(\"Training set shape:\", train.shape)\n",
    "print(\"Test set shape:\", test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a4",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T06:13:58.729501Z",
     "iopub.status.busy": "2025-01-01T06:13:58.729119Z",
     "iopub.status.idle": "2025-01-01T06:13:58.903311Z",
     "shell.execute_reply": "2025-01-01T06:13:58.902228Z",
     "shell.execute_reply.started": "2025-01-01T06:13:58.72947Z"
    }
   },
   "outputs": [],
   "source": [
    "# Display basic information about the training data\n",
    "print(\"\\nTraining data info:\")\n",
    "print(train.info())\n",
    "\n",
    "# Show first few rows\n",
    "print(\"\\nFirst few rows of training data:\")\n",
    "display(train.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values in training data:\")\n",
    "print(train.isnull().sum())\n",
    "\n",
    "# Basic statistics of numerical columns\n",
    "print(\"\\nDescriptive statistics:\")\n",
    "display(train.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697a",
   "metadata": {},
   "source": [
    "### Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T06:14:07.51218Z",
     "iopub.status.busy": "2025-01-01T06:14:07.511815Z",
     "iopub.status.idle": "2025-01-01T06:14:08.650357Z",
     "shell.execute_reply": "2025-01-01T06:14:08.649311Z",
     "shell.execute_reply.started": "2025-01-01T06:14:07.512154Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a figure with multiple subplots\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Average sales by day of week\n",
    "plt.subplot(2, 2, 1)\n",
    "train['date'] = pd.to_datetime(train['date'])\n",
    "train['dayofweek'] = train['date'].dt.dayofweek\n",
    "avg_sales_by_day = train.groupby('dayofweek')['num_sold'].mean()\n",
    "sns.barplot(x=avg_sales_by_day.index, y=avg_sales_by_day.values)\n",
    "plt.title('Average Sales by Day of Week')\n",
    "plt.xlabel('Day of Week')\n",
    "plt.ylabel('Average Sales')\n",
    "\n",
    "# Plot 2: Sales distribution\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.histplot(train['num_sold'], bins=50)\n",
    "plt.title('Distribution of Sales')\n",
    "plt.xlabel('Number Sold')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Plot 3: Average sales by country\n",
    "plt.subplot(2, 2, 3)\n",
    "avg_sales_by_country = train.groupby('country')['num_sold'].mean().sort_values(ascending=False)\n",
    "sns.barplot(x=avg_sales_by_country.index, y=avg_sales_by_country.values)\n",
    "plt.title('Average Sales by Country')\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Country')\n",
    "plt.ylabel('Average Sales')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e67",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T06:14:11.590948Z",
     "iopub.status.busy": "2025-01-01T06:14:11.58999Z",
     "iopub.status.idle": "2025-01-01T06:14:11.880923Z",
     "shell.execute_reply": "2025-01-01T06:14:11.879736Z",
     "shell.execute_reply.started": "2025-01-01T06:14:11.590903Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    # Create a copy to avoid modifying original dataframe\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Convert date to datetime if it's not already\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    # Time-based features\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['dayofweek'] = df['date'].dt.dayofweek\n",
    "    df['is_weekend'] = df['dayofweek'].isin([5, 6]).astype(int)\n",
    "    \n",
    "    # Seasonal features\n",
    "    df['season'] = pd.cut(df['month'], \n",
    "                         bins=[0, 3, 6, 9, 12], \n",
    "                         labels=['Winter', 'Spring', 'Summer', 'Fall'])\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    le = LabelEncoder()\n",
    "    categorical_cols = ['country', 'store', 'item', 'season']\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            df[f'{col}_encoded'] = le.fit_transform(df[col])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply feature engineering\n",
    "train_processed = create_features(train)\n",
    "test_processed = create_features(test)\n",
    "\n",
    "# Display new features\n",
    "print(\"New features created:\")\n",
    "print(train_processed.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3c",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f2",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-01-01T06:17:30.242995Z",
     "iopub.status.busy": "2025-01-01T06:17:30.242544Z",
     "iopub.status.idle": "2025-01-01T06:17:35.573413Z",
     "shell.execute_reply": "2025-01-01T06:17:35.572432Z",
     "shell.execute_reply.started": "2025-01-01T06:17:30.242968Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"# Prepare features for modeling\n",
    "feature_cols = ['year', 'month', 'day', 'dayofweek', 'is_weekend',\n",
    "               'country_encoded', 'store_encoded', 'item_encoded']\n",
    "\n",
    "X = train_processed[feature_cols]\n",
    "y = train_processed['num_sold']\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize and train model\n",
    "model = lgb.LGBMRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=8,\n",
    "    num_leaves=31,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric='mape',\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "# Calculate validation score\n",
    "val_predictions = model.predict(X_val)\n",
    "val_mape = mean_absolute_percentage_error(y_val, val_predictions)\n",
    "print(f\"\\nValidation MAPE: {val_mape:.4f}\")\"\"\"\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "\n",
    "train_data = pd.read_csv('./input/playground-series-s5e1/train.csv')\n",
    "test_data = pd.read_csv('./input/playground-series-s5e1/test.csv')\n",
    "\n",
    "print(\"Training set shape:\", train_data.shape)\n",
    "print(\"Test set shape:\", test_data.shape)\n",
    "\n",
    "train_data = train_data.dropna(subset=['num_sold'])\n",
    "\n",
    "train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "train_data['year'] = train_data['date'].dt.year\n",
    "train_data['month'] = train_data['date'].dt.month\n",
    "train_data['day'] = train_data['date'].dt.day\n",
    "train_data['dayofweek'] = train_data['date'].dt.dayofweek\n",
    "train_data['is_weekend'] = (train_data['dayofweek'] >= 5).astype(int)\n",
    "\n",
    "label_encoders = {}\n",
    "for col in ['country', 'store', 'product']:\n",
    "    le = LabelEncoder()\n",
    "    train_data[col + '_encoded'] = le.fit_transform(train_data[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "feature_cols = ['year', 'month', 'day', 'dayofweek', 'is_weekend',\n",
    "                'country_encoded', 'store_encoded', 'product_encoded']\n",
    "X = train_data[feature_cols]\n",
    "y = train_data['num_sold']\n",
    "min_gain_to_split=0.1 \n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = lgb.LGBMRegressor(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=64,\n",
    "    max_depth=8,\n",
    "    min_gain_to_split=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric='mape',\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=50),\n",
    "        lgb.log_evaluation(period=100)\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_predictions = model.predict(X_val)\n",
    "val_mape = mean_absolute_percentage_error(y_val, val_predictions)\n",
    "print(f\"Validation MAPE: {val_mape:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1e",
   "metadata": {},
   "source": [
    "### Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': model.feature_importances_\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance)\n",
    "plt.title('Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da3",
   "metadata": {},
   "source": [
    "### Make Predictions and Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for test set\n",
    "test_predictions = model.predict(test_processed[feature_cols])\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'num_sold': test_predictions\n",
    "})\n",
    "\n",
    "# Save submission file\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Submission file created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb6",
   "metadata": {},
   "source": [
    "### Next Steps for Improvement\n",
    "1. Feature Engineering:\n",
    "   - Add lag features for time series aspects\n",
    "   - Create rolling statistics\n",
    "   - Add holiday indicators\n",
    "   - Include price-related features if available\n",
    "\n",
    "2. Modeling:\n",
    "   - Experiment with other algorithms (XGBoost, CatBoost)\n",
    "   - Implement cross-validation\n",
    "   - Add hyperparameter tuning\n",
    "   - Consider ensemble methods\n",
    "\n",
    "3. Analysis:\n",
    "   - Analyze prediction errors\n",
    "   - Investigate seasonal patterns\n",
    "   - Study country-specific trends"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10652996,
     "sourceId": 85723,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
