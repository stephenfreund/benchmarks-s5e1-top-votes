{"metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "language_info": {"name": "python", "version": "3.10.12", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}, "kaggle": {"accelerator": "none", "dataSources": [{"sourceId": 85723, "databundleVersionId": 10652996, "sourceType": "competition"}, {"sourceId": 3325325, "sourceType": "datasetVersion", "datasetId": 2007861}, {"sourceId": 218469538, "sourceType": "kernelVersion"}], "dockerImageVersionId": 30822, "isInternetEnabled": true, "language": "python", "sourceType": "notebook", "isGpuEnabled": false}}, "nbformat_minor": 4, "nbformat": 4, "cells": [{"cell_type": "markdown", "source": "# Overview\n\nIn this notebook, we will try to decompose the competition data into several factors, following the ideas from **[the first discussion](https://www.kaggle.com/competitions/playground-series-s5e1/discussion/554349)**, **[the second discussion](https://www.kaggle.com/competitions/playground-series-s5e1/discussion/556900)** and **[the third discussion](https://www.kaggle.com/competitions/playground-series-s5e1/discussion/557955)**.\n\nThe time series model that will be used looks as follows:.\n$$S(s, c, p, t) = \\text{GDP}(c, y)\\cdot \n    \\text{ST}(s) \\cdot \n    \\text{PROD}(p, t) \\cdot\n    \\text{WD}(t) \\cdot\n    \\text{DY}(d) \\cdot\n    \\text{SC}(t) \\cdot\n    \\text{C}(c) \\cdot\n    \\text{CY}(c, d).$$\n\nHere, \n- \\\\(S\\\\) is the number of items of product \\\\(p\\\\) sold at the date \\\\(t\\\\), country \\\\(c\\\\) by the store \\\\(s\\\\);\n- \\\\(\\text{GDP}(c, y)\\\\) is the GDP per capita factor that depends on the country and the year \\\\(y=y(t)\\\\) (**stepwise function on time**);\n- \\\\(\\text{ST}(s)\\\\) is the store factor representing the ratio of items sold by each of the stores (**constant**);\n- \\\\(\\text{PROD}(p, t)\\\\) is the product factor representing the ratio of items of each product \\\\(p\\\\) sold at each date \\\\(t\\\\) (**periodic**);\n- \\\\(\\text{WD}(t)\\\\) is the day-of-week factor that is responsible for the rise of sales at Friday-Sunday (**constant**);\n- \\\\(\\text{DY}(d)\\\\) is the day-of-year factor that is responsible for the dependence of sales on the day number \\\\(d(t)\\\\) in the year (**periodic**);\n- \\\\(\\text{SC}(t)\\\\) additional dependence of the sales on the date (**periodic**);\n- \\\\(\\text{C}(c)\\\\) is the country-dependent factor (**constant**);\n- \\\\(\\text{CY}(c, d)\\\\) is the country-dependent day-of-year factor.\n\nNote: \\\\(\\text{DY}(d)\\\\) and \\\\(\\text{SC}(t)\\\\) can be represented as a common \\\\(\\text{SC}(t)\\\\) factor. So, in the last cell \\\\(\\text{DY}(d)\\\\) factor was excluded from the final prediction. You can turn it back, if you like.", "metadata": {}}, {"cell_type": "markdown", "source": "# 1. Import libraries, load data and define config", "metadata": {}}, {"cell_type": "code", "source": "import holidays\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_absolute_percentage_error\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nplt.rcParams.update({'font.size': 14})\n\nTRAIN_CSV = './input/playground-series-s5e1/train.csv'\nTEST_CSV = './input/playground-series-s5e1/test.csv'\n\ntrain_df = pd.read_csv(TRAIN_CSV)\ntest_df = pd.read_csv(TEST_CSV)\n\ntrain_df.date = pd.DatetimeIndex(train_df.date)\ntrain_df['test'] = 0\ntest_df.date = pd.DatetimeIndex(test_df.date)\ntest_df['test'] = 1\n\nclass CFG:\n    years_train = train_df.date.dt.year.unique()\n    years_test = test_df.date.dt.year.unique()    \n    years = np.concatenate((train_df.date.dt.year.unique(), test_df.date.dt.year.unique()))\n    \n    validation_year = 2000 # non-existing year or the year we use in validation - fill free to change this to 2016 or whatever you like\n    \n    countries = train_df.country.unique()\n    stores = train_df.store.unique()\n    products = train_df['product'].unique()\n\n    alpha3 = {'Finland': 'FIN', 'Canada': 'CAN', 'Italy': 'IT', 'Kenya': 'KEN', 'Singapore': 'SGP', 'Norway': 'NOR'}\n\n    fft_filter_width = 8\n\n    countries_2l = {'Finland': 'FI', 'Canada': 'CA', 'Italy': 'IT', 'Kenya': 'KE', 'Singapore': 'SG', 'Norway': 'NO'}\n    holiday_response_len = 10", "metadata": {"_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5", "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19", "trusted": true, "execution": {"iopub.status.busy": "2025-01-21T20:30:44.678491Z", "iopub.execute_input": "2025-01-21T20:30:44.678857Z", "iopub.status.idle": "2025-01-21T20:30:46.85033Z", "shell.execute_reply.started": "2025-01-21T20:30:44.678826Z", "shell.execute_reply": "2025-01-21T20:30:46.849545Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "# 2. Decomposition\n\nWe start with defining many features, that could be helpful in the future work", "metadata": {}}, {"cell_type": "code", "source": "df = pd.concat((train_df, test_df))\ndf.date = pd.DatetimeIndex(df.date)\ndf['year'] = df['date'].dt.year\ndf['weekday'] = df['date'].dt.weekday\ndf['dayofyear'] = df['date'].dt.dayofyear\ndf['daynum'] = (df.date - df.date.iloc[0]).dt.days\ndf['weeknum'] = df['daynum'] // 7\ndf['month'] = df.date.dt.month\n\n# Sinusoidal features\ndaysinyear = (df.groupby('year').id.count() / len(CFG.countries) / len(CFG.stores) / len(CFG.products)).rename('daysinyear').astype(int).to_frame()\ndf = df.join(daysinyear, on='year', how='left')\ndf['partofyear'] = (df['dayofyear'] - 1) / df['daysinyear']\ndf['partof2year'] = df['partofyear'] + df['year'] % 2\ndf['partof2year'] = df['partofyear'] + df['year'] % 2\n\nCFG.sincoscol = [f'sin t', f'cos t', f'sin t/2', f'cos t/2']\nCFG.sincoscol2 = [f'sin 2t', f'cos 2t', f'sin t', f'cos t', f'sin t/2', f'cos t/2']\ndf['sin 4t'] = np.sin(8 * np.pi * df['partofyear'])\ndf['cos 4t'] = np.cos(8 * np.pi * df['partofyear'])\ndf['sin 3t'] = np.sin(6 * np.pi * df['partofyear'])\ndf['cos 3t'] = np.cos(6 * np.pi * df['partofyear'])\ndf['sin 2t'] = np.sin(4 * np.pi * df['partofyear'])\ndf['cos 2t'] = np.cos(4 * np.pi * df['partofyear'])\ndf['sin t'] = np.sin(2 * np.pi * df['partofyear'])\ndf['cos t'] = np.cos(2 * np.pi * df['partofyear'])\ndf['sin t/2'] = np.sin(np.pi * df['partof2year'])\ndf['cos t/2'] = np.cos(np.pi * df['partof2year'])\ndf.drop(['daysinyear', 'partofyear', 'partof2year'], axis=1, inplace=True)", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-21T20:30:46.851622Z", "iopub.execute_input": "2025-01-21T20:30:46.851876Z", "iopub.status.idle": "2025-01-21T20:30:47.182956Z", "shell.execute_reply.started": "2025-01-21T20:30:46.851854Z", "shell.execute_reply": "2025-01-21T20:30:47.181914Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "## 2.1. GDP factor\n\nRefer to **[this discussion](https://www.kaggle.com/competitions/playground-series-s5e1/discussion/554349)**, where it was proposed to use GDP per capita factor and **[this discussion](https://www.kaggle.com/competitions/playground-series-s5e1/discussion/555716)** about the data sources to be used.", "metadata": {}}, {"cell_type": "code", "source": "import requests\ndef get_gdp_per_capita(country,year):\n    url=\"https://api.worldbank.org/v2/country/{0}/indicator/NY.GDP.PCAP.CD?date={1}&format=json\".format(CFG.alpha3[country],year)\n    response = requests.get(url).json()\n    return response[1][0]['value']\n\n# gdp[counties x years]\ngdp = np.array([[get_gdp_per_capita(country, year) for year in CFG.years] for country in CFG.countries])\ngdp_df = pd.DataFrame(gdp, index=train_df.country.unique(), columns=CFG.years)\n\ndf['gdp_factor'] = None\nfor year in CFG.years:\n    for country in CFG.countries:\n        df.loc[(df.country == country) & (df.year == year), 'gdp_factor'] = gdp_df.loc[country, year]", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-21T20:30:47.185059Z", "iopub.execute_input": "2025-01-21T20:30:47.185362Z", "iopub.status.idle": "2025-01-21T20:31:09.096454Z", "shell.execute_reply.started": "2025-01-21T20:30:47.185339Z", "shell.execute_reply": "2025-01-21T20:31:09.095398Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "## 2.2. Store factor\nWe exclude Canada and Kenya because of missed values", "metadata": {}}, {"cell_type": "code", "source": "df_no_can_ken = df[~df.country.isin(('Canada', 'Kenya'))]\n\nstore_df = df_no_can_ken.groupby(by='store').num_sold.mean().rename('store_factor').to_frame()\ndf = df.drop('store_factor', axis=1, errors='ignore').join(store_df, on='store', how='left')", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-21T20:31:09.097814Z", "iopub.execute_input": "2025-01-21T20:31:09.098123Z", "iopub.status.idle": "2025-01-21T20:31:09.405114Z", "shell.execute_reply.started": "2025-01-21T20:31:09.09809Z", "shell.execute_reply": "2025-01-21T20:31:09.404013Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "## 2.3. Product factor\nWe exclude Canada and Kenya because of missed values", "metadata": {}}, {"cell_type": "code", "source": "from sklearn.linear_model import Ridge\n\ndf_no_can_ken = df[~df.country.isin(('Canada', 'Kenya'))].copy()\n\ntotal = df_no_can_ken.groupby(by='date').num_sold.sum().rename('num_sold_total')\ndf_no_can_ken = df_no_can_ken.join(total, on='date', how='left')\ndf_no_can_ken['num_sold_ratio'] = df_no_can_ken['num_sold'] / df_no_can_ken['num_sold_total']\n\nplt.figure(figsize=(24, 6))\ndf['product_factor'] = None\nfor product in CFG.products:\n    df_no_can_ken_date = df_no_can_ken[(df_no_can_ken['product'] == product) & (df_no_can_ken['test'] == 0)].groupby(by='date')\n    x = df_no_can_ken_date[CFG.sincoscol].mean().to_numpy()\n    y = df_no_can_ken_date.num_sold_ratio.sum().to_numpy()\n\n    reg = Ridge()\n    reg.fit(x, y)\n    p = reg.predict(x)\n    df.loc[(df['product'] == product), 'product_factor'] = reg.predict(df.loc[(df['product'] == product), CFG.sincoscol].to_numpy())\n   \n    plt.plot(y, 'b')\n    plt.plot(p, 'r')\nplt.show();", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-21T20:31:09.406204Z", "iopub.execute_input": "2025-01-21T20:31:09.406483Z", "iopub.status.idle": "2025-01-21T20:31:10.948706Z", "shell.execute_reply.started": "2025-01-21T20:31:09.406459Z", "shell.execute_reply": "2025-01-21T20:31:10.947613Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "## 2.4. Day of week\nTo analyze this factor, we have to exclude holiday periods that affect the sales. For this, we use an excellent `holidays` library.", "metadata": {}}, {"cell_type": "code", "source": "df['holiday'] = 0\n\nfor country in CFG.countries:\n    days = [str(day) for day in holidays.CountryHoliday(CFG.countries_2l[country], years=CFG.years)]\n    df.loc[(df.country==country) & (df.date.isin(days)), 'holiday'] = 1\n\nnum_sold_per_week_country_weekday = df.groupby(['weeknum', 'country', 'weekday'])['num_sold'].sum().reset_index().pivot(index=['weeknum', 'country'], columns='weekday')\nratio_sold_per_week_country_weekday = num_sold_per_week_country_weekday.apply(lambda row: row/sum(row), axis=1).reset_index()\n\nratio_weekday = pd.DataFrame(columns=CFG.countries, data=[[0, ]*len(CFG.countries)]*7)\n\nfor n, country in enumerate(CFG.countries):\n    for d in range(7):\n        dt = ratio_sold_per_week_country_weekday.loc[ratio_sold_per_week_country_weekday.country == country, ('num_sold', d)][:-60]\n        ratio_weekday.loc[d, country] = dt.median()\n\nratio_weekday_mean = ratio_weekday.mean(axis=1)\nratio_weekday['mean'] = ratio_weekday_mean\n\ndf['weekday_factor'] = df.weekday.map(ratio_weekday_mean)\n\n# The total ratio taking into account all factors\ndf['ratio'] = df['gdp_factor'] * df['product_factor'] * df['store_factor'] * df['weekday_factor']\n\n# The total sold items taking into account all factors\ndf['total'] = df['num_sold'] / df['ratio']", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-21T20:31:10.949826Z", "iopub.execute_input": "2025-01-21T20:31:10.950173Z", "iopub.status.idle": "2025-01-21T20:31:12.069197Z", "shell.execute_reply.started": "2025-01-21T20:31:10.950143Z", "shell.execute_reply": "2025-01-21T20:31:12.068243Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "## 2.5. The general dependence on the day of year\n\nWe have to exclude holiday-related effects before calculating this factor.\n**It could be clearly seen that sales form a sinusoidal pattern.**", "metadata": {}}, {"cell_type": "code", "source": "# Exclude holidays\ndf_holidays = df.copy()\ndf_holidays['holiday_response'] = 0\nfor country in CFG.countries:\n    for holiday, _ in holidays.CountryHoliday(CFG.countries_2l[country], years=CFG.years).items():\n        df_holidays.loc[(df_holidays.country==country) & df_holidays.date.isin(pd.date_range(holiday, periods=CFG.holiday_response_len)), 'holiday_response'] = 1\n\nfig = plt.figure(figsize=(24,6))\ndata = pd.DataFrame()\nfor n, country in enumerate(CFG.countries):\n    dt = df_holidays[(df_holidays.country==country) & (df_holidays.holiday_response == 0)].groupby(['dayofyear']).total.median()\n    data[country]= dt\n    plt.plot(dt, label=country)\ndata['median'] = data.median(axis=1)\n\n# Linear regression on fourier series\nx = data.index.to_numpy()\ny = data['median'].to_numpy()\nfourier = lambda t: np.array([np.sin(2*np.pi/365*t), np.cos(2*np.pi/365*t)])\n\nyear_ratio = Ridge(alpha=0.01).fit(fourier(x).T, y.T).predict(fourier(np.arange(1, 366)).T)\nyear_ratio = np.append(year_ratio, year_ratio[-1])\n\ndf['dayofyear_factor'] = df.dayofyear.map(dict(zip(np.arange(1, 367), year_ratio)))\n\n# The total ratio taking into account all factors\ndf['ratio'] = df['gdp_factor'] * df['product_factor'] * df['store_factor'] * df['weekday_factor'] * df['dayofyear_factor']\n\n# The total sold items taking into account all factors\ndf['total'] = df['num_sold'] / df['ratio']\n\nplt.plot(year_ratio, 'k', linewidth=4)\nplt.legend();", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-21T20:31:12.070163Z", "iopub.execute_input": "2025-01-21T20:31:12.07045Z", "iopub.status.idle": "2025-01-21T20:31:30.792445Z", "shell.execute_reply.started": "2025-01-21T20:31:12.070412Z", "shell.execute_reply": "2025-01-21T20:31:30.791475Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "## 2.6. The sincos dependence on the day of year\n\nMoreover, we can see a periodic dependence on the longer time periods. We can take this into account, too, by incorporating more fourier members.", "metadata": {}}, {"cell_type": "code", "source": "fig = plt.figure(figsize=(24,6))\ndata = pd.DataFrame()\nfor n, country in enumerate(CFG.countries):\n    dt = df_holidays[(df_holidays.test == 0) & (df_holidays.country==country) & (df_holidays.holiday_response == 0)].groupby(['date']).total.median()\n    data[country]= dt\n    plt.plot(dt, label=country)\ndata['median'] = data.median(axis=1)", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-21T20:31:30.795364Z", "iopub.execute_input": "2025-01-21T20:31:30.795832Z", "iopub.status.idle": "2025-01-21T20:31:33.597528Z", "shell.execute_reply.started": "2025-01-21T20:31:30.795785Z", "shell.execute_reply": "2025-01-21T20:31:33.59654Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "CFG.sincoscol2 = ['sin 4t', 'cos 4t', 'sin 3t', 'cos 3t', 'sin 2t', 'cos 2t', 'sin t', 'cos t', 'sin t/2', 'cos t/2']\n\n# Linear regression on fourier series\ndfsc = df[df.test == 0].groupby('date')[CFG.sincoscol2].mean()#.to_numpy()\ndfsc['median'] = data['median']\n\nx = dfsc[~pd.isna(dfsc['median'])][CFG.sincoscol2].to_numpy()\ny = dfsc[~pd.isna(dfsc['median'])]['median'].to_numpy()\n\nreg = Ridge(alpha=0.01, fit_intercept=True)\nreg.fit(x, y)\n\nfig = plt.figure(figsize=(24,6))\nplt.plot(y, 'k')\nplt.plot(reg.predict(x), 'r')\n\ndf['sincos_factor'] = reg.intercept_ + (df[CFG.sincoscol2] * reg.coef_).sum(axis=1)", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-21T20:31:33.599144Z", "iopub.execute_input": "2025-01-21T20:31:33.599474Z", "iopub.status.idle": "2025-01-21T20:31:34.039209Z", "shell.execute_reply.started": "2025-01-21T20:31:33.599445Z", "shell.execute_reply": "2025-01-21T20:31:34.037988Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "## 2.7. Country factor\n\nKenyan sales are clearly different from the other countries'. So, it seems a good idea to introduce additional country-related feature.\n\nWe can look at the `Kaggle` product that has no missing values.", "metadata": {}}, {"cell_type": "code", "source": "# The total ratio taking into account all factors\ndf['ratio'] = df['gdp_factor'] * df['product_factor'] * df['store_factor'] * df['weekday_factor'] * df['sincos_factor']\n\n# The total sold items taking into account all factors\ndf['total'] = df['num_sold'] / df['ratio']\n\nfig = plt.figure(figsize=(24,6))\nfor c in CFG.countries:\n    df_p = df[(df.country == c) & (df['product'] == 'Kaggle')].groupby('date').total.sum().to_numpy()\n    plt.plot(df_p, label=c)\n\nplt.legend();", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-21T20:31:34.040343Z", "iopub.execute_input": "2025-01-21T20:31:34.0407Z", "iopub.status.idle": "2025-01-21T20:31:35.014149Z", "shell.execute_reply.started": "2025-01-21T20:31:34.040673Z", "shell.execute_reply": "2025-01-21T20:31:35.01315Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "Calculating the country factor is quite simple.", "metadata": {}}, {"cell_type": "code", "source": "country_factor = df[(df['product'] == 'Kaggle')].groupby('country').total.sum().rename('country_factor')\ncountry_factor = country_factor / country_factor.median()\ndf = df.join(country_factor, on='country', how='left')", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-21T20:31:35.015133Z", "iopub.execute_input": "2025-01-21T20:31:35.01538Z", "iopub.status.idle": "2025-01-21T20:31:35.2731Z", "shell.execute_reply.started": "2025-01-21T20:31:35.01536Z", "shell.execute_reply": "2025-01-21T20:31:35.272347Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "## 2.8. Non-periodic day-of-year dependence\nWe can see some dates of increased sales, and these dates seem to occur each year. So, let's take an average of the sales each year as an additional factor.", "metadata": {}}, {"cell_type": "code", "source": "df['ratio'] = df['gdp_factor'] * df['product_factor'] * df['store_factor'] * df['weekday_factor'] * df['sincos_factor'] * df['country_factor']\n\n# The total sold items taking into account all factors\ndf['total'] = df['num_sold'] / df['ratio']\n\nfig = plt.figure(figsize=(24,6))\nfor c in CFG.countries:\n    df_p = df[(df.country == c) & (df['product'] == 'Kaggle')].groupby('date').total.sum().to_numpy()\n    plt.plot(df_p, label=c)\n\nplt.legend();", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-21T20:31:35.27391Z", "iopub.execute_input": "2025-01-21T20:31:35.274186Z", "iopub.status.idle": "2025-01-21T20:31:36.164461Z", "shell.execute_reply.started": "2025-01-21T20:31:35.274163Z", "shell.execute_reply": "2025-01-21T20:31:36.163453Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "npdoy_factor = df.groupby('dayofyear').total.median().rename('npdoy_factor')\ndf = df.join(npdoy_factor, on='dayofyear', how='left')", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-21T20:31:36.165503Z", "iopub.execute_input": "2025-01-21T20:31:36.165774Z", "iopub.status.idle": "2025-01-21T20:31:36.420022Z", "shell.execute_reply.started": "2025-01-21T20:31:36.165751Z", "shell.execute_reply": "2025-01-21T20:31:36.419203Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "# 3. Prediction & submission\n\nI have spyed the 1.06 multiplier in [the notebook](https://www.kaggle.com/code/cabaxiom/s5e1-eda-and-linear-regression-baseline) by @cabaxiom.\nCurrently, I don't know how to explain this. Fill free to change and experiment with it (though it seems optimal)!\n\n## 3.1. Calculate MAPE on the train dataset", "metadata": {}}, {"cell_type": "code", "source": "#df['ratio'] = df['gdp_factor'] * df['product_factor'] * df['store_factor'] * df['weekday_factor'] * df['dayofyear_factor'] * df['sincos_factor'] * df['country_factor']\ndf['ratio'] = df['gdp_factor'] * df['product_factor'] * df['store_factor'] * df['weekday_factor'] * df['sincos_factor'] * df['country_factor']\n\ndf['total'] = df['num_sold'] / df['ratio']\nconst_factor = df['total'].median() * 1.06\n\ndf['prediction'] = const_factor * df['ratio']\nmape_train = mean_absolute_percentage_error(df[(df.test == 0) & (~pd.isna(df.num_sold))].num_sold, df[(df.test == 0) & (~pd.isna(df.num_sold))].prediction)\n\nprint(f'Without new npdoy_factor {mape_train=}')", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-21T20:31:36.420978Z", "iopub.execute_input": "2025-01-21T20:31:36.421364Z", "iopub.status.idle": "2025-01-21T20:31:36.740243Z", "shell.execute_reply.started": "2025-01-21T20:31:36.421333Z", "shell.execute_reply": "2025-01-21T20:31:36.739223Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "df['ratio'] = df['gdp_factor'] * df['product_factor'] * df['store_factor'] * df['weekday_factor'] * df['sincos_factor'] * df['country_factor'] * df['npdoy_factor']\n\ndf['total'] = df['num_sold'] / df['ratio']\nconst_factor = df['total'].mean() * 1.06\n\ndf['prediction'] = const_factor * df['ratio']\nmape_train = mean_absolute_percentage_error(df[(df.test == 0) & (~pd.isna(df.num_sold))].num_sold, df[(df.test == 0) & (~pd.isna(df.num_sold))].prediction)\n\nprint(f'With new npdoy_factor {mape_train=}')", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-21T20:31:36.741113Z", "iopub.execute_input": "2025-01-21T20:31:36.741366Z", "iopub.status.idle": "2025-01-21T20:31:37.10537Z", "shell.execute_reply.started": "2025-01-21T20:31:36.741344Z", "shell.execute_reply": "2025-01-21T20:31:37.104532Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "## 3.2. One improvement is rounding the prediction to the nearest integer value.", "metadata": {}}, {"cell_type": "code", "source": "df['prediction'] = np.round(df['prediction'].astype(float)).astype(int)", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-21T20:31:37.106304Z", "iopub.execute_input": "2025-01-21T20:31:37.106549Z", "iopub.status.idle": "2025-01-21T20:31:37.120441Z", "shell.execute_reply.started": "2025-01-21T20:31:37.106527Z", "shell.execute_reply": "2025-01-21T20:31:37.119653Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "## 3.3. Prepare the submission and check that it's ok", "metadata": {}}, {"cell_type": "code", "source": "submission = df[(df.test == 1)][['id', 'prediction']].rename(columns={'prediction': 'num_sold'})\nsubmission.to_csv('submission.csv', index=False)\n\n!head submission.csv", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-21T20:31:37.121531Z", "iopub.execute_input": "2025-01-21T20:31:37.121936Z", "iopub.status.idle": "2025-01-21T20:31:37.369895Z", "shell.execute_reply.started": "2025-01-21T20:31:37.121902Z", "shell.execute_reply": "2025-01-21T20:31:37.368789Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "# 4. Blending with other solutions\n\n- https://www.kaggle.com/code/act18l/hill-climbing-for-ensembling-baseline", "metadata": {}}, {"cell_type": "code", "source": "blend = pd.read_csv('./input/playground-series-s5e1/sample_submission.csv')\n\nsub1 = pd.read_csv('./input/linearregression-subtract-bias-makes-lb-0-054/submission.csv')\n\nblend['num_sold'] = (0.5 * submission['num_sold'] + 0.5 * sub1['num_sold']).round()\n\nblend.to_csv('blend.csv', index=False)\n\n!head blend.csv", "metadata": {"trusted": true}, "outputs": [], "execution_count": null}]}