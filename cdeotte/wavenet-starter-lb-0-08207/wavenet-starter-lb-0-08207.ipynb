{"metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "language_info": {"name": "python", "version": "3.10.12", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}, "kaggle": {"accelerator": "nvidiaTeslaT4", "dataSources": [{"sourceId": 85723, "databundleVersionId": 10652996, "sourceType": "competition"}, {"sourceId": 10565821, "sourceType": "datasetVersion", "datasetId": 6523667}], "dockerImageVersionId": 30636, "isInternetEnabled": false, "language": "python", "sourceType": "notebook", "isGpuEnabled": true}}, "nbformat_minor": 4, "nbformat": 4, "cells": [{"cell_type": "markdown", "source": "# WaveNet Starter \nThis notebook is a CNN WaveNet starter. The advantage of using time series deep learning (like CNN, RNN, or Transformer) is that we do not need to do feature engieering! The model does the feature engineeing for us! We input only `num_sold` and the model does the rest! Note this model has lots of room for improvement and can beat GBDT! Discussions about this notebook are [here][1] and [here][2]\n\nTo make a submission CSV, we need to run this notebook 5 times with `PROD = 0,1,2,3,4`. Each time we train a model for one specific product and predict test for one specific product. Each run loads and saves an intermediate CSV file with partial predicitons. When we run this notebook the fifth time with `PROD=4` then it will write `submission.csv` to submit to comp. \n\n# Model Details\nThis notebook uses the following techniques\n* **No feature engineering. Uses the raw `num_sold` time series data only!**\n* Train one `WaveNet` model for each of the 5 `products`.\n* Standardize all 18 time series for each `product` into the same range of values.\n* Standardize with `GDP`, `store ratio`, time series `mean`, time series `std`.\n* Use all 18 `store` and `country` time series for each `product` to learn about `product`\n* `WaveNet` takes input time series of length `1440` days and predicts `32` future days\n* Infer using `auto regression`. We use predictions to make more predictions 35 times.\n* `WaveNet` trains with `random crops` from train data\n* Train using `2x T4 GPU` and `mixed precision`\n\n# Update\nVersion 2 fixes two small bugs. The indexes in code cell #15 to reverse Kenya fudge factor were incorrectly `[3,6,12]`, they should be `[3,9,15]`. The data loader length was wrong and would give batches with size zero after the indexes ran out (in code cell #6)\n\n# Load Data\nLoad train data and define which product to train and predict.\n\n[1]: https://www.kaggle.com/competitions/playground-series-s5e1/discussion/557904\n[2]: https://www.kaggle.com/competitions/playground-series-s5e1/discussion/557425", "metadata": {}}, {"cell_type": "code", "source": "# VERSION\nVER=2\n# WHICH OF 5 PRODUCTS 0,1,2,3,4 TO TRAIN AND PREDICT\nPROD = 0\n# LENGTH OF TRAIN FEATURES\nLEN = 1440\n\n# TRAIN OR LOAD MODEL\nTRAIN_MODEL = True\nPATH = \"./input/kaggle-sticker-comp-sub-v1/\"\nUSE_INTERNET = False", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-24T03:29:54.326987Z", "iopub.execute_input": "2025-01-24T03:29:54.327369Z", "iopub.status.idle": "2025-01-24T03:29:54.337759Z", "shell.execute_reply.started": "2025-01-24T03:29:54.32734Z", "shell.execute_reply": "2025-01-24T03:29:54.336874Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "import pandas as pd, numpy as np, os\nimport matplotlib.pyplot as plt\n\ntrain = pd.read_csv(\"./input/playground-series-s5e1/train.csv\")\ntrain.date = pd.to_datetime(train.date)\nprint(\"Train shape:\", train.shape )\ntrain.head()", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-24T03:29:54.339194Z", "iopub.execute_input": "2025-01-24T03:29:54.339496Z", "iopub.status.idle": "2025-01-24T03:29:55.018474Z", "shell.execute_reply.started": "2025-01-24T03:29:54.339469Z", "shell.execute_reply": "2025-01-24T03:29:55.017496Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "# Download GDP\nWe download GDP per country as explained in discussion [here][1]\n\n[1]: https://www.kaggle.com/competitions/playground-series-s5e1/discussion/554349", "metadata": {}}, {"cell_type": "code", "source": "import requests \ndef get_gdp_per_capita(alpha3, year):\n    url='https://api.worldbank.org/v2/country/{0}/indicator/NY.GDP.PCAP.CD?date={1}&format=json'\n    response = requests.get(url.format(alpha3,year)).json()\n    return response[1][0]['value']\n\nalpha3s = ['CAN', 'FIN', 'ITA', 'KEN', 'NOR', 'SGP']\ntrain['alpha3'] = train['country'].map(dict(zip(\n    np.sort(train['country'].unique()), alpha3s)))\nyears = np.sort(train['date'].dt.year.unique())\ntrain['year'] = train['date'].dt.year\nif USE_INTERNET:\n    gdp = np.array([\n        [get_gdp_per_capita(alpha3, year) for year in years]\n        for alpha3 in alpha3s\n    ])\n    gdp = pd.DataFrame(gdp, index=alpha3s, columns=years)\nelse:\n    gdp = pd.read_csv(f\"{PATH}gdp0.csv\")\n    gdp = gdp.set_index(\"Unnamed: 0\")\n    gdp = gdp.rename(columns=lambda x: int(x))\ntrain['GDP'] = train.apply(lambda s: gdp.loc[s['alpha3'], s['year']], axis=1)\ntrain = train.drop(['alpha3','year'],axis=1)\ntrain.head()", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-24T03:29:55.020139Z", "iopub.execute_input": "2025-01-24T03:29:55.020572Z", "iopub.status.idle": "2025-01-24T03:29:58.917733Z", "shell.execute_reply.started": "2025-01-24T03:29:55.020538Z", "shell.execute_reply": "2025-01-24T03:29:58.916819Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "# Standardize with GDP and Store Ratio\nWe now divide each `num_sold` by it's countries GDP, this normalizes all the `num_sold` and makes different countries have similar values so we can use all the data together to train our model. We also normalize per store by dividing by each store's percentage of total sales.\n\nAfter these two normalizations, then all 18 country store combinations will have `num_sold` values that are similar and we can train with everything together.", "metadata": {}}, {"cell_type": "code", "source": "train[\"num_sold\"] /= train[\"GDP\"]\nstore_ratio = train.groupby(\"store\").num_sold.mean().to_dict()\ntrain[\"store_ratio\"] = train.store.map(store_ratio)\ntrain[\"num_sold\"] /= train[\"store_ratio\"]", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-24T03:29:58.918689Z", "iopub.execute_input": "2025-01-24T03:29:58.918915Z", "iopub.status.idle": "2025-01-24T03:29:58.955588Z", "shell.execute_reply.started": "2025-01-24T03:29:58.918895Z", "shell.execute_reply": "2025-01-24T03:29:58.954806Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "# Create Train Data (standardize with mean and std)\nWe will standarize all time series by subtracting mean and dividing by standard deviation for training (to get mean=0 and std=1). Additionally we multiply Kenya by 1.15 because even after GDP normalization (in above code cell), Kenya predictions are lower than other countries. \n\nLater, after making test predictions, we reverse this by dividing Kenya predictions by 1.15 in code cell #15. And we reverse standardization by multiplying by standard deviation and adding mean (which we save now for later).", "metadata": {}}, {"cell_type": "code", "source": "C = list( train.country.unique() )\nS = list( train.store.unique() )\nP = list( train[\"product\"].unique() )\nprint(\"Countries:\", C )\nprint(\"Stores:\", S )\nprint(\"Products:\", P)\n\n# DATA IS PRODUCT X 7 YEARS X STORE+COUNTRY\ndata = np.zeros( (5,2557,18) )\nfor i in range(5):\n    for j in range(3):\n        for k in range(6):\n            f = 1 \n            if k==3: f=1.15 # FUDGE FACTOR FOR KENYA\n            df = train.loc[(train.country==C[k])&(train.store==S[j])&(train[\"product\"]==P[i])].copy()\n            data[i,:,j*6+k] = df[\"num_sold\"].values*f\n\n# COMPUTE MEANS AND STDS\nmeans = {}; stds = {}\nfor k in range(5):\n    m = np.nanmean( data[k,:,:] )\n    s = np.nanstd( data[k,:,:] )\n    means[k]=m; stds[k]=s\n\n# PLOT ALL TIME SERIES DATA\nfor i in range(5):\n    plt.figure(figsize=(10,5))\n    for j in range(3):\n        for k in range(6):\n            f = 1\n            if k==3: f=1.15\n            df = train.loc[(train.country==C[k])&(train.store==S[j])&(train[\"product\"]==P[i])].copy()\n            df[\"smooth_sold\"] = df[\"num_sold\"].rolling(window=180).mean()\n            m = means[i]; s = stds[i]\n            plt.plot(df[\"date\"], (df[\"smooth_sold\"]*f-m)/s )\n            data[i,:,j*6+k] = (df[\"num_sold\"].values*f-m)/s\n    plt.title(f\"Product = {P[i]}, All 18 Country Store Pairs smoothed and standardized.\",size=10)\n    plt.show()", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-24T03:29:58.957464Z", "iopub.execute_input": "2025-01-24T03:29:58.957758Z", "iopub.status.idle": "2025-01-24T03:30:08.610605Z", "shell.execute_reply.started": "2025-01-24T03:29:58.957735Z", "shell.execute_reply": "2025-01-24T03:30:08.609795Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "# Data Loader\nWe will make a dataloader to train our WaveNet NN. This can be improved. Currently we just take 32x1024 `random crops` from the train data per epoch.", "metadata": {}}, {"cell_type": "code", "source": "import tensorflow as tf\n\nclass DataGenerator(tf.keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, data, batch_size=32, shuffle=False, product=0, f_length=768, t_length=32): \n\n        self.data = data\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.product = product\n        self.f_length = f_length\n        self.t_length = t_length\n        self.on_epoch_end()\n        \n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        ct = int(np.ceil(32*1024/self.batch_size))\n        return ct\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        X, y = self.__data_generation(indexes)\n        return X, y\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange( 32*1024 )\n        if self.shuffle: np.random.shuffle(self.indexes)\n                        \n    def __data_generation(self, indexes):\n        'Generates data containing batch_size samples' \n    \n        SIZE = self.f_length\n        TARGET = self.t_length\n        X = np.zeros((len(indexes),SIZE,1),dtype='float32')\n        y = np.zeros((len(indexes),TARGET),dtype='float32')\n        \n        for k in range(len(indexes)):\n            r = np.random.randint(0,self.data.shape[2])\n            a = np.random.randint(0,self.data.shape[1]-SIZE-TARGET)\n            X[k,:,0] = self.data[self.product,a:a+SIZE,r]\n            y[k,:] = self.data[self.product,a+SIZE:a+SIZE+TARGET,r]\n        return np.nan_to_num(X),np.nan_to_num(y)", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-24T03:30:08.611564Z", "iopub.execute_input": "2025-01-24T03:30:08.611791Z", "iopub.status.idle": "2025-01-24T03:30:19.34924Z", "shell.execute_reply.started": "2025-01-24T03:30:08.611771Z", "shell.execute_reply": "2025-01-24T03:30:19.348529Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "# DISPLAY DATA LOADER\ngen = DataGenerator(data, shuffle=False, f_length=LEN, product=PROD)\nfor x,y in gen:\n    for k in range(4):\n        plt.figure(figsize=(20,5))\n        LN = x.shape[1]\n        LN2 = y.shape[1]\n        plt.plot(np.arange(LN),x[k,:,0],label='features')\n        plt.plot(np.arange(LN2)+LN,y[k,:],label='target')\n        plt.legend()\n        plt.title(f\"Product = {P[PROD]}. Sample Dataloader.\",size=14)\n        plt.show()\n    break", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-24T03:30:19.350271Z", "iopub.execute_input": "2025-01-24T03:30:19.350737Z", "iopub.status.idle": "2025-01-24T03:30:20.41535Z", "shell.execute_reply.started": "2025-01-24T03:30:19.350714Z", "shell.execute_reply": "2025-01-24T03:30:20.414457Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "# Train Schedule\nWe train 5 epochs with learning rates 1e-3, 1e-3, 1e-4, 1e-4, 1e-5.", "metadata": {}}, {"cell_type": "code", "source": "# TRAIN SCHEDULE\ndef lrfn(epoch):\n        return [1e-3,1e-3,1e-4,1e-4,1e-5][epoch]\nLR = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\nEPOCHS = 5", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-24T03:30:20.416382Z", "iopub.execute_input": "2025-01-24T03:30:20.416631Z", "iopub.status.idle": "2025-01-24T03:30:20.421104Z", "shell.execute_reply.started": "2025-01-24T03:30:20.41661Z", "shell.execute_reply": "2025-01-24T03:30:20.420299Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "# Build WaveNet Model\nWe build WaveNet model copied from my previous notebook [here][1]. WaveNet uses dilated convolutions to detect different sine wave frequencies in time series.\n\n[1]: https://www.kaggle.com/code/cdeotte/wavenet-starter-lb-0-52", "metadata": {}}, {"cell_type": "code", "source": "from tensorflow.keras.layers import Input, Dense, Multiply, Add, Conv1D, Concatenate\n\ndef wave_block(x, filters, kernel_size, n):\n    dilation_rates = [2**i for i in range(n)]\n    x = Conv1D(filters = filters,\n               kernel_size = 1,\n               padding = 'same')(x)\n    res_x = x\n    for dilation_rate in dilation_rates:\n        tanh_out = Conv1D(filters = filters,\n                          kernel_size = kernel_size,\n                          padding = 'same', \n                          activation = 'tanh', \n                          dilation_rate = dilation_rate)(x)\n        sigm_out = Conv1D(filters = filters,\n                          kernel_size = kernel_size,\n                          padding = 'same',\n                          activation = 'sigmoid', \n                          dilation_rate = dilation_rate)(x)\n        x = Multiply()([tanh_out, sigm_out])\n        x = Conv1D(filters = filters,\n                   kernel_size = 1,\n                   padding = 'same')(x)\n        res_x = Add()([res_x, x])\n    return res_x", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-24T03:30:20.421963Z", "iopub.execute_input": "2025-01-24T03:30:20.422197Z", "iopub.status.idle": "2025-01-24T03:30:20.437264Z", "shell.execute_reply.started": "2025-01-24T03:30:20.422177Z", "shell.execute_reply": "2025-01-24T03:30:20.436417Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "def build_model():\n        \n    # INPUT \n    inp = tf.keras.Input(shape=(LEN,1))\n    x = wave_block(inp, 8, 3, 12)\n    x = wave_block(x, 16, 3, 8)\n    x = wave_block(x, 32, 3, 4)\n    x = wave_block(x, 64, 3, 1)\n    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n    x = tf.keras.layers.Dense(64, activation='relu')(x)\n    x = tf.keras.layers.Dense(32,activation='linear', dtype='float32')(x)\n    \n    # COMPILE MODEL\n    model = tf.keras.Model(inputs=inp, outputs=x)\n    opt = tf.keras.optimizers.Adam(learning_rate = 1e-3)\n    loss = tf.keras.losses.MeanSquaredError()\n    model.compile(loss=loss, optimizer = opt)\n    \n    return model", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-24T03:30:20.438253Z", "iopub.execute_input": "2025-01-24T03:30:20.43855Z", "iopub.status.idle": "2025-01-24T03:30:20.447561Z", "shell.execute_reply.started": "2025-01-24T03:30:20.438522Z", "shell.execute_reply": "2025-01-24T03:30:20.446731Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "# Initialize GPUs\nWe will use both 2x T4 Kaggle GPUs. And we will enable mixed precision. This will speed up training.", "metadata": {}}, {"cell_type": "code", "source": "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\nimport tensorflow as tf\nprint('TensorFlow version =',tf.__version__)\n\n# USE MULTIPLE GPUS\ngpus = tf.config.list_physical_devices('GPU')\nif len(gpus)<=1: \n    strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n    print(f'Using {len(gpus)} GPU')\nelse: \n    strategy = tf.distribute.MirroredStrategy()\n    print(f'Using {len(gpus)} GPUs')", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-24T03:30:20.44968Z", "iopub.execute_input": "2025-01-24T03:30:20.449907Z", "iopub.status.idle": "2025-01-24T03:30:21.856924Z", "shell.execute_reply.started": "2025-01-24T03:30:20.449888Z", "shell.execute_reply": "2025-01-24T03:30:21.856131Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "# USE MIXED PRECISION\nMIX = True\nif MIX:\n    tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\n    print('Mixed precision enabled')\nelse:\n    print('Using full precision')", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-24T03:30:21.857993Z", "iopub.execute_input": "2025-01-24T03:30:21.858351Z", "iopub.status.idle": "2025-01-24T03:30:21.86387Z", "shell.execute_reply.started": "2025-01-24T03:30:21.858319Z", "shell.execute_reply": "2025-01-24T03:30:21.862925Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "# Train Model\nWe train without proper validation. The validation data overlaps the train data. We need to update this to proper validation. One way to create a non-overlapping train and valid data is to use 6 KFolds. During each of the 6 folds, we train with 5 countries and validate on the other 1 country. This is not a time series cross validation but it could work here and allow us to use maximum time data to train our models.", "metadata": {}}, {"cell_type": "code", "source": "train_gen = DataGenerator(data, shuffle=True, batch_size=64, f_length=LEN, product=PROD)\nvalid_gen = DataGenerator(data, shuffle=False, batch_size=128, f_length=LEN, product=PROD)\n\nwith strategy.scope():\n    model = build_model()\nif TRAIN_MODEL:\n    model.fit(train_gen, verbose=1,\n          validation_data = valid_gen,\n          epochs=EPOCHS, callbacks = [LR])\n    model.save_weights(f'model_v{VER}_p{PROD}.h5')\nelse:\n    model.load_weights(f'{PATH}model_v{VER}_p{PROD}.h5')", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-24T03:30:21.864807Z", "iopub.execute_input": "2025-01-24T03:30:21.865046Z", "iopub.status.idle": "2025-01-24T03:33:44.992716Z", "shell.execute_reply.started": "2025-01-24T03:30:21.865027Z", "shell.execute_reply": "2025-01-24T03:33:44.992013Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "# Predict Test\nWe predict and display predictions. We iterate over each country store pair. Train data was standardized so test predictions will be standarized. We will \"un-standardize\" test predictions below.\n\nWe predict using `auto regression`. Our model predicts 32 days into the future. Then we use those predictions together with train data to predict another 32 days into the future. We repeat this process 35 times to predict 3 years into the future.", "metadata": {}}, {"cell_type": "code", "source": "preds = np.zeros((18,32*35))\n\n# ITERATE OVER ALL COMBINATIONS OF COUNTRY AND STORE FOR SPECIFIC PRODUCT\n# PREDICT 3 YEARS INTO THE FUTURE\n\nbad_rows = []\nfor jj in range(18):\n    ddd0 = data[PROD:PROD+1,-LEN:,jj:jj+1].copy()\n    if np.isnan(ddd0).sum()==LEN:\n        bad_rows.append(jj)\n    \n    pp = []\n    for j in range(0,35):\n        print(j,\", \",end=\"\")\n        if j==0: dd2 = ddd0\n        else: dd2 = np.concatenate([ddd0[:,32*j:,:]]+[z.reshape((1,32,1)) for z in pp],axis=1) \n        p2 = model.predict( np.nan_to_num(dd2[:,-LEN:,:]) ,verbose=0)\n        pp.append(p2)\n        if j==34:\n            print()\n            plt.figure(figsize=(20,5))\n            plt.plot(np.arange(LEN), np.nan_to_num(ddd0[0,:,0]) )\n            for k in range(j+1):\n                plt.plot(np.arange(32)+LEN+32*k,pp[k][0,:])\n            cc = C[jj%6]\n            ss = S[jj//6]\n            plt.title(f\"Product={P[PROD]}, Country={cc}, Store={ss}\",size=16)\n            plt.show()\n            \n    preds[jj,:] = np.concatenate([z.reshape((1,32,1)) for z in pp],axis=1).flatten() ", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-24T03:33:44.993775Z", "iopub.execute_input": "2025-01-24T03:33:44.994024Z", "iopub.status.idle": "2025-01-24T03:35:37.322344Z", "shell.execute_reply.started": "2025-01-24T03:33:44.994003Z", "shell.execute_reply": "2025-01-24T03:35:37.321451Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "# Reverse Standardize\nWe standarized the train data by minus mean divide std. So now before we can submit predictions, we need to reverse this with multiply std and add mean to preds. Also we need to reverse our Kenya fudge factor. When all train features are NAN, we convert predictions to the average prediction.", "metadata": {}}, {"cell_type": "code", "source": "# FILLNAN PREDS\nFILLNAN = np.nanmean(preds,axis=0)\nfor r in bad_rows:\n    preds[r,:] = FILLNAN\n\n# REVERSE STANDARIZE PREDICTIONS AND FIX KENYA\npreds = (preds*stds[PROD])+means[PROD]\nfor i in [3,9,15]: preds[i,:] = preds[i,:]/1.15", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-24T03:35:37.323388Z", "iopub.execute_input": "2025-01-24T03:35:37.323642Z", "iopub.status.idle": "2025-01-24T03:35:37.330293Z", "shell.execute_reply.started": "2025-01-24T03:35:37.323621Z", "shell.execute_reply": "2025-01-24T03:35:37.329377Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "# Create Submission File", "metadata": {}}, {"cell_type": "code", "source": "if PROD==0:\n    test = pd.read_csv(\"./input/playground-series-s5e1/test.csv\")\n    test.date = pd.to_datetime(test.date)\n    test['alpha3'] = test['country'].map(dict(zip(\n        np.sort(test['country'].unique()), alpha3s)))\n    years = np.sort(test['date'].dt.year.unique())\n    test['year'] = test['date'].dt.year\n    if USE_INTERNET:\n        gdp = np.array([\n            [get_gdp_per_capita(alpha3, year) for year in years]\n            for alpha3 in alpha3s\n        ])\n        gdp1 = pd.DataFrame(gdp, index=alpha3s, columns=years)\n    else:\n        gdp1 = pd.read_csv(f\"{PATH}gdp1.csv\")\n        gdp1 = gdp1.set_index(\"Unnamed: 0\")\n        gdp1 = gdp1.rename(columns=lambda x: int(x))\n    test['GDP'] = test.apply(lambda s: gdp1.loc[s['alpha3'], s['year']], axis=1)\n    test[\"num_sold\"] = 0.0\nelse:\n    test = pd.read_csv(f\"test_v{VER}_p{PROD-1}.csv\")\n    test.date = pd.to_datetime(test.date)", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-24T03:35:37.331614Z", "iopub.execute_input": "2025-01-24T03:35:37.331825Z", "iopub.status.idle": "2025-01-24T03:35:39.047397Z", "shell.execute_reply.started": "2025-01-24T03:35:37.331807Z", "shell.execute_reply": "2025-01-24T03:35:39.046601Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "# Reverse GDP, Store Ratio\nWe divided train by GDP and divided train by Store Ratio, so we need to reverse this and multiple test by GDP and multiply test by Store Ratio", "metadata": {}}, {"cell_type": "code", "source": "for i in range(3):\n    for j in range(6):\n        test.loc[(test['product']==P[PROD])&(test.store==S[i])&(test.country==C[j]),'num_sold'] =\\\n            preds[i*6+j,:1095]\ntest[\"store_ratio\"] = test.store.map(store_ratio)\ntest.loc[test['product']==P[PROD],\"num_sold\"] =\\\n    test.loc[test['product']==P[PROD],\"num_sold\"] * test.loc[test['product']==P[PROD],\"GDP\"]\ntest.loc[test['product']==P[PROD],\"num_sold\"] =\\\n    test.loc[test['product']==P[PROD],\"num_sold\"] * test.loc[test['product']==P[PROD],\"store_ratio\"]\n\nprint( test.shape )\ndisplay( test.head() )", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-24T03:35:39.048461Z", "iopub.execute_input": "2025-01-24T03:35:39.048794Z", "iopub.status.idle": "2025-01-24T03:35:39.454834Z", "shell.execute_reply.started": "2025-01-24T03:35:39.04876Z", "shell.execute_reply": "2025-01-24T03:35:39.453951Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "# Display Test Predictions\nThese are train and test data without any standardization.", "metadata": {}}, {"cell_type": "code", "source": "for ss in range(3):\n    for cc in range(6):\n        df1 = train.loc[(train.country==C[cc])&(train['product']==P[PROD])&(train.store==S[ss])]\n        df2 = test.loc[(test.country==C[cc])&(test['product']==P[PROD])&(test.store==S[ss])]\n\n        plt.figure(figsize=(20,5))\n        tmp = df1.num_sold * df1.GDP * df1.store_ratio\n        tmp.iloc[0] = np.nan_to_num( tmp.iloc[0] )\n        plt.plot(np.arange(len(df1)), tmp )\n        plt.plot(np.arange(len(df2))+len(df1),df2.num_sold)\n        plt.title(f\"Product={P[PROD]}, Country={cc}, Store={ss}\",size=14)\n        plt.show()", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-24T03:35:39.456117Z", "iopub.execute_input": "2025-01-24T03:35:39.45702Z", "iopub.status.idle": "2025-01-24T03:35:45.401972Z", "shell.execute_reply.started": "2025-01-24T03:35:39.456983Z", "shell.execute_reply": "2025-01-24T03:35:45.40107Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "# Write Partial Submission CSV\nWe run this notebook 5 times with `PROD = 0,1,2,3,4`. The first 4 times write intermediate files. The last 5th time will write our `submission.csv` file to submit to competition.", "metadata": {}}, {"cell_type": "code", "source": "# DELETE THIS IF YOU RUN THIS NOTEBOOK 5 TIMES AND MAKE YOUR OWN SUBMISSION.CSV\nif PROD!=4:\n    os.system(f\"cp {PATH}submission_v2.csv submission_v2.csv\")", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-24T03:35:45.402941Z", "iopub.execute_input": "2025-01-24T03:35:45.403172Z", "iopub.status.idle": "2025-01-24T03:35:45.446681Z", "shell.execute_reply.started": "2025-01-24T03:35:45.403152Z", "shell.execute_reply": "2025-01-24T03:35:45.445989Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "if PROD<4:\n    test.to_csv(f\"test_v{VER}_p{PROD}.csv\",index=False)\n    print(f\"Saved partial predictions for product {P[PROD]} (PROD = {PROD})\")\n    print(f\"Now run this notebook again with PROD = {PROD+1} to make more predictions.\")\nelse:\n    test[['id','num_sold']].to_csv(f\"submission_v{VER}.csv\",index=False)\n    print(f\"Wrote submission_v{VER}.csv, now submit to comp!\")\ntest[['id','num_sold']].head()", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-24T03:35:45.447559Z", "iopub.execute_input": "2025-01-24T03:35:45.447769Z", "iopub.status.idle": "2025-01-24T03:35:46.126238Z", "shell.execute_reply.started": "2025-01-24T03:35:45.447751Z", "shell.execute_reply": "2025-01-24T03:35:46.125405Z"}}, "outputs": [], "execution_count": null}]}