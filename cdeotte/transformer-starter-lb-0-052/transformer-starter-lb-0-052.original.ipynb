{"metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "language_info": {"name": "python", "version": "3.10.12", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}, "kaggle": {"accelerator": "nvidiaTeslaT4", "dataSources": [{"sourceId": 85723, "databundleVersionId": 10652996, "sourceType": "competition"}, {"sourceId": 10566196, "sourceType": "datasetVersion", "datasetId": 6538310}], "dockerImageVersionId": 30840, "isInternetEnabled": false, "language": "python", "sourceType": "notebook", "isGpuEnabled": true}}, "nbformat_minor": 4, "nbformat": 4, "cells": [{"cell_type": "markdown", "source": "# Transformer Starter - LB 0.052 - No Feature Engineering\nThis notebook is a Transformer starter (built upon my WaveNet starter [here][1]). The advantage of using time series deep learning (like CNN, RNN, or Transformer) is that we do not need to do feature engieering! The model does the feature engineeing for us! We input only `num_sold` and the model does the rest! This Transformer model is as accurate as GBDT (like XGB, CAT, LGBM) all on it's own without us engineering any features! Wow!\n\nTo make a submission CSV, we need to run this notebook 5 times with `PROD = 0,1,2,3,4`. Each time we train a model for one specific product and predict test for one specific product. Each run loads and saves an intermediate CSV file with partial predicitons. When we run this notebook the fifth time with `PROD=4` then it will write `submission.csv` to submit to comp. \n\n# New Transformer Model Details\nThis notebook is a fork of my WaveNet starter [here][1]. Then I made the following changes:\n* Replaced WaveNet with Transformer\n* Use WaveNet to extract features for Transformer\n* Data loader avoids outputting samples with NAN in target\n* Train data adds a boolean indicating whether `num_sold` is NAN or not\n* Train for 10 epochs using cosine learning schedule\n* Use TensorFlow version 2.17.0 instead of 2.13.0\n\n# Old WaveNet Model Details\nThis notebook uses the following techniques from my WaveNet starter:\n* **No feature engineering. Uses the raw `num_sold` time series data only!**\n* Train one `Transformer` model for each of the 5 `products`.\n* Standardize all 18 time series for each `product` into the same range of values.\n* Standardize with `GDP`, `store ratio`, time series `mean`, time series `std`.\n* Use all 18 `store` and `country` time series for each `product` to learn about `product`\n* `Transformer` takes input time series of length `1440` days and predicts `32` future days\n* Infer using `auto regression`. We use predictions to make more predictions 35 times.\n* `Transformer` trains with `random crops` from train data\n* Train using `2x T4 GPU` and `mixed precision`\n\n# Load Data\nLoad train data and define which product to train and predict.\n\n[1]: https://www.kaggle.com/code/cdeotte/wavenet-starter-lb-0-08207", "metadata": {"_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5", "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}}, {"cell_type": "code", "source": "# VERSION\nVER=1\n# WHICH OF 5 PRODUCTS 0,1,2,3,4 TO TRAIN AND PREDICT\nPROD = 0\n# LENGTH OF TRAIN FEATURES\nLEN = 1440\n\n# TRAIN OR LOAD MODEL\nTRAIN_MODEL = True\nPATH = \"./input/kaggle-sticker-comp-sub-v2/\"\nUSE_INTERNET = False", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-24T05:10:59.431216Z", "iopub.execute_input": "2025-01-24T05:10:59.431533Z", "iopub.status.idle": "2025-01-24T05:10:59.436096Z", "shell.execute_reply.started": "2025-01-24T05:10:59.431491Z", "shell.execute_reply": "2025-01-24T05:10:59.435133Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "import pandas as pd, numpy as np, os\nimport matplotlib.pyplot as plt\n\ntrain = pd.read_csv(\"./input/playground-series-s5e1/train.csv\")\ntrain.date = pd.to_datetime(train.date)\nprint(\"Train shape:\", train.shape )\ntrain.head()", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-24T05:10:59.437004Z", "iopub.execute_input": "2025-01-24T05:10:59.437315Z", "iopub.status.idle": "2025-01-24T05:11:01.145835Z", "shell.execute_reply.started": "2025-01-24T05:10:59.437287Z", "shell.execute_reply": "2025-01-24T05:11:01.14486Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "# Download GDP\nWe download GDP per country as explained in discussion [here][1]\n\n[1]: https://www.kaggle.com/competitions/playground-series-s5e1/discussion/554349", "metadata": {}}, {"cell_type": "code", "source": "import requests \ndef get_gdp_per_capita(alpha3, year):\n    url='https://api.worldbank.org/v2/country/{0}/indicator/NY.GDP.PCAP.CD?date={1}&format=json'\n    response = requests.get(url.format(alpha3,year)).json()\n    return response[1][0]['value']\n\nalpha3s = ['CAN', 'FIN', 'ITA', 'KEN', 'NOR', 'SGP']\ntrain['alpha3'] = train['country'].map(dict(zip(\n    np.sort(train['country'].unique()), alpha3s)))\nyears = np.sort(train['date'].dt.year.unique())\ntrain['year'] = train['date'].dt.year\nif USE_INTERNET:\n    gdp = np.array([\n        [get_gdp_per_capita(alpha3, year) for year in years]\n        for alpha3 in alpha3s\n    ])\n    gdp = pd.DataFrame(gdp, index=alpha3s, columns=years)\nelse:\n    gdp = pd.read_csv(f\"{PATH}gdp0.csv\")\n    gdp = gdp.set_index(\"Unnamed: 0\")\n    gdp = gdp.rename(columns=lambda x: int(x))\ntrain['GDP'] = train.apply(lambda s: gdp.loc[s['alpha3'], s['year']], axis=1)\ntrain = train.drop(['alpha3','year'],axis=1)\ntrain.head()", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-24T05:11:01.146525Z", "iopub.execute_input": "2025-01-24T05:11:01.146752Z", "iopub.status.idle": "2025-01-24T05:11:05.439776Z", "shell.execute_reply.started": "2025-01-24T05:11:01.146732Z", "shell.execute_reply": "2025-01-24T05:11:05.438828Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "# Standardize with GDP and Store Ratio\nWe now divide each `num_sold` by it's countries GDP, this normalizes all the `num_sold` and makes different countries have similar values so we can use all the data together to train our model. We also normalize per store by dividing by each store's percentage of total sales.\n\nAfter these two normalizations, then all 18 country store combinations will have `num_sold` values that are similar and we can train with everything together.", "metadata": {}}, {"cell_type": "code", "source": "train[\"num_sold\"] /= train[\"GDP\"]\nstore_ratio = train.groupby(\"store\").num_sold.mean().to_dict()\ntrain[\"store_ratio\"] = train.store.map(store_ratio)\ntrain[\"num_sold\"] /= train[\"store_ratio\"]", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-24T05:11:05.442027Z", "iopub.execute_input": "2025-01-24T05:11:05.442303Z", "iopub.status.idle": "2025-01-24T05:11:05.502148Z", "shell.execute_reply.started": "2025-01-24T05:11:05.442272Z", "shell.execute_reply": "2025-01-24T05:11:05.501519Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "# Create Train Data (standardize with mean and std)\nWe will standarize all time series by subtracting mean and dividing by standard deviation for training (to get mean=0 and std=1). Additionally we multiply Kenya by 1.15 because even after GDP normalization (in above code cell), Kenya predictions are lower than other countries. \n\nLater, after making test predictions, we reverse this by dividing Kenya predictions by 1.15 in code cell #15. And we reverse standardization by multiplying by standard deviation and adding mean (which we save now for later).", "metadata": {}}, {"cell_type": "code", "source": "C = list( train.country.unique() )\nS = list( train.store.unique() )\nP = list( train[\"product\"].unique() )\nprint(\"Countries:\", C )\nprint(\"Stores:\", S )\nprint(\"Products:\", P)\n\n# DATA IS PRODUCT X 7 YEARS X STORE+COUNTRY\ndata = np.zeros( (5,2557,18) )\nfor i in range(5):\n    for j in range(3):\n        for k in range(6):\n            f = 1 \n            if k==3: f=1.15 # FUDGE FACTOR FOR KENYA\n            df = train.loc[(train.country==C[k])&(train.store==S[j])&(train[\"product\"]==P[i])].copy()\n            data[i,:,j*6+k] = df[\"num_sold\"].values*f\n\n# COMPUTE MEANS AND STDS\nmeans = {}; stds = {}\nfor k in range(5):\n    m = np.nanmean( data[k,:,:] )\n    s = np.nanstd( data[k,:,:] )\n    means[k]=m; stds[k]=s\n\n# PLOT ALL TIME SERIES DATA\nfor i in range(5):\n    plt.figure(figsize=(10,5))\n    for j in range(3):\n        for k in range(6):\n            f = 1\n            if k==3: f=1.15\n            df = train.loc[(train.country==C[k])&(train.store==S[j])&(train[\"product\"]==P[i])].copy()\n            df[\"smooth_sold\"] = df[\"num_sold\"].rolling(window=180).mean()\n            m = means[i]; s = stds[i]\n            plt.plot(df[\"date\"], (df[\"smooth_sold\"]*f-m)/s )\n            data[i,:,j*6+k] = (df[\"num_sold\"].values*f-m)/s\n    plt.title(f\"Product = {P[i]}, All 18 Country Store Pairs smoothed and standardized.\",size=10)\n    plt.show()", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-24T05:11:05.503608Z", "iopub.execute_input": "2025-01-24T05:11:05.503926Z", "iopub.status.idle": "2025-01-24T05:11:15.469622Z", "shell.execute_reply.started": "2025-01-24T05:11:05.503896Z", "shell.execute_reply": "2025-01-24T05:11:15.468776Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "# Data Loader\nWe will make a dataloader to train our Transformer NN. This can be improved. Currently we just take 32x1024 `random crops` from the train data per epoch. We avoid outputting samples where target contains NAN.", "metadata": {}}, {"cell_type": "code", "source": "import tensorflow as tf\n\nclass DataGenerator(tf.keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, data, batch_size=32, shuffle=False, product=0, f_length=768, t_length=32): \n\n        self.data = np.expand_dims(data,axis=-1)\n        nans = np.isnan(self.data).astype('float32')\n        self.data = np.concatenate([self.data,nans],axis=-1)\n        \n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.product = product\n        self.f_length = f_length\n        self.t_length = t_length\n        self.on_epoch_end()\n        \n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        ct = int(np.ceil(32*1024/self.batch_size))\n        return ct\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        X, y = self.__data_generation(indexes)\n        return X, y\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange( 32*1024 )\n        if self.shuffle: np.random.shuffle(self.indexes)\n                        \n    def __data_generation(self, indexes):\n        'Generates data containing batch_size samples' \n    \n        SIZE = self.f_length\n        TARGET = self.t_length\n        X = np.zeros((len(indexes),SIZE,2),dtype='float32')\n        y = np.zeros((len(indexes),TARGET),dtype='float32')\n        \n        for k in range(len(indexes)):\n            r = np.random.randint(0,self.data.shape[2])\n            a = np.random.randint(0,self.data.shape[1]-SIZE-TARGET)\n            y[k,:] = self.data[self.product,a+SIZE:a+SIZE+TARGET,r,0]\n            while np.isnan(y[k,:]).sum()>0:\n                r = np.random.randint(0,self.data.shape[2])\n                a = np.random.randint(0,self.data.shape[1]-SIZE-TARGET)\n                y[k,:] = self.data[self.product,a+SIZE:a+SIZE+TARGET,r,0]\n            X[k,:,:] = self.data[self.product,a:a+SIZE,r,:]\n\n        return np.nan_to_num(X),np.nan_to_num(y)", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-24T05:11:15.470503Z", "iopub.execute_input": "2025-01-24T05:11:15.470872Z", "iopub.status.idle": "2025-01-24T05:11:28.50883Z", "shell.execute_reply.started": "2025-01-24T05:11:15.470836Z", "shell.execute_reply": "2025-01-24T05:11:28.508161Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "# DISPLAY DATA LOADER\ngen = DataGenerator(data, shuffle=False, f_length=LEN, product=PROD)\nfor x,y in gen:\n    for k in range(4):\n        plt.figure(figsize=(20,5))\n        LN = x.shape[1]\n        LN2 = y.shape[1]\n        plt.plot(np.arange(LN),x[k,:,0],label='features')\n        plt.plot(np.arange(LN2)+LN,y[k,:],label='target')\n        plt.legend()\n        plt.title(f\"Product = {P[PROD]}. Sample Dataloader.\",size=14)\n        plt.show()\n    break", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-24T05:11:28.509582Z", "iopub.execute_input": "2025-01-24T05:11:28.510107Z", "iopub.status.idle": "2025-01-24T05:11:29.486851Z", "shell.execute_reply.started": "2025-01-24T05:11:28.510084Z", "shell.execute_reply": "2025-01-24T05:11:29.485997Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "# Train Schedule\nWe use cosine learning schedule and train our transformer for 10 epochs.", "metadata": {}}, {"cell_type": "code", "source": "import math, numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nLR_START = 1e-6\nLR_MAX = 1e-3\nLR_MIN = 1e-6\nLR_RAMPUP_EPOCHS = 0\nLR_SUSTAIN_EPOCHS = 0\nEPOCHS = 10\n\ndef lrfn(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        decay_total_epochs = EPOCHS - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS - 1\n        decay_epoch_index = epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS\n        phase = math.pi * decay_epoch_index / decay_total_epochs\n        cosine_decay = 0.5 * (1 + math.cos(phase))\n        lr = (LR_MAX - LR_MIN) * cosine_decay + LR_MIN\n    return lr\n\nrng = [i for i in range(EPOCHS)]\nlr_y = [lrfn(x) for x in rng]\nplt.figure(figsize=(10, 4))\nplt.plot(rng, lr_y, '-o')\nplt.xlabel('Epoch'); plt.ylabel('LR')\nprint(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\". \\\n      format(lr_y[0], max(lr_y), lr_y[-1]))\nLR = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-24T05:14:35.794055Z", "iopub.execute_input": "2025-01-24T05:14:35.794403Z", "iopub.status.idle": "2025-01-24T05:14:35.961865Z", "shell.execute_reply.started": "2025-01-24T05:14:35.794373Z", "shell.execute_reply": "2025-01-24T05:14:35.960767Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "# Build Transformer Layers\nThis code is from my gold medal solution to Brain comp [here][1] with discussion [here][2]\n\n[1]: https://www.kaggle.com/code/cdeotte/11th-place-gold-cv-835-public-lb-788\n[2]: https://www.kaggle.com/competitions/child-mind-institute-detect-sleep-states/discussion/459596", "metadata": {}}, {"cell_type": "code", "source": "import tensorflow.keras.backend as K\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nprint(\"Tensorflow version\",tf.__version__)", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-24T05:11:29.657833Z", "iopub.execute_input": "2025-01-24T05:11:29.65808Z", "iopub.status.idle": "2025-01-24T05:11:29.665236Z", "shell.execute_reply.started": "2025-01-24T05:11:29.658058Z", "shell.execute_reply": "2025-01-24T05:11:29.664564Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "class TransformerBlock(layers.Layer):\n    def __init__(self, embed_dim, feat_dim, num_heads, ff_dim, rate=0.1):\n        super(TransformerBlock, self).__init__()\n        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = keras.Sequential(\n            [layers.Dense(ff_dim, activation=\"gelu\"), layers.Dense(feat_dim)]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs, training=None):  \n        attn_output = self.att(inputs, inputs, training=training)  \n        attn_output = self.dropout1(attn_output, training=training)  \n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)  \n        return self.layernorm2(out1 + ffn_output)", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-24T05:11:29.66606Z", "iopub.execute_input": "2025-01-24T05:11:29.666387Z", "iopub.status.idle": "2025-01-24T05:11:29.680879Z", "shell.execute_reply.started": "2025-01-24T05:11:29.666356Z", "shell.execute_reply": "2025-01-24T05:11:29.680193Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "def positional_encoding(maxlen, num_hid):\n        depth = num_hid/2\n        positions = tf.range(maxlen, dtype = tf.float32)[..., tf.newaxis]\n        depths = tf.range(depth, dtype = tf.float32)[np.newaxis, :]/depth\n        angle_rates = tf.math.divide(1, tf.math.pow(tf.cast(10000, tf.float32), depths))\n        angle_rads = tf.linalg.matmul(positions, angle_rates)\n        pos_encoding = tf.concat(\n          [tf.math.sin(angle_rads), tf.math.cos(angle_rads)],\n          axis=-1)\n        return pos_encoding", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-24T05:11:29.681629Z", "iopub.execute_input": "2025-01-24T05:11:29.681836Z", "iopub.status.idle": "2025-01-24T05:11:29.701272Z", "shell.execute_reply.started": "2025-01-24T05:11:29.681817Z", "shell.execute_reply": "2025-01-24T05:11:29.700603Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "# Build WaveNet Layers\nWe will use WaveNet layers to extract features for our transformer layers. This code is from my gold medal solution to Brain comp [here][1] with discussion [here][2]\n\n[1]: https://www.kaggle.com/code/cdeotte/11th-place-gold-cv-835-public-lb-788\n[2]: https://www.kaggle.com/competitions/child-mind-institute-detect-sleep-states/discussion/459596", "metadata": {}}, {"cell_type": "code", "source": "from tensorflow.keras.layers import Input, Dense, Multiply, Add, Conv1D, Concatenate\n\ndef wave_block(x, filters, kernel_size, n):\n    dilation_rates = [2**i for i in range(n)]\n    x = Conv1D(filters = filters,\n               kernel_size = 1,\n               padding = 'same')(x)\n    res_x = x\n    for dilation_rate in dilation_rates:\n        tanh_out = Conv1D(filters = filters,\n                          kernel_size = kernel_size,\n                          padding = 'same', \n                          activation = 'tanh', \n                          dilation_rate = dilation_rate)(x)\n        sigm_out = Conv1D(filters = filters,\n                          kernel_size = kernel_size,\n                          padding = 'same',\n                          activation = 'sigmoid', \n                          dilation_rate = dilation_rate)(x)\n        x = Multiply()([tanh_out, sigm_out])\n        x = Conv1D(filters = filters,\n                   kernel_size = 1,\n                   padding = 'same')(x)\n        res_x = Add()([res_x, x])\n    return res_x", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-24T05:11:29.702131Z", "iopub.execute_input": "2025-01-24T05:11:29.702465Z", "iopub.status.idle": "2025-01-24T05:11:29.719029Z", "shell.execute_reply.started": "2025-01-24T05:11:29.702434Z", "shell.execute_reply": "2025-01-24T05:11:29.718288Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "# Build Transformer Model\nThis model is based on my model from gold medal solution to Brain comp [here][1] with discussion [here][2]\n\n[1]: https://www.kaggle.com/code/cdeotte/11th-place-gold-cv-835-public-lb-788\n[2]: https://www.kaggle.com/competitions/child-mind-institute-detect-sleep-states/discussion/459596", "metadata": {}}, {"cell_type": "code", "source": "feat_dim = 16 \nembed_dim = 32  \nnum_heads = 4  \nff_dim = 64\ndropout_rate = 0.0\nnum_blocks = 3\n\ndef build_model():\n        \n    # INPUT \n    inp = tf.keras.Input(shape=(LEN,2))\n    \n    # POSITIONAL ENCODING\n    x = layers.Dense(feat_dim)(inp)\n    p = positional_encoding(1440,feat_dim)\n    x = x + p\n    \n    # THREE BLOCKS of WAVENET and TRANSFORMER\n    for k in range(num_blocks):\n        skip = x\n        x = wave_block(x, feat_dim, 3, 12)\n        x = wave_block(x, feat_dim, 3, 12)\n        x = TransformerBlock(embed_dim, feat_dim, num_heads, ff_dim, dropout_rate)(x)\n        x = 0.9*x + 0.1*skip \n\n    # HEAD\n    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n    x = tf.keras.layers.Dense(64, activation='relu')(x)\n    x = tf.keras.layers.Dense(32,activation='linear', dtype='float32')(x)\n    \n    # COMPILE MODEL\n    model = tf.keras.Model(inputs=inp, outputs=x)\n    opt = tf.keras.optimizers.Adam(learning_rate = 1e-3)\n    loss = tf.keras.losses.MeanSquaredError()\n    model.compile(loss=loss, optimizer = opt)\n    \n    return model", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-24T05:11:29.722216Z", "iopub.execute_input": "2025-01-24T05:11:29.722465Z", "iopub.status.idle": "2025-01-24T05:11:29.738737Z", "shell.execute_reply.started": "2025-01-24T05:11:29.722447Z", "shell.execute_reply": "2025-01-24T05:11:29.737952Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "# Initialize GPUs\nWe will use both 2x T4 Kaggle GPUs. And we will enable mixed precision. This will speed up training.", "metadata": {}}, {"cell_type": "code", "source": "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\nimport tensorflow as tf\nprint('TensorFlow version =',tf.__version__)\n\n# USE MULTIPLE GPUS\ngpus = tf.config.list_physical_devices('GPU')\nif len(gpus)<=1: \n    strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n    print(f'Using {len(gpus)} GPU')\nelse: \n    strategy = tf.distribute.MirroredStrategy()\n    print(f'Using {len(gpus)} GPUs')", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-24T05:11:29.73975Z", "iopub.execute_input": "2025-01-24T05:11:29.739947Z", "iopub.status.idle": "2025-01-24T05:11:30.952882Z", "shell.execute_reply.started": "2025-01-24T05:11:29.739929Z", "shell.execute_reply": "2025-01-24T05:11:30.951957Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "# USE MIXED PRECISION\nfrom tensorflow.keras.mixed_precision import set_global_policy\n\npolicy = 'mixed_float16'\nset_global_policy(policy)\nprint(f\"Mixed precision policy set to: {policy}\")", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-24T05:11:30.953804Z", "iopub.execute_input": "2025-01-24T05:11:30.954127Z", "iopub.status.idle": "2025-01-24T05:11:30.960758Z", "shell.execute_reply.started": "2025-01-24T05:11:30.954094Z", "shell.execute_reply": "2025-01-24T05:11:30.959959Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "# Train Model\nWe train without proper validation. The validation data overlaps the train data. We need to update this to proper validation. One way to create a non-overlapping train and valid data is to use 6 KFolds. During each of the 6 folds, we train with 5 countries and validate on the other 1 country. This is not a time series cross validation but it could work here and allow us to use maximum time data to train our models.", "metadata": {}}, {"cell_type": "code", "source": "os.makedirs(f'models_v{VER}', exist_ok=True)\n\ntrain_gen = DataGenerator(data, shuffle=True, batch_size=64, f_length=LEN, product=PROD)\nvalid_gen = DataGenerator(data, shuffle=False, batch_size=128, f_length=LEN, product=PROD)\n\nwith strategy.scope():\n    model = build_model()\nif TRAIN_MODEL:\n    model.fit(train_gen, verbose=1,\n          validation_data = valid_gen,\n          epochs=EPOCHS, callbacks = [LR])\n    model.save_weights(f'models_v{VER}/model_v{VER}_p{PROD}.weights.h5')\nelse:\n    model.load_weights(f'{PATH}model_v{VER}_p{PROD}.weights.h5')", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-24T05:11:30.961486Z", "iopub.execute_input": "2025-01-24T05:11:30.961681Z", "iopub.status.idle": "2025-01-24T05:14:04.918498Z", "shell.execute_reply.started": "2025-01-24T05:11:30.961663Z", "shell.execute_reply": "2025-01-24T05:14:04.917095Z"}, "collapsed": true, "jupyter": {"outputs_hidden": true}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "# Predict Test\nWe predict and display predictions. We iterate over each country store pair. Train data was standardized so test predictions will be standarized. We will \"un-standardize\" test predictions below.\n\nWe predict using `auto regression`. Our model predicts 32 days into the future. Then we use those predictions together with train data to predict another 32 days into the future. We repeat this process 35 times to predict 3 years into the future.", "metadata": {}}, {"cell_type": "code", "source": "# ADD NAN INDICATOR FEATURE TO DATA\ndata = np.expand_dims(data,axis=-1)\nnans = np.isnan(data).astype('float32')\ndata = np.concatenate([data,nans],axis=-1)", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-24T05:14:04.919344Z", "iopub.status.idle": "2025-01-24T05:14:04.919784Z", "shell.execute_reply": "2025-01-24T05:14:04.919591Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "preds = np.zeros((18,32*35))\n\n# ITERATE OVER ALL COMBINATIONS OF COUNTRY AND STORE FOR SPECIFIC PRODUCT\n# PREDICT 3 YEARS INTO THE FUTURE\n\nbad_rows = []\nfor jj in range(18):\n    ddd0 = data[PROD:PROD+1,-LEN:,jj,:].copy()\n    if np.isnan(ddd0[:,:,0]).sum()==LEN:\n        bad_rows.append(jj)\n    \n    pp = []\n    for j in range(0,35):\n        print(j,\", \",end=\"\")\n        if j==0: dd2 = ddd0\n        else: dd2 = np.concatenate([ddd0[:,32*j:,:]]+pp,axis=1) \n        p2 = model.predict( np.nan_to_num(dd2[:,-LEN:,:]) ,verbose=0)\n        p2 = p2.reshape((1,32,1)) # ADD NAN INDICATOR TO PREDICTIONS\n        p2 = np.concatenate([p2,np.zeros_like(p2)],axis=-1)\n        pp.append(p2)\n        if j==34:\n            print()\n            plt.figure(figsize=(20,5))\n            plt.plot(np.arange(LEN), np.nan_to_num(ddd0[0,:,0]) )\n            for k in range(j+1):\n                plt.plot(np.arange(32)+LEN+32*k,pp[k][0,:,0])\n            cc = C[jj%6]\n            ss = S[jj//6]\n            plt.title(f\"Product={P[PROD]}, Country={cc}, Store={ss}\",size=16)\n            plt.show()\n            \n    preds[jj,:] = np.concatenate([z[:,:,:1] for z in pp],axis=1).flatten() ", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-24T05:14:04.920564Z", "iopub.status.idle": "2025-01-24T05:14:04.920904Z", "shell.execute_reply": "2025-01-24T05:14:04.920775Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "# Reverse Standardize\nWe standarized the train data by minus mean divide std. So now before we can submit predictions, we need to reverse this with multiply std and add mean to preds. Also we need to reverse our Kenya fudge factor. When all train features are NAN, we convert predictions to the average prediction.", "metadata": {}}, {"cell_type": "code", "source": "# FILLNAN PREDS\nFILLNAN = np.nanmean(preds,axis=0)\nfor r in bad_rows:\n    preds[r,:] = FILLNAN\n\n# REVERSE STANDARIZE PREDICTIONS AND FIX KENYA\npreds = (preds*stds[PROD])+means[PROD]\nfor i in [3,9,15]: preds[i,:] = preds[i,:]/1.15", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-24T05:14:04.921649Z", "iopub.status.idle": "2025-01-24T05:14:04.921931Z", "shell.execute_reply": "2025-01-24T05:14:04.921796Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "# Create Submission File", "metadata": {}}, {"cell_type": "code", "source": "if PROD==0:\n    test = pd.read_csv(\"./input/playground-series-s5e1/test.csv\")\n    test.date = pd.to_datetime(test.date)\n    test['alpha3'] = test['country'].map(dict(zip(\n        np.sort(test['country'].unique()), alpha3s)))\n    years = np.sort(test['date'].dt.year.unique())\n    test['year'] = test['date'].dt.year\n    if USE_INTERNET:\n        gdp = np.array([\n            [get_gdp_per_capita(alpha3, year) for year in years]\n            for alpha3 in alpha3s\n        ])\n        gdp1 = pd.DataFrame(gdp, index=alpha3s, columns=years)\n    else:\n        gdp1 = pd.read_csv(f\"{PATH}gdp1.csv\")\n        gdp1 = gdp1.set_index(\"Unnamed: 0\")\n        gdp1 = gdp1.rename(columns=lambda x: int(x))\n    test['GDP'] = test.apply(lambda s: gdp1.loc[s['alpha3'], s['year']], axis=1)\n    test[\"num_sold\"] = 0.0\nelse:\n    test = pd.read_csv(f\"test_v{VER}_p{PROD-1}.csv\")\n    test.date = pd.to_datetime(test.date)", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-24T05:14:04.922439Z", "iopub.status.idle": "2025-01-24T05:14:04.922772Z", "shell.execute_reply": "2025-01-24T05:14:04.922652Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "# Reverse GDP, Store Ratio\nWe divided train by GDP and divided train by Store Ratio, so we need to reverse this and multiple test by GDP and multiply test by Store Ratio", "metadata": {}}, {"cell_type": "code", "source": "for i in range(3):\n    for j in range(6):\n        test.loc[(test['product']==P[PROD])&(test.store==S[i])&(test.country==C[j]),'num_sold'] =\\\n            preds[i*6+j,:1095]\ntest[\"store_ratio\"] = test.store.map(store_ratio)\ntest.loc[test['product']==P[PROD],\"num_sold\"] =\\\n    test.loc[test['product']==P[PROD],\"num_sold\"] * test.loc[test['product']==P[PROD],\"GDP\"]\ntest.loc[test['product']==P[PROD],\"num_sold\"] =\\\n    test.loc[test['product']==P[PROD],\"num_sold\"] * test.loc[test['product']==P[PROD],\"store_ratio\"]\n\nprint( test.shape )\ndisplay( test.head() )", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-24T05:14:04.923399Z", "iopub.status.idle": "2025-01-24T05:14:04.923744Z", "shell.execute_reply": "2025-01-24T05:14:04.923627Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "# Display Test Predictions\nThese are train and test data without any standardization.", "metadata": {}}, {"cell_type": "code", "source": "for ss in range(3):\n    for cc in range(6):\n        df1 = train.loc[(train.country==C[cc])&(train['product']==P[PROD])&(train.store==S[ss])]\n        df2 = test.loc[(test.country==C[cc])&(test['product']==P[PROD])&(test.store==S[ss])]\n\n        plt.figure(figsize=(20,5))\n        tmp = df1.num_sold * df1.GDP * df1.store_ratio\n        tmp.iloc[0] = np.nan_to_num( tmp.iloc[0] )\n        plt.plot(np.arange(len(df1)), tmp )\n        plt.plot(np.arange(len(df2))+len(df1),df2.num_sold)\n        plt.title(f\"Product={P[PROD]}, Country={cc}, Store={ss}\",size=14)\n        plt.show()", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-24T05:14:04.924454Z", "iopub.status.idle": "2025-01-24T05:14:04.924739Z", "shell.execute_reply": "2025-01-24T05:14:04.924598Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "# Write Partial Submission CSV\nWe run this notebook 5 times with `PROD = 0,1,2,3,4`. The first 4 times write intermediate files. The last 5th time will write our `submission.csv` file to submit to competition.", "metadata": {}}, {"cell_type": "code", "source": "# DELETE THIS IF YOU RUN THIS NOTEBOOK 5 TIMES AND MAKE YOUR OWN SUBMISSION.CSV\nif PROD!=4:\n    os.system(f\"cp {PATH}submission_v1.csv submission_v1.csv\")", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-24T05:14:04.925386Z", "iopub.status.idle": "2025-01-24T05:14:04.925729Z", "shell.execute_reply": "2025-01-24T05:14:04.925594Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "if PROD<4:\n    test.to_csv(f\"test_v{VER}_p{PROD}.csv\",index=False)\n    print(f\"Saved partial predictions for product {P[PROD]} (PROD = {PROD})\")\n    print(f\"Now run this notebook again with PROD = {PROD+1} to make more predictions.\")\nelse:\n    test[['id','num_sold']].to_csv(f\"submission_v{VER}.csv\",index=False)\n    print(f\"Wrote submission_v{VER}.csv, now submit to comp!\")\ntest[['id','num_sold']].head()", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-24T05:14:04.926353Z", "iopub.status.idle": "2025-01-24T05:14:04.926691Z", "shell.execute_reply": "2025-01-24T05:14:04.926566Z"}}, "outputs": [], "execution_count": null}]}