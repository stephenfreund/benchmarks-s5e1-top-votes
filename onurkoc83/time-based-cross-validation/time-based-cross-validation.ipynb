{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T02:17:29.092932Z",
     "iopub.status.busy": "2025-01-01T02:17:29.092534Z",
     "iopub.status.idle": "2025-01-01T02:17:29.419789Z",
     "shell.execute_reply": "2025-01-01T02:17:29.418499Z",
     "shell.execute_reply.started": "2025-01-01T02:17:29.092884Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Read the data\n",
    "df = pd.read_csv('./input/playground-series-s5e1/train.csv')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Sort by date\n",
    "df = df.sort_values('date')\n",
    "\n",
    "# Categorical features\n",
    "cat_features = ['country', 'product', 'store']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0c",
   "metadata": {},
   "source": [
    "# NaN Values for unique categories \n",
    "\n",
    "This code checks and removes categories that contain NaN (missing) values:\n",
    "\n",
    "1. First, it counts how many rows we have in total\n",
    "2. Then finds any categories that have NaN values in their 'num_sold' column\n",
    "3. These categories are completely removed from the dataset\n",
    "4. These removed categories won't be used in cross-validation\n",
    "\n",
    "This way, we ensure that our cross-validation process only works with complete data, avoiding any potential issues with missing values during model training and validation.\n",
    "\n",
    "Before dropping NaN categories ‚Üí Shows total row count\n",
    "After dropping NaN categories ‚Üí Shows remaining rows after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T02:17:31.963002Z",
     "iopub.status.busy": "2025-01-01T02:17:31.962419Z",
     "iopub.status.idle": "2025-01-01T02:17:32.608914Z",
     "shell.execute_reply": "2025-01-01T02:17:32.60777Z",
     "shell.execute_reply.started": "2025-01-01T02:17:31.962944Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Shape before dropping NaN categories: {len(df)}\")\n",
    "# Calculate NaN percentage for each category combination\n",
    "nan_percentages = df.groupby(cat_features)['num_sold'].apply(\n",
    "    lambda x: (x.isna().sum() / len(x)) * 100\n",
    ").reset_index(name='nan_percentage')\n",
    "\n",
    "# Get category combinations to drop (those with NaN values)\n",
    "categories_to_drop = nan_percentages[nan_percentages['nan_percentage'] > 0]\n",
    "\n",
    "\n",
    "# Drop rows with these category combinations from the original dataframe\n",
    "for _, row in categories_to_drop.iterrows():\n",
    "    mask = True\n",
    "    for feat in cat_features:\n",
    "        mask &= (df[feat] == row[feat])\n",
    "    df = df[~mask]\n",
    "\n",
    "\n",
    "print(f\"Shape after dropping NaN categories: {len(df)}\")\n",
    "categories_to_drop.sort_values('nan_percentage')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T02:17:33.862491Z",
     "iopub.status.busy": "2025-01-01T02:17:33.861995Z",
     "iopub.status.idle": "2025-01-01T02:17:33.869869Z",
     "shell.execute_reply": "2025-01-01T02:17:33.868569Z",
     "shell.execute_reply.started": "2025-01-01T02:17:33.862451Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_date_features(data):\n",
    "\n",
    "   # 1. Quarter (√áeyrek)\n",
    "   data['quarter'] = data['date'].dt.quarter\n",
    "\n",
    "   # 2. Month (Ay)\n",
    "   data['month'] = data['date'].dt.month \n",
    "\n",
    "   # 3. Day (G√ºn)\n",
    "   data['day'] = data['date'].dt.day\n",
    "\n",
    "   # 4. Day of week (Haftanƒ±n g√ºn√º)\n",
    "   data['day_of_week'] = data['date'].dt.dayofweek\n",
    "\n",
    "   # 5. Day of year (Yƒ±lƒ±n g√ºn√º)\n",
    "   data['day_of_year'] = data['date'].dt.dayofyear\n",
    "\n",
    "   # 6. Week of month (Ayƒ±n haftasƒ±)\n",
    "   data['week_of_month'] = data['date'].dt.day.apply(lambda x: (x-1)//7 + 1)\n",
    "\n",
    "   # 7. Week of year (Yƒ±lƒ±n haftasƒ±)\n",
    "   data['week_of_year'] = data['date'].dt.isocalendar().week\n",
    "\n",
    "   # 8. Is weekend (Hafta sonu mu?)\n",
    "   data['is_weekend'] = data['date'].dt.dayofweek.isin([5,6]).astype(int)\n",
    "\n",
    "   # 9. Is month end (Ayƒ±n son g√ºn√º m√º?)\n",
    "   data['is_month_end'] = data['date'].dt.is_month_end.astype(int)\n",
    "\n",
    "   # 10. Year (Yƒ±l)\n",
    "   data['year'] = data['date'].dt.year\n",
    "   \n",
    "   date_feats = [ \n",
    "                 'quarter',\n",
    "                 'month',\n",
    "                 'day',\n",
    "                 'day_of_week',\n",
    "                 'day_of_year',\n",
    "                 'week_of_month',\n",
    "                 'week_of_year',\n",
    "                 'is_weekend',\n",
    "                 'is_month_end',\n",
    "                 'year']\n",
    "   \n",
    "   return data, date_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8c",
   "metadata": {},
   "source": [
    "# Time-Based Cross Validation \n",
    "\n",
    "This code implements a special type of cross-validation for time series data. Here's how it works:\n",
    "\n",
    "## Main Idea\n",
    "- The data is split by time periods\n",
    "- Each unqiue category (country,product,store) is handled separately\n",
    "- We use a sliding window approach to create train and validation sets\n",
    "- The process is repeated 5 times (5 folds)\n",
    "\n",
    "## How the Splits Work\n",
    "1. For each category:\n",
    "   - Takes 40% of the data for initial training\n",
    "   - Uses 20% for validation\n",
    "   - Slides forward for each fold\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T02:33:31.458318Z",
     "iopub.status.busy": "2025-01-01T02:33:31.457883Z",
     "iopub.status.idle": "2025-01-01T02:33:31.46852Z",
     "shell.execute_reply": "2025-01-01T02:33:31.467089Z",
     "shell.execute_reply.started": "2025-01-01T02:33:31.458284Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(filename='./input/expanding-window/expanding_window.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T02:17:37.449666Z",
     "iopub.status.busy": "2025-01-01T02:17:37.449312Z",
     "iopub.status.idle": "2025-01-01T02:18:25.671478Z",
     "shell.execute_reply": "2025-01-01T02:18:25.669857Z",
     "shell.execute_reply.started": "2025-01-01T02:17:37.449635Z"
    }
   },
   "outputs": [],
   "source": [
    "# Label encode categorical features\n",
    "label_encoders = {}\n",
    "for feature in cat_features:\n",
    "    label_encoders[feature] = LabelEncoder()\n",
    "    df[f'{feature}_encoded'] = label_encoders[feature].fit_transform(df[feature])\n",
    "\n",
    "# Update cat_features to use encoded versions\n",
    "encoded_cat_features = [f'{feature}_encoded' for feature in cat_features]\n",
    "\n",
    "# ANSI color codes\n",
    "BLUE = '\\033[94m'\n",
    "RED = '\\033[91m'\n",
    "GREEN = '\\033[92m'\n",
    "RESET = '\\033[0m'  # Reset color formatting\n",
    "\n",
    "# Function to create splits for a single category combination\n",
    "def create_splits(category_data, n_splits=5):\n",
    "    splits = []\n",
    "    dates = category_data['date'].sort_values().unique()\n",
    "    n_dates = len(dates)\n",
    "    \n",
    "    # Calculate validation size (approximately 20% of total data)\n",
    "    val_size = int(n_dates * 0.2)\n",
    "    \n",
    "    # Calculate initial training size\n",
    "    initial_train_size = int(n_dates * 0.4)  # Start with 40% of data\n",
    "    \n",
    "    # Calculate step size for sliding\n",
    "    remaining_points = n_dates - initial_train_size - val_size\n",
    "    if remaining_points < n_splits - 1:\n",
    "        return []\n",
    "        \n",
    "    step_size = remaining_points // (n_splits - 1)\n",
    "    \n",
    "    for i in range(n_splits):\n",
    "        train_end_idx = initial_train_size + (i * step_size)\n",
    "        val_end_idx = train_end_idx + val_size\n",
    "        \n",
    "        if val_end_idx > n_dates:\n",
    "            break\n",
    "            \n",
    "        train_dates = dates[:train_end_idx]\n",
    "        val_dates = dates[train_end_idx:val_end_idx]\n",
    "        \n",
    "        splits.append((train_dates, val_dates))\n",
    "    \n",
    "    return splits\n",
    "\n",
    "# Initialize lists to store results\n",
    "all_mapes = []\n",
    "\n",
    "# Get unique category combinations\n",
    "category_combinations = df.groupby(cat_features).size().reset_index()[cat_features]\n",
    "\n",
    "# Create 5 folds\n",
    "for fold in range(5):\n",
    "    print(f\"\\nFold {fold + 1}:\")\n",
    "    print()\n",
    "    # Initialize lists to store train and validation indices for this fold\n",
    "    train_indices = []\n",
    "    val_indices = []\n",
    "    \n",
    "    # Process each category combination\n",
    "    for idx, cat_combo in category_combinations.iterrows():\n",
    "        # Get data for this category combination\n",
    "        mask = True\n",
    "        for feat, value in cat_combo.items():\n",
    "            mask = mask & (df[feat] == value)\n",
    "        category_data = df[mask].copy()\n",
    "            \n",
    "        # Create splits for this category\n",
    "        splits = create_splits(category_data, n_splits=5)\n",
    "        \n",
    "        if splits and fold < len(splits):  # Only proceed if we have enough data for this fold\n",
    "            train_dates, val_dates = splits[fold]\n",
    "            \n",
    "            # Add indices to our lists\n",
    "            train_indices.extend(category_data[category_data['date'].isin(train_dates)].index)\n",
    "            val_indices.extend(category_data[category_data['date'].isin(val_dates)].index)\n",
    "    \n",
    "    # Create train and validation sets\n",
    "    train_data = df.loc[train_indices]\n",
    "    val_data = df.loc[val_indices]\n",
    "    \n",
    "    # Print date ranges and shapes with colors\n",
    "    print(f\"{BLUE}Train date range: {train_data['date'].dt.date.min()} to {train_data['date'].dt.date.max()}\")\n",
    "    print(f\"Val date range: {val_data['date'].dt.date.min()} to {val_data['date'].dt.date.max()}{RESET}\")\n",
    "    print()  # Add empty line between date ranges and shapes\n",
    "    print(f\"{RED}Train shape: {train_data.shape}\")\n",
    "    print(f\"Val shape: {val_data.shape}{RESET}\")\n",
    "    print()  # Add empty line between shapes and MAPE\n",
    "    \n",
    "    # Apply the function to train and validation data\n",
    "    train_data, date_feats = create_date_features(train_data)\n",
    "    val_data, _ = create_date_features(val_data)\n",
    "    \n",
    "    # Update feature columns\n",
    "    feature_cols = encoded_cat_features + date_feats\n",
    "\n",
    "    X_train = train_data[feature_cols]\n",
    "    X_val = val_data[feature_cols]\n",
    "    y_train = train_data['num_sold']\n",
    "    y_val = val_data['num_sold']\n",
    "    \n",
    "    # Train model\n",
    "    model = LGBMRegressor(n_estimators=1000,\n",
    "                         learning_rate=0.1,\n",
    "                         boosting_type='gbdt',\n",
    "                          verbosity=-1,\n",
    "                         categorical_feature=encoded_cat_features,\n",
    "                         random_state=42)\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, \n",
    "        y_train,\n",
    "        categorical_feature=encoded_cat_features\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    val_preds = model.predict(X_val)\n",
    "    \n",
    "    # Calculate and print MAPE with color\n",
    "    mape = mean_absolute_percentage_error(y_val, val_preds)\n",
    "    all_mapes.append(mape)\n",
    "    print(f\"{GREEN}Fold {fold + 1} MAPE: {mape:.4f}{RESET}\")\n",
    "    print()  # Add empty line after MAPE\n",
    "\n",
    "# Print average MAPE\n",
    "valid_mapes = [m for m in all_mapes if not np.isnan(m)]\n",
    "if valid_mapes:\n",
    "    print(f\"\\n{GREEN}Average MAPE across all folds: {np.mean(valid_mapes):.4f}{RESET}\")\n",
    "else:\n",
    "    print(\"\\nNo valid MAPE scores calculated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T02:14:31.552736Z",
     "iopub.status.busy": "2025-01-01T02:14:31.55232Z",
     "iopub.status.idle": "2025-01-01T02:14:31.560324Z",
     "shell.execute_reply": "2025-01-01T02:14:31.558825Z",
     "shell.execute_reply.started": "2025-01-01T02:14:31.552704Z"
    }
   },
   "source": [
    "## üîÆ Coming Soon...\n",
    "### The prediction phase is on the horizon!\n",
    "\n",
    "> **Don't forget to show your support with a vote if you find this notebook helpful!** \n",
    "> \n",
    "> *Every upvote motivates me to create better content* ‚≠ê\n",
    "\n",
    "---\n",
    "*Thanks for reading! See you in the next notebook!* üëã"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10652996,
     "sourceId": 85723,
     "sourceType": "competition"
    },
    {
     "datasetId": 6406117,
     "sourceId": 10345089,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
