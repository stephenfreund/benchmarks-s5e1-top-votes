{"metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}, "language_info": {"name": "python", "version": "3.10.12", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}, "kaggle": {"accelerator": "none", "dataSources": [{"sourceId": 85723, "databundleVersionId": 10652996, "sourceType": "competition"}, {"sourceId": 10345089, "sourceType": "datasetVersion", "datasetId": 6406117}], "dockerImageVersionId": 30822, "isInternetEnabled": true, "language": "python", "sourceType": "notebook", "isGpuEnabled": false}}, "nbformat_minor": 4, "nbformat": 4, "cells": [{"id": "05da6094-8c6b-4746-b8ab-52af6dc748e6", "cell_type": "code", "source": "import pandas as pd\nimport numpy as np\nfrom lightgbm import LGBMRegressor\nfrom sklearn.metrics import mean_absolute_percentage_error\nfrom sklearn.preprocessing import LabelEncoder\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Read the data\ndf = pd.read_csv('./input/playground-series-s5e1/train.csv')\ndf['date'] = pd.to_datetime(df['date'])\n\n# Sort by date\ndf = df.sort_values('date')\n\n# Categorical features\ncat_features = ['country', 'product', 'store']\n", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-01T02:17:29.092534Z", "iopub.execute_input": "2025-01-01T02:17:29.092932Z", "iopub.status.idle": "2025-01-01T02:17:29.419789Z", "shell.execute_reply.started": "2025-01-01T02:17:29.092884Z", "shell.execute_reply": "2025-01-01T02:17:29.418499Z"}}, "outputs": [], "execution_count": null}, {"id": "3a0c1643-ab0b-44e3-ac68-0c589286160d", "cell_type": "markdown", "source": "# NaN Values for unique categories \n\nThis code checks and removes categories that contain NaN (missing) values:\n\n1. First, it counts how many rows we have in total\n2. Then finds any categories that have NaN values in their 'num_sold' column\n3. These categories are completely removed from the dataset\n4. These removed categories won't be used in cross-validation\n\nThis way, we ensure that our cross-validation process only works with complete data, avoiding any potential issues with missing values during model training and validation.\n\nBefore dropping NaN categories \u2192 Shows total row count\nAfter dropping NaN categories \u2192 Shows remaining rows after cleaning", "metadata": {}}, {"id": "d2a4e471-911e-423d-adfd-8b6abd6226d9", "cell_type": "code", "source": "print(f\"Shape before dropping NaN categories: {len(df)}\")\n# Calculate NaN percentage for each category combination\nnan_percentages = df.groupby(cat_features)['num_sold'].apply(\n    lambda x: (x.isna().sum() / len(x)) * 100\n).reset_index(name='nan_percentage')\n\n# Get category combinations to drop (those with NaN values)\ncategories_to_drop = nan_percentages[nan_percentages['nan_percentage'] > 0]\n\n\n# Drop rows with these category combinations from the original dataframe\nfor _, row in categories_to_drop.iterrows():\n    mask = True\n    for feat in cat_features:\n        mask &= (df[feat] == row[feat])\n    df = df[~mask]\n\n\nprint(f\"Shape after dropping NaN categories: {len(df)}\")\ncategories_to_drop.sort_values('nan_percentage')\n", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-01T02:17:31.962419Z", "iopub.execute_input": "2025-01-01T02:17:31.963002Z", "iopub.status.idle": "2025-01-01T02:17:32.608914Z", "shell.execute_reply.started": "2025-01-01T02:17:31.962944Z", "shell.execute_reply": "2025-01-01T02:17:32.60777Z"}}, "outputs": [], "execution_count": null}, {"id": "0a26d849-f73e-40ac-9744-e1158da8c6ab", "cell_type": "code", "source": "def create_date_features(data):\n\n   # 1. Quarter (\u00c7eyrek)\n   data['quarter'] = data['date'].dt.quarter\n\n   # 2. Month (Ay)\n   data['month'] = data['date'].dt.month \n\n   # 3. Day (G\u00fcn)\n   data['day'] = data['date'].dt.day\n\n   # 4. Day of week (Haftan\u0131n g\u00fcn\u00fc)\n   data['day_of_week'] = data['date'].dt.dayofweek\n\n   # 5. Day of year (Y\u0131l\u0131n g\u00fcn\u00fc)\n   data['day_of_year'] = data['date'].dt.dayofyear\n\n   # 6. Week of month (Ay\u0131n haftas\u0131)\n   data['week_of_month'] = data['date'].dt.day.apply(lambda x: (x-1)//7 + 1)\n\n   # 7. Week of year (Y\u0131l\u0131n haftas\u0131)\n   data['week_of_year'] = data['date'].dt.isocalendar().week\n\n   # 8. Is weekend (Hafta sonu mu?)\n   data['is_weekend'] = data['date'].dt.dayofweek.isin([5,6]).astype(int)\n\n   # 9. Is month end (Ay\u0131n son g\u00fcn\u00fc m\u00fc?)\n   data['is_month_end'] = data['date'].dt.is_month_end.astype(int)\n\n   # 10. Year (Y\u0131l)\n   data['year'] = data['date'].dt.year\n   \n   date_feats = [ \n                 'quarter',\n                 'month',\n                 'day',\n                 'day_of_week',\n                 'day_of_year',\n                 'week_of_month',\n                 'week_of_year',\n                 'is_weekend',\n                 'is_month_end',\n                 'year']\n   \n   return data, date_feats", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-01T02:17:33.861995Z", "iopub.execute_input": "2025-01-01T02:17:33.862491Z", "iopub.status.idle": "2025-01-01T02:17:33.869869Z", "shell.execute_reply.started": "2025-01-01T02:17:33.862451Z", "shell.execute_reply": "2025-01-01T02:17:33.868569Z"}}, "outputs": [], "execution_count": null}, {"id": "eb8cd2f9-33fb-48fc-8403-08bd400f5589", "cell_type": "markdown", "source": "# Time-Based Cross Validation \n\nThis code implements a special type of cross-validation for time series data. Here's how it works:\n\n## Main Idea\n- The data is split by time periods\n- Each unqiue category (country,product,store) is handled separately\n- We use a sliding window approach to create train and validation sets\n- The process is repeated 5 times (5 folds)\n\n## How the Splits Work\n1. For each category:\n   - Takes 40% of the data for initial training\n   - Uses 20% for validation\n   - Slides forward for each fold\n\n", "metadata": {}}, {"id": "dbba8c90-91c2-405a-b7e0-86861c688834", "cell_type": "code", "source": "from IPython.display import Image, display\ndisplay(Image(filename='./input/expanding-window/expanding_window.png'))", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-01T02:33:31.457883Z", "iopub.execute_input": "2025-01-01T02:33:31.458318Z", "iopub.status.idle": "2025-01-01T02:33:31.46852Z", "shell.execute_reply.started": "2025-01-01T02:33:31.458284Z", "shell.execute_reply": "2025-01-01T02:33:31.467089Z"}}, "outputs": [], "execution_count": null}, {"id": "19b19190-530a-40cf-8e58-683b030dff4c", "cell_type": "code", "source": "# Label encode categorical features\nlabel_encoders = {}\nfor feature in cat_features:\n    label_encoders[feature] = LabelEncoder()\n    df[f'{feature}_encoded'] = label_encoders[feature].fit_transform(df[feature])\n\n# Update cat_features to use encoded versions\nencoded_cat_features = [f'{feature}_encoded' for feature in cat_features]\n\n# ANSI color codes\nBLUE = '\\033[94m'\nRED = '\\033[91m'\nGREEN = '\\033[92m'\nRESET = '\\033[0m'  # Reset color formatting\n\n# Function to create splits for a single category combination\ndef create_splits(category_data, n_splits=5):\n    splits = []\n    dates = category_data['date'].sort_values().unique()\n    n_dates = len(dates)\n    \n    # Calculate validation size (approximately 20% of total data)\n    val_size = int(n_dates * 0.2)\n    \n    # Calculate initial training size\n    initial_train_size = int(n_dates * 0.4)  # Start with 40% of data\n    \n    # Calculate step size for sliding\n    remaining_points = n_dates - initial_train_size - val_size\n    if remaining_points < n_splits - 1:\n        return []\n        \n    step_size = remaining_points // (n_splits - 1)\n    \n    for i in range(n_splits):\n        train_end_idx = initial_train_size + (i * step_size)\n        val_end_idx = train_end_idx + val_size\n        \n        if val_end_idx > n_dates:\n            break\n            \n        train_dates = dates[:train_end_idx]\n        val_dates = dates[train_end_idx:val_end_idx]\n        \n        splits.append((train_dates, val_dates))\n    \n    return splits\n\n# Initialize lists to store results\nall_mapes = []\n\n# Get unique category combinations\ncategory_combinations = df.groupby(cat_features).size().reset_index()[cat_features]\n\n# Create 5 folds\nfor fold in range(5):\n    print(f\"\\nFold {fold + 1}:\")\n    print()\n    # Initialize lists to store train and validation indices for this fold\n    train_indices = []\n    val_indices = []\n    \n    # Process each category combination\n    for idx, cat_combo in category_combinations.iterrows():\n        # Get data for this category combination\n        mask = True\n        for feat, value in cat_combo.items():\n            mask = mask & (df[feat] == value)\n        category_data = df[mask].copy()\n            \n        # Create splits for this category\n        splits = create_splits(category_data, n_splits=5)\n        \n        if splits and fold < len(splits):  # Only proceed if we have enough data for this fold\n            train_dates, val_dates = splits[fold]\n            \n            # Add indices to our lists\n            train_indices.extend(category_data[category_data['date'].isin(train_dates)].index)\n            val_indices.extend(category_data[category_data['date'].isin(val_dates)].index)\n    \n    # Create train and validation sets\n    train_data = df.loc[train_indices]\n    val_data = df.loc[val_indices]\n    \n    # Print date ranges and shapes with colors\n    print(f\"{BLUE}Train date range: {train_data['date'].dt.date.min()} to {train_data['date'].dt.date.max()}\")\n    print(f\"Val date range: {val_data['date'].dt.date.min()} to {val_data['date'].dt.date.max()}{RESET}\")\n    print()  # Add empty line between date ranges and shapes\n    print(f\"{RED}Train shape: {train_data.shape}\")\n    print(f\"Val shape: {val_data.shape}{RESET}\")\n    print()  # Add empty line between shapes and MAPE\n    \n    # Apply the function to train and validation data\n    train_data, date_feats = create_date_features(train_data)\n    val_data, _ = create_date_features(val_data)\n    \n    # Update feature columns\n    feature_cols = encoded_cat_features + date_feats\n\n    X_train = train_data[feature_cols]\n    X_val = val_data[feature_cols]\n    y_train = train_data['num_sold']\n    y_val = val_data['num_sold']\n    \n    # Train model\n    model = LGBMRegressor(n_estimators=1000,\n                         learning_rate=0.1,\n                         boosting_type='gbdt',\n                          verbosity=-1,\n                         categorical_feature=encoded_cat_features,\n                         random_state=42)\n    \n    model.fit(\n        X_train, \n        y_train,\n        categorical_feature=encoded_cat_features\n    )\n    \n    # Make predictions\n    val_preds = model.predict(X_val)\n    \n    # Calculate and print MAPE with color\n    mape = mean_absolute_percentage_error(y_val, val_preds)\n    all_mapes.append(mape)\n    print(f\"{GREEN}Fold {fold + 1} MAPE: {mape:.4f}{RESET}\")\n    print()  # Add empty line after MAPE\n\n# Print average MAPE\nvalid_mapes = [m for m in all_mapes if not np.isnan(m)]\nif valid_mapes:\n    print(f\"\\n{GREEN}Average MAPE across all folds: {np.mean(valid_mapes):.4f}{RESET}\")\nelse:\n    print(\"\\nNo valid MAPE scores calculated\")", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-01T02:17:37.449312Z", "iopub.execute_input": "2025-01-01T02:17:37.449666Z", "iopub.status.idle": "2025-01-01T02:18:25.671478Z", "shell.execute_reply.started": "2025-01-01T02:17:37.449635Z", "shell.execute_reply": "2025-01-01T02:18:25.669857Z"}}, "outputs": [], "execution_count": null}, {"id": "38f22f59-1a61-4a53-991e-766499b1c7e3", "cell_type": "markdown", "source": "## \ud83d\udd2e Coming Soon...\n### The prediction phase is on the horizon!\n\n> **Don't forget to show your support with a vote if you find this notebook helpful!** \n> \n> *Every upvote motivates me to create better content* \u2b50\n\n---\n*Thanks for reading! See you in the next notebook!* \ud83d\udc4b", "metadata": {"execution": {"iopub.status.busy": "2025-01-01T02:14:31.55232Z", "iopub.execute_input": "2025-01-01T02:14:31.552736Z", "iopub.status.idle": "2025-01-01T02:14:31.560324Z", "shell.execute_reply.started": "2025-01-01T02:14:31.552704Z", "shell.execute_reply": "2025-01-01T02:14:31.558825Z"}}}]}