{"metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "language_info": {"name": "python", "version": "3.10.12", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}, "kaggle": {"accelerator": "none", "dataSources": [{"sourceId": 85723, "databundleVersionId": 10652996, "sourceType": "competition"}, {"sourceId": 3325325, "sourceType": "datasetVersion", "datasetId": 2007861}], "dockerImageVersionId": 30822, "isInternetEnabled": true, "language": "python", "sourceType": "notebook", "isGpuEnabled": false}}, "nbformat_minor": 4, "nbformat": 4, "cells": [{"cell_type": "markdown", "source": "# Forecasting using previous years as a baseline", "metadata": {}}, {"cell_type": "markdown", "source": "See my EDA and Linear Regression baseline notebook for more information about this dataset:\n- https://www.kaggle.com/code/cabaxiom/s5e1-eda-and-linear-regression-baseline?scriptVersionId=216127469", "metadata": {}}, {"cell_type": "markdown", "source": "This forecast contains much of the same first steps as my previous notebook - aggregating on country, product and store, then making the forecast for the daily number of sales for the next three years, before disagregating back to get the number of sales for each country, product and store.\n\n**Forecast:**\n\nThe forecast in this notebook takes the mean number of sales for each day of the year for the previous x years and uses this as the forecast. **No model is used**.\n\nDay of the week has been considered.", "metadata": {}}, {"cell_type": "markdown", "source": "### Preliminaries", "metadata": {}}, {"cell_type": "code", "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style('whitegrid')", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-05T21:07:05.632386Z", "iopub.execute_input": "2025-01-05T21:07:05.632823Z", "iopub.status.idle": "2025-01-05T21:07:06.914073Z", "shell.execute_reply.started": "2025-01-05T21:07:05.632787Z", "shell.execute_reply": "2025-01-05T21:07:06.912931Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "import warnings\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"seaborn._oldcore\")", "metadata": {"trusted": true, "_kg_hide-input": true, "execution": {"iopub.status.busy": "2025-01-05T21:07:06.915374Z", "iopub.execute_input": "2025-01-05T21:07:06.915818Z", "iopub.status.idle": "2025-01-05T21:07:06.92052Z", "shell.execute_reply.started": "2025-01-05T21:07:06.91578Z", "shell.execute_reply": "2025-01-05T21:07:06.919244Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "train_df = pd.read_csv(\"./input/playground-series-s5e1/train.csv\", parse_dates=[\"date\"])\ntest_df = pd.read_csv(\"./input/playground-series-s5e1/test.csv\", parse_dates=[\"date\"])", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-05T21:07:06.922753Z", "iopub.execute_input": "2025-01-05T21:07:06.923082Z", "iopub.status.idle": "2025-01-05T21:07:07.467448Z", "shell.execute_reply.started": "2025-01-05T21:07:06.923052Z", "shell.execute_reply": "2025-01-05T21:07:07.466209Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "print(\"Train - Earliest date:\", train_df[\"date\"].min())\nprint(\"Train - Latest date:\", train_df[\"date\"].max())\n\nprint(\"Test - Earliest date:\", test_df[\"date\"].min())\nprint(\"Test - Latest date:\", test_df[\"date\"].max())", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-05T21:07:07.46888Z", "iopub.execute_input": "2025-01-05T21:07:07.469302Z", "iopub.status.idle": "2025-01-05T21:07:07.48132Z", "shell.execute_reply.started": "2025-01-05T21:07:07.469269Z", "shell.execute_reply": "2025-01-05T21:07:07.480158Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "## Country Weights", "metadata": {}}, {"cell_type": "markdown", "source": "We aggregate the country variable when making the forecast as we found in my EDA [notebook](https://www.kaggle.com/code/cabaxiom/s5e1-eda-and-linear-regression-baseline?scriptVersionId=216127469). This works as the proportion of sales in each country for every year is equal to the proportion of GDP per captia for that country and year.", "metadata": {}}, {"cell_type": "code", "source": "gdp_per_capita_df = pd.read_csv(\"./input/world-gdpgdp-gdp-per-capita-and-annual-growths/gdp_per_capita.csv\")\n\nyears =  [\"2010\", \"2011\", \"2012\", \"2013\", \"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020\"]\ngdp_per_capita_filtered_df = gdp_per_capita_df.loc[gdp_per_capita_df[\"Country Name\"].isin(train_df[\"country\"].unique()), [\"Country Name\"] + years].set_index(\"Country Name\")\ngdp_per_capita_filtered_df[\"2010_ratio\"] = gdp_per_capita_filtered_df[\"2010\"] / gdp_per_capita_filtered_df.sum()[\"2010\"]\nfor year in years:\n    gdp_per_capita_filtered_df[f\"{year}_ratio\"] = gdp_per_capita_filtered_df[year] / gdp_per_capita_filtered_df.sum()[year]\ngdp_per_capita_filtered_ratios_df = gdp_per_capita_filtered_df[[i+\"_ratio\" for i in years]]\ngdp_per_capita_filtered_ratios_df.columns = [int(i) for i in years]\ngdp_per_capita_filtered_ratios_df = gdp_per_capita_filtered_ratios_df.unstack().reset_index().rename(columns = {\"level_0\": \"year\", 0: \"ratio\", \"Country Name\": \"country\"})\ngdp_per_capita_filtered_ratios_df['year'] = pd.to_datetime(gdp_per_capita_filtered_ratios_df['year'], format='%Y')", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-05T21:07:07.482642Z", "iopub.execute_input": "2025-01-05T21:07:07.483091Z", "iopub.status.idle": "2025-01-05T21:07:07.551579Z", "shell.execute_reply.started": "2025-01-05T21:07:07.483034Z", "shell.execute_reply": "2025-01-05T21:07:07.550323Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "## Imputing Missing Values", "metadata": {}}, {"cell_type": "markdown", "source": "Imputing missing values, using this information.", "metadata": {}}, {"cell_type": "code", "source": "gdp_per_capita_filtered_ratios_df[\"year\"] = gdp_per_capita_filtered_ratios_df[\"year\"].dt.year\ntrain_df_imputed = train_df.copy()\nprint(f\"Missing values remaining: {train_df_imputed['num_sold'].isna().sum()}\")\n\ntrain_df_imputed[\"year\"] = train_df_imputed[\"date\"].dt.year\nfor year in train_df_imputed[\"year\"].unique():\n    # Impute Time Series 1 (Canada, Discount Stickers, Holographic Goose)\n    target_ratio = gdp_per_capita_filtered_ratios_df.loc[(gdp_per_capita_filtered_ratios_df[\"year\"] == year) & (gdp_per_capita_filtered_ratios_df[\"country\"] == \"Norway\"), \"ratio\"].values[0] # Using Norway as should have the best precision\n    current_raito = gdp_per_capita_filtered_ratios_df.loc[(gdp_per_capita_filtered_ratios_df[\"year\"] == year) & (gdp_per_capita_filtered_ratios_df[\"country\"] == \"Canada\"), \"ratio\"].values[0]\n    ratio_can = current_raito / target_ratio\n    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Canada\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year), \"num_sold\"] * ratio_can).values\n    \n    # Impute Time Series 2 (Only Missing Values)\n    current_ts =  train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Canada\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year)]\n    missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Canada\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] * ratio_can).values\n\n    # Impute Time Series 3 (Only Missing Values)\n    current_ts =  train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Canada\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year)]\n    missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Canada\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] * ratio_can).values\n    \n    # Impute Time Series 4 (Kenya, Discount Stickers, Holographic Goose)\n    current_raito = gdp_per_capita_filtered_ratios_df.loc[(gdp_per_capita_filtered_ratios_df[\"year\"] == year) & (gdp_per_capita_filtered_ratios_df[\"country\"] == \"Kenya\"), \"ratio\"].values[0]\n    ratio_ken = current_raito / target_ratio\n    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Holographic Goose\")& (train_df_imputed[\"year\"] == year), \"num_sold\"] * ratio_ken).values\n\n    # Impute Time Series 5 (Only Missing Values)\n    current_ts = train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year)]\n    missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] * ratio_ken).values\n\n    # Impute Time Series 6 (Only Missing Values)\n    current_ts = train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year)]\n    missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] * ratio_ken).values\n\n    # Impute Time Series 7 (Only Missing Values)\n    current_ts = train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Kerneler\") & (train_df_imputed[\"year\"] == year)]\n    missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Kerneler\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Kerneler\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] * ratio_ken).values\n    \nprint(f\"Missing values remaining: {train_df_imputed['num_sold'].isna().sum()}\")", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-05T21:07:07.552853Z", "iopub.execute_input": "2025-01-05T21:07:07.553254Z", "iopub.status.idle": "2025-01-05T21:07:14.521153Z", "shell.execute_reply.started": "2025-01-05T21:07:07.553221Z", "shell.execute_reply": "2025-01-05T21:07:14.520046Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "missing_rows = train_df_imputed.loc[train_df_imputed[\"num_sold\"].isna()]\n\ntrain_df_imputed.loc[train_df_imputed[\"id\"] == 23719, \"num_sold\"] = 4\ntrain_df_imputed.loc[train_df_imputed[\"id\"] == 207003, \"num_sold\"] = 195\n\nprint(f\"Missing values remaining: {train_df_imputed['num_sold'].isna().sum()}\")", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-05T21:07:14.522187Z", "iopub.execute_input": "2025-01-05T21:07:14.522499Z", "iopub.status.idle": "2025-01-05T21:07:14.533325Z", "shell.execute_reply.started": "2025-01-05T21:07:14.522469Z", "shell.execute_reply": "2025-01-05T21:07:14.532248Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "## Store Weights", "metadata": {}}, {"cell_type": "markdown", "source": "We aggregate the stores, as we found that each store has the same proportion of sales on every day during the EDA.", "metadata": {}}, {"cell_type": "code", "source": "store_weights = train_df_imputed.groupby(\"store\")[\"num_sold\"].sum()/train_df_imputed[\"num_sold\"].sum()", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-05T21:07:14.535992Z", "iopub.execute_input": "2025-01-05T21:07:14.536288Z", "iopub.status.idle": "2025-01-05T21:07:14.575223Z", "shell.execute_reply.started": "2025-01-05T21:07:14.536263Z", "shell.execute_reply": "2025-01-05T21:07:14.574065Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "## Product Weights", "metadata": {}}, {"cell_type": "markdown", "source": "We aggregate over products, as we found during the EDA that the proportion of sales for each product can be fitted with a trigonometric function.", "metadata": {}}, {"cell_type": "code", "source": "product_df = train_df_imputed.groupby([\"date\",\"product\"])[\"num_sold\"].sum().reset_index()\nproduct_ratio_df = product_df.pivot(index=\"date\", columns=\"product\", values=\"num_sold\")\nproduct_ratio_df = product_ratio_df.apply(lambda x: x/x.sum(),axis=1)\nproduct_ratio_df = product_ratio_df.stack().rename(\"ratios\").reset_index()\n\n\nproduct_ratio_2017_df = product_ratio_df.loc[product_ratio_df[\"date\"].dt.year == 2015].copy()\nproduct_ratio_2018_df = product_ratio_df.loc[product_ratio_df[\"date\"].dt.year == 2016].copy()\nproduct_ratio_2019_df = product_ratio_df.loc[product_ratio_df[\"date\"].dt.year == 2015].copy()\n\nproduct_ratio_2017_df[\"date\"] = product_ratio_2017_df[\"date\"] + pd.DateOffset(years=2)\nproduct_ratio_2018_df[\"date\"] = product_ratio_2018_df[\"date\"] + pd.DateOffset(years=2)\nproduct_ratio_2019_df[\"date\"] =  product_ratio_2019_df[\"date\"] + pd.DateOffset(years=4)\n\nforecasted_ratios_df = pd.concat([product_ratio_2017_df, product_ratio_2018_df, product_ratio_2019_df])\n\n", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-05T21:07:14.576896Z", "iopub.execute_input": "2025-01-05T21:07:14.577325Z", "iopub.status.idle": "2025-01-05T21:07:15.02722Z", "shell.execute_reply.started": "2025-01-05T21:07:14.577285Z", "shell.execute_reply": "2025-01-05T21:07:15.026137Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "## Aggregating Time Series", "metadata": {}}, {"cell_type": "markdown", "source": "Making the aggregation over all countries, stores and products, obtaining the total sales each day as our new time series that we want to forecast.", "metadata": {}}, {"cell_type": "code", "source": "original_train_df_imputed = train_df_imputed.copy()\ntrain_df_imputed = train_df_imputed.groupby([\"date\"])[\"num_sold\"].sum().reset_index()", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-05T21:07:15.028343Z", "iopub.execute_input": "2025-01-05T21:07:15.028616Z", "iopub.status.idle": "2025-01-05T21:07:15.062809Z", "shell.execute_reply.started": "2025-01-05T21:07:15.028593Z", "shell.execute_reply": "2025-01-05T21:07:15.061732Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "Lets have a look at the number of sales each day over time:", "metadata": {}}, {"cell_type": "code", "source": "f,ax = plt.subplots(figsize=(20,9))\nsns.lineplot(data=train_df_imputed, x=\"date\", y=\"num_sold\", linewidth = 1);", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-05T21:07:15.063837Z", "iopub.execute_input": "2025-01-05T21:07:15.064166Z", "iopub.status.idle": "2025-01-05T21:07:15.935731Z", "shell.execute_reply.started": "2025-01-05T21:07:15.064134Z", "shell.execute_reply": "2025-01-05T21:07:15.934499Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "train_df_imputed[\"year\"] = train_df_imputed[\"date\"].dt.year\ntrain_df_imputed[\"month\"] = train_df_imputed[\"date\"].dt.month\ntrain_df_imputed[\"day\"] = train_df_imputed[\"date\"].dt.day\ntrain_df_imputed[\"day_of_week\"] = train_df_imputed[\"date\"].dt.dayofweek", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-05T21:07:15.936879Z", "iopub.execute_input": "2025-01-05T21:07:15.937274Z", "iopub.status.idle": "2025-01-05T21:07:15.947675Z", "shell.execute_reply.started": "2025-01-05T21:07:15.937239Z", "shell.execute_reply": "2025-01-05T21:07:15.946497Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "#get the dates to forecast for\ntest_total_sales_df = test_df.groupby([\"date\"])[\"id\"].first().reset_index().drop(columns=\"id\")\n\ntest_total_sales_df[\"month\"] = test_total_sales_df[\"date\"].dt.month\ntest_total_sales_df[\"day\"] = test_total_sales_df[\"date\"].dt.day\ntest_total_sales_df[\"day_of_week\"] = test_total_sales_df[\"date\"].dt.day_of_week", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-05T21:07:15.948784Z", "iopub.execute_input": "2025-01-05T21:07:15.949114Z", "iopub.status.idle": "2025-01-05T21:07:15.973204Z", "shell.execute_reply.started": "2025-01-05T21:07:15.949083Z", "shell.execute_reply": "2025-01-05T21:07:15.972178Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "Account for day of week in the forecast by removing its effect on the number of sales. For example Sataurday has more sales than other days, so we multipy it by a number less than 1, to try and take the variation caused by day of the week out.Thus standardising the number of sales by the day of week.", "metadata": {}}, {"cell_type": "code", "source": "day_of_week_ratio = (train_df_imputed.groupby(\"day_of_week\")[\"num_sold\"].mean() / train_df_imputed.groupby(\"day_of_week\")[\"num_sold\"].mean().mean()).rename(\"day_of_week_ratios\")\ndisplay(day_of_week_ratio)\ntrain_df_imputed = pd.merge(train_df_imputed, day_of_week_ratio, how=\"left\", on=\"day_of_week\")\ntrain_df_imputed[\"adjusted_num_sold\"] = train_df_imputed[\"num_sold\"] / train_df_imputed[\"day_of_week_ratios\"]", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-05T21:07:15.974383Z", "iopub.execute_input": "2025-01-05T21:07:15.974803Z", "iopub.status.idle": "2025-01-05T21:07:15.995666Z", "shell.execute_reply.started": "2025-01-05T21:07:15.974718Z", "shell.execute_reply": "2025-01-05T21:07:15.994764Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "Ignore this, its just to check we didn't make a mistake when standardising:", "metadata": {}}, {"cell_type": "code", "source": "sanity_check = (train_df_imputed[\"num_sold\"].sum() - train_df_imputed[\"adjusted_num_sold\"].sum()) / train_df_imputed[\"num_sold\"].sum()\nprint(f\"This number should be very small {sanity_check:.6f}\")", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-05T21:07:15.996717Z", "iopub.execute_input": "2025-01-05T21:07:15.997119Z", "iopub.status.idle": "2025-01-05T21:07:16.013448Z", "shell.execute_reply.started": "2025-01-05T21:07:15.997081Z", "shell.execute_reply": "2025-01-05T21:07:16.012378Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "## Making the forecast", "metadata": {}}, {"cell_type": "markdown", "source": "We make the forecast by taking the mean number of sales for that day of the year over the past x years. it's probably a good idea to use the full data, but if any trends exist over time it might be best to only use the past few years. Feel free to experiment with this value.", "metadata": {}}, {"cell_type": "code", "source": "train_last_x_years_df = train_df_imputed.loc[train_df_imputed[\"year\"] >= 2010]\ntrain_day_mean_df = train_last_x_years_df.groupby([\"month\", \"day\"])[\"adjusted_num_sold\"].mean().reset_index()", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-05T21:07:16.014791Z", "iopub.execute_input": "2025-01-05T21:07:16.015267Z", "iopub.status.idle": "2025-01-05T21:07:16.037672Z", "shell.execute_reply.started": "2025-01-05T21:07:16.015223Z", "shell.execute_reply": "2025-01-05T21:07:16.036463Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "test_total_sales_df = pd.merge(test_total_sales_df, train_day_mean_df, how=\"left\", on=[\"month\", \"day\"])\n# Applying the day of week ratios back, thus increasing the number of sales if its a Sataurday for example\ntest_total_sales_df = pd.merge(test_total_sales_df, day_of_week_ratio.reset_index(), how=\"left\", on=\"day_of_week\")\ntest_total_sales_df[\"num_sold\"] = test_total_sales_df[\"adjusted_num_sold\"] * test_total_sales_df[\"day_of_week_ratios\"]", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-05T21:07:16.038688Z", "iopub.execute_input": "2025-01-05T21:07:16.039063Z", "iopub.status.idle": "2025-01-05T21:07:16.059495Z", "shell.execute_reply.started": "2025-01-05T21:07:16.039025Z", "shell.execute_reply": "2025-01-05T21:07:16.058376Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "Lets take a look at our forecast:", "metadata": {}}, {"cell_type": "code", "source": "f,ax = plt.subplots(figsize=(20,10))\nsns.lineplot(data = pd.concat([train_df_imputed,test_total_sales_df]).reset_index(drop=True), x=\"date\", y=\"num_sold\", linewidth=0.6);\nax.axvline(pd.to_datetime(\"2017-01-01\"), color='black', linestyle='--');", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-05T21:07:16.060755Z", "iopub.execute_input": "2025-01-05T21:07:16.061168Z", "iopub.status.idle": "2025-01-05T21:07:16.97115Z", "shell.execute_reply.started": "2025-01-05T21:07:16.061129Z", "shell.execute_reply": "2025-01-05T21:07:16.969957Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "## Disaggregating Total Sales Forecast", "metadata": {}}, {"cell_type": "markdown", "source": "We need to divide the total sales forecast for each day between the categorical variables so we get the forecast for each day, country, product and store.", "metadata": {}}, {"cell_type": "code", "source": "# Adding in the store ratios\nstore_weights_df = store_weights.reset_index()\ntest_sub_df = pd.merge(test_df, test_total_sales_df, how=\"left\", on=\"date\")\ntest_sub_df = test_sub_df.rename(columns = {\"num_sold\":\"day_num_sold\"})\n# Adding in the product ratios\ntest_sub_df = pd.merge(test_sub_df, store_weights_df, how=\"left\", on=\"store\")\ntest_sub_df = test_sub_df.rename(columns = {\"num_sold\":\"store_ratio\"})\n# Adding in the country ratios\ntest_sub_df[\"year\"] = test_sub_df[\"date\"].dt.year\ntest_sub_df = pd.merge(test_sub_df, gdp_per_capita_filtered_ratios_df, how=\"left\", on=[\"year\", \"country\"])\ntest_sub_df = test_sub_df.rename(columns = {\"ratio\":\"country_ratio\"})\n# Adding in the product ratio\ntest_sub_df = pd.merge(test_sub_df, forecasted_ratios_df, how=\"left\", on=[\"date\", \"product\"])\ntest_sub_df = test_sub_df.rename(columns = {\"ratios\":\"product_ratio\"})\n\n# Disaggregating the forecast\ntest_sub_df[\"num_sold\"] = test_sub_df[\"day_num_sold\"] * test_sub_df[\"store_ratio\"] * test_sub_df[\"country_ratio\"] * test_sub_df[\"product_ratio\"]\ntest_sub_df[\"num_sold\"] = test_sub_df[\"num_sold\"].round()", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-05T21:07:16.972088Z", "iopub.execute_input": "2025-01-05T21:07:16.972387Z", "iopub.status.idle": "2025-01-05T21:07:17.100808Z", "shell.execute_reply.started": "2025-01-05T21:07:16.972359Z", "shell.execute_reply": "2025-01-05T21:07:17.099788Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "def plot_individual_ts(df):\n    colour_map = {\"Canada\": \"blue\", \"Finland\": \"orange\", \"Italy\": \"green\", \"Kenya\":\"red\", \"Norway\": \"purple\", \"Singapore\": \"brown\"}\n    for country in df[\"country\"].unique():\n        f,axes = plt.subplots(df[\"store\"].nunique()*df[\"product\"].nunique(),figsize=(20,70))\n        count = 0\n        for store in df[\"store\"].unique():\n            for product in df[\"product\"].unique():\n                plot_df = df.loc[(df[\"product\"] == product) & (df[\"country\"] == country) & (df[\"store\"] == store)]\n                sns.lineplot(data = plot_df, x=\"date\", y=\"num_sold\", linewidth=0.5, ax=axes[count], color=colour_map[country])\n                axes[count].set_title(f\"{country} - {store} - {product}\")\n                axes[count].axvline(pd.to_datetime(\"2017-01-01\"), color='black', linestyle='--');\n                count+=1", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-05T21:07:17.101834Z", "iopub.execute_input": "2025-01-05T21:07:17.102136Z", "iopub.status.idle": "2025-01-05T21:07:17.109301Z", "shell.execute_reply.started": "2025-01-05T21:07:17.10211Z", "shell.execute_reply": "2025-01-05T21:07:17.108095Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "Lets take a look at each of the 90 time series to see if the forecasts look reasonable:", "metadata": {}}, {"cell_type": "code", "source": "plot_individual_ts(pd.concat([original_train_df_imputed,test_sub_df]).reset_index(drop=True))", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-05T21:07:17.110546Z", "iopub.execute_input": "2025-01-05T21:07:17.110979Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "## Submission", "metadata": {}}, {"cell_type": "code", "source": "submission = pd.read_csv(\"./input/playground-series-s5e1/sample_submission.csv\")\nsubmission[\"num_sold\"] = test_sub_df[\"num_sold\"]\n\nsubmission.to_csv('submission.csv', index = False)\n\ndisplay(submission.head(2))", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-05T00:11:23.753501Z", "iopub.execute_input": "2025-01-05T00:11:23.753914Z", "iopub.status.idle": "2025-01-05T00:11:23.911409Z", "shell.execute_reply.started": "2025-01-05T00:11:23.753884Z", "shell.execute_reply": "2025-01-05T00:11:23.910407Z"}}, "outputs": [], "execution_count": null}]}