{"metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "language_info": {"name": "python", "version": "3.7.12", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}, "kaggle": {"accelerator": "none", "dataSources": [{"sourceId": 85723, "databundleVersionId": 10652996, "sourceType": "competition"}, {"sourceId": 3325325, "sourceType": "datasetVersion", "datasetId": 2007861}], "dockerImageVersionId": 30235, "isInternetEnabled": true, "language": "python", "sourceType": "notebook", "isGpuEnabled": false}}, "nbformat_minor": 4, "nbformat": 4, "cells": [{"cell_type": "markdown", "source": "**Credit to @Cabaxiom 's [excellent notebook(version 15)](https://www.kaggle.com/code/cabaxiom/s5e1-eda-and-linear-regression-baseline) about Linear Regression.**\n\nThe following code is based on his notebook and has been modified as follows.\n\n\n**(1)Ridge-> Lasso: 0.05726->0.05709**\n\n**(2)add holiday: 0.05709->0.05706**\n\nRelated discussion: [here](https://www.kaggle.com/competitions/playground-series-s5e1/discussion/555496#3091064) and [here](https://www.kaggle.com/competitions/playground-series-s5e1/discussion/554680) \n\n**(3)subtract a value  for Kenya's GDP ratio: 0.05706->0.05456**\n\nRelated discussion: [here](https://www.kaggle.com/competitions/playground-series-s5e1/discussion/555500)\n\nPS: If you  add the the third skill  to @Cabaxiom 's [\"no model\" (version 5)](https://www.kaggle.com/code/cabaxiom/s5e1-previous-years-baseline-no-model), you will get **0.05663->0.05410**.\n\n**(4)Use weekly means to normalize the data: 0.05456->0.05448**\n\n**(5)add half year sin/cos: 0.05448->0.05397**\n\n**(6)add 2~4 year sin/cos: 0.05397->0.05387**\n\n(Inspired by @kdmitrie 's notebook \"[PGS501: Model 1. Time series decomposition\n(version 11)](https://www.kaggle.com/code/kdmitrie/pgs501-model-1-time-series-decomposition)\"\n\n**(7)add 3.5 and 1/4 year sin/cos: 0.05387->0.5391**\n\nThis skill is not work well here, but it work in @kdmitrie 's notebook \"[PGS501: Model 1. Time series decomposition\n(version 11)](https://www.kaggle.com/code/kdmitrie/pgs501-model-1-time-series-decomposition)\"\n\n\n**(8)[Ongoing] try adding some nonlinearity: 0.05387->?**\n\n---\n\nOnce again, I extend my gratitude to @Cabaxiom for the two outstanding notebooks, which also contain a wealth of excellent EDA that is highly commendable.", "metadata": {"_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5", "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19", "execution": {"iopub.status.busy": "2025-01-09T13:24:57.642736Z", "iopub.execute_input": "2025-01-09T13:24:57.643201Z", "iopub.status.idle": "2025-01-09T13:24:58.539568Z", "shell.execute_reply.started": "2025-01-09T13:24:57.64309Z", "shell.execute_reply": "2025-01-09T13:24:58.538351Z"}}}, {"cell_type": "code", "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.linear_model import Ridge,Lasso\n\nsns.set_style('darkgrid')", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-20T15:28:02.274768Z", "iopub.execute_input": "2025-01-20T15:28:02.275196Z", "iopub.status.idle": "2025-01-20T15:28:02.281769Z", "shell.execute_reply.started": "2025-01-20T15:28:02.275159Z", "shell.execute_reply": "2025-01-20T15:28:02.280428Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "train_df = pd.read_csv(\"./input/playground-series-s5e1/train.csv\", parse_dates=[\"date\"])\noriginal_train_df = train_df.copy()\ntest_df = pd.read_csv(\"./input/playground-series-s5e1/test.csv\", parse_dates=[\"date\"])", "metadata": {"execution": {"iopub.status.busy": "2025-01-20T15:28:02.296955Z", "iopub.execute_input": "2025-01-20T15:28:02.297376Z", "iopub.status.idle": "2025-01-20T15:28:02.643372Z", "shell.execute_reply.started": "2025-01-20T15:28:02.297339Z", "shell.execute_reply": "2025-01-20T15:28:02.642092Z"}, "trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "gdp_per_capita_df = pd.read_csv(\"./input/world-gdpgdp-gdp-per-capita-and-annual-growths/gdp_per_capita.csv\")\n\nyears =  [\"2010\", \"2011\", \"2012\", \"2013\", \"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020\"]\ngdp_per_capita_filtered_df = gdp_per_capita_df.loc[gdp_per_capita_df[\"Country Name\"].isin(train_df[\"country\"].unique()), [\"Country Name\"] + years].set_index(\"Country Name\")\ngdp_per_capita_filtered_df[\"2010_ratio\"] = gdp_per_capita_filtered_df[\"2010\"] / gdp_per_capita_filtered_df.sum()[\"2010\"]\nfor year in years:\n    gdp_per_capita_filtered_df[f\"{year}_ratio\"] = gdp_per_capita_filtered_df[year] / gdp_per_capita_filtered_df.sum()[year]\ngdp_per_capita_filtered_ratios_df = gdp_per_capita_filtered_df[[i+\"_ratio\" for i in years]]\ngdp_per_capita_filtered_ratios_df.columns = [int(i) for i in years]\ngdp_per_capita_filtered_ratios_df = gdp_per_capita_filtered_ratios_df.unstack().reset_index().rename(columns = {\"level_0\": \"year\", 0: \"ratio\", \"Country Name\": \"country\"})\ngdp_per_capita_filtered_ratios_df['year'] = pd.to_datetime(gdp_per_capita_filtered_ratios_df['year'], format='%Y')\n\n# For plotting purposes\ngdp_per_capita_filtered_ratios_df_2 = gdp_per_capita_filtered_ratios_df.copy()\ngdp_per_capita_filtered_ratios_df_2[\"year\"] = pd.to_datetime(gdp_per_capita_filtered_ratios_df_2['year'].astype(str)) + pd.offsets.YearEnd(1)\ngdp_per_capita_filtered_ratios_df = pd.concat([gdp_per_capita_filtered_ratios_df, gdp_per_capita_filtered_ratios_df_2]).reset_index()", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-20T15:28:02.645457Z", "iopub.execute_input": "2025-01-20T15:28:02.645825Z", "iopub.status.idle": "2025-01-20T15:28:02.709408Z", "shell.execute_reply.started": "2025-01-20T15:28:02.645791Z", "shell.execute_reply": "2025-01-20T15:28:02.708391Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "gdp_per_capita_filtered_ratios_df_2[\"year\"] = gdp_per_capita_filtered_ratios_df_2[\"year\"].dt.year", "metadata": {"execution": {"iopub.status.busy": "2025-01-20T15:28:02.711049Z", "iopub.execute_input": "2025-01-20T15:28:02.711484Z", "iopub.status.idle": "2025-01-20T15:28:02.71853Z", "shell.execute_reply.started": "2025-01-20T15:28:02.711443Z", "shell.execute_reply": "2025-01-20T15:28:02.717313Z"}, "trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "train_df_imputed = train_df.copy()\nmissing_value_ids = train_df.loc[train_df[\"num_sold\"].isna(), \"id\"].values\nprint(f\"Missing values remaining: {train_df_imputed['num_sold'].isna().sum()}\")\n\ntrain_df_imputed[\"year\"] = train_df_imputed[\"date\"].dt.year\nfor year in train_df_imputed[\"year\"].unique():\n    # Impute Time Series 1 (Canada, Discount Stickers, Holographic Goose)\n    target_ratio = gdp_per_capita_filtered_ratios_df_2.loc[(gdp_per_capita_filtered_ratios_df_2[\"year\"] == year) & (gdp_per_capita_filtered_ratios_df_2[\"country\"] == \"Norway\"), \"ratio\"].values[0] # Using Norway as should have the best precision\n    current_raito = gdp_per_capita_filtered_ratios_df_2.loc[(gdp_per_capita_filtered_ratios_df_2[\"year\"] == year) & (gdp_per_capita_filtered_ratios_df_2[\"country\"] == \"Canada\"), \"ratio\"].values[0]\n    ratio_can = current_raito / target_ratio\n    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Canada\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year), \"num_sold\"] * ratio_can).values\n    \n    # Impute Time Series 2 (Only Missing Values)\n    current_ts =  train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Canada\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year)]\n    missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Canada\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] * ratio_can).values\n\n    # Impute Time Series 3 (Only Missing Values)\n    current_ts =  train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Canada\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year)]\n    missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Canada\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] * ratio_can).values\n    \n    # Impute Time Series 4 (Kenya, Discount Stickers, Holographic Goose)\n    current_raito = gdp_per_capita_filtered_ratios_df_2.loc[(gdp_per_capita_filtered_ratios_df_2[\"year\"] == year) & (gdp_per_capita_filtered_ratios_df_2[\"country\"] == \"Kenya\"), \"ratio\"].values[0]\n    ratio_ken = current_raito / target_ratio\n    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Holographic Goose\")& (train_df_imputed[\"year\"] == year), \"num_sold\"] * ratio_ken).values\n\n    # Impute Time Series 5 (Only Missing Values)\n    current_ts = train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year)]\n    missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] * ratio_ken).values\n\n    # Impute Time Series 6 (Only Missing Values)\n    current_ts = train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year)]\n    missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] * ratio_ken).values\n\n    # Impute Time Series 7 (Only Missing Values)\n    current_ts = train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Kerneler\") & (train_df_imputed[\"year\"] == year)]\n    missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Kerneler\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Kerneler\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] * ratio_ken).values\n    \nprint(f\"Missing values remaining: {train_df_imputed['num_sold'].isna().sum()}\")", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-20T15:28:02.722067Z", "iopub.execute_input": "2025-01-20T15:28:02.722561Z", "iopub.status.idle": "2025-01-20T15:28:09.736542Z", "shell.execute_reply.started": "2025-01-20T15:28:02.722515Z", "shell.execute_reply": "2025-01-20T15:28:09.735296Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "missing_rows = train_df_imputed.loc[train_df_imputed[\"num_sold\"].isna()]\ndisplay(missing_rows)\ntrain_df_imputed.loc[train_df_imputed[\"id\"] == 23719, \"num_sold\"] = 4\ntrain_df_imputed.loc[train_df_imputed[\"id\"] == 207003, \"num_sold\"] = 195\n\nprint(f\"Missing values remaining: {train_df_imputed['num_sold'].isna().sum()}\")", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-20T15:28:09.738096Z", "iopub.execute_input": "2025-01-20T15:28:09.738425Z", "iopub.status.idle": "2025-01-20T15:28:09.760726Z", "shell.execute_reply.started": "2025-01-20T15:28:09.738394Z", "shell.execute_reply": "2025-01-20T15:28:09.759422Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "store_weights = train_df_imputed.groupby(\"store\")[\"num_sold\"].sum()/train_df_imputed[\"num_sold\"].sum()\nstore_weights", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-20T15:28:09.76206Z", "iopub.execute_input": "2025-01-20T15:28:09.762389Z", "iopub.status.idle": "2025-01-20T15:28:09.792482Z", "shell.execute_reply.started": "2025-01-20T15:28:09.762358Z", "shell.execute_reply": "2025-01-20T15:28:09.791331Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "product_df = train_df_imputed.groupby([\"date\",\"product\"])[\"num_sold\"].sum().reset_index()\nproduct_ratio_df = product_df.pivot(index=\"date\", columns=\"product\", values=\"num_sold\")\nproduct_ratio_df = product_ratio_df.apply(lambda x: x/x.sum(),axis=1)\nproduct_ratio_df = product_ratio_df.stack().rename(\"ratios\").reset_index()\nproduct_ratio_df.head(4)", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-20T15:28:09.794284Z", "iopub.execute_input": "2025-01-20T15:28:09.794738Z", "iopub.status.idle": "2025-01-20T15:28:10.512587Z", "shell.execute_reply.started": "2025-01-20T15:28:09.794692Z", "shell.execute_reply": "2025-01-20T15:28:10.511375Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "train_df_imputed", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-20T15:28:10.514153Z", "iopub.execute_input": "2025-01-20T15:28:10.514582Z", "iopub.status.idle": "2025-01-20T15:28:10.534961Z", "shell.execute_reply.started": "2025-01-20T15:28:10.514541Z", "shell.execute_reply": "2025-01-20T15:28:10.533618Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "original_train_df_imputed = train_df_imputed.copy()\ntrain_df_imputed = train_df_imputed.groupby([\"date\"])[\"num_sold\"].sum().reset_index()", "metadata": {"execution": {"iopub.status.busy": "2025-01-20T15:28:10.536355Z", "iopub.execute_input": "2025-01-20T15:28:10.536647Z", "iopub.status.idle": "2025-01-20T15:28:10.561856Z", "shell.execute_reply.started": "2025-01-20T15:28:10.53662Z", "shell.execute_reply": "2025-01-20T15:28:10.560609Z"}, "trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "train_df_imputed", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-20T15:28:10.566893Z", "iopub.execute_input": "2025-01-20T15:28:10.56731Z", "iopub.status.idle": "2025-01-20T15:28:10.582192Z", "shell.execute_reply.started": "2025-01-20T15:28:10.567278Z", "shell.execute_reply": "2025-01-20T15:28:10.580984Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "train_df_imputed[\"day_of_week\"] = train_df_imputed[\"date\"].dt.dayofweek\nday_of_week_ratio = (train_df_imputed.groupby(\"day_of_week\")[\"num_sold\"].mean() / train_df_imputed.groupby(\"day_of_week\")[\"num_sold\"].mean().mean()).rename(\"day_of_week_ratios\")\ndisplay(day_of_week_ratio)\ntrain_df_imputed = pd.merge(train_df_imputed, day_of_week_ratio, how=\"left\", on=\"day_of_week\")\ntrain_df_imputed[\"num_sold\"] = train_df_imputed[\"num_sold\"] / train_df_imputed[\"day_of_week_ratios\"]\ntrain_df_imputed = train_df_imputed.drop(\"day_of_week_ratios\",axis=1)# we don't need it in training.", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-20T15:28:10.584014Z", "iopub.execute_input": "2025-01-20T15:28:10.584464Z", "iopub.status.idle": "2025-01-20T15:28:10.610147Z", "shell.execute_reply.started": "2025-01-20T15:28:10.584428Z", "shell.execute_reply": "2025-01-20T15:28:10.608841Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "test_total_sales_df = test_df.groupby([\"date\"])[\"id\"].first().reset_index().drop(columns=\"id\")\ntest_total_sales_dates = test_total_sales_df[[\"date\"]]", "metadata": {"execution": {"iopub.status.busy": "2025-01-20T15:28:10.611769Z", "iopub.execute_input": "2025-01-20T15:28:10.612225Z", "iopub.status.idle": "2025-01-20T15:28:10.630896Z", "shell.execute_reply.started": "2025-01-20T15:28:10.612182Z", "shell.execute_reply": "2025-01-20T15:28:10.62969Z"}, "trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "def feature_engineer(df):\n    new_df = df.copy()\n    new_df[\"month\"] = df[\"date\"].dt.month\n    new_df[\"month_sin\"] = np.sin(new_df['month'] * (2 * np.pi / 12))\n    new_df[\"month_cos\"] = np.cos(new_df['month'] * (2 * np.pi / 12))\n    new_df[\"day_of_week\"] = df[\"date\"].dt.dayofweek\n    new_df[\"day_of_week\"] = new_df[\"day_of_week\"].apply(lambda x: 0 if x<=3 else(1 if x==4 else (2 if x==5 else (3))))    \n    new_df[\"day_of_year\"] = df['date'].apply(\n        lambda x: x.timetuple().tm_yday if not (x.is_leap_year and x.month > 2) else x.timetuple().tm_yday - 1\n    )\n\n    new_df['day_sin4'] = np.sin(new_df['day_of_year'] * (8 * np.pi /  365.0))\n    new_df['day_cos4'] = np.cos(new_df['day_of_year'] * (8 * np.pi /  365.0))\n    new_df['day_sin3'] = np.sin(new_df['day_of_year'] * (6 * np.pi /  365.0))\n    new_df['day_cos3'] = np.cos(new_df['day_of_year'] * (6 * np.pi /  365.0))\n    new_df['day_sin2'] = np.sin(new_df['day_of_year'] * (4 * np.pi /  365.0))\n    new_df['day_cos2'] = np.cos(new_df['day_of_year'] * (4 * np.pi /  365.0))\n    new_df['day_sin'] = np.sin(new_df['day_of_year'] * (2 * np.pi /  365.0))\n    new_df['day_cos'] = np.cos(new_df['day_of_year'] * (2 * np.pi /  365.0)) \n    new_df['day_sin_0.5'] = np.sin(new_df['day_of_year'] * (1 * np.pi /  365.0))\n    new_df['day_cos_0.5'] = np.cos(new_df['day_of_year'] * (1 * np.pi /  365.0))    \n    new_df[\"important_dates\"] = new_df[\"day_of_year\"].apply(lambda x: x if x in [1,2,3,4,5,6,7,8,9,10,99, 100, 101, 125,126,355,256,357,358,359,360,361,362,363,364,365] else 0)\n    \n    new_df = new_df.drop(columns=[\"date\",\"month\",\"day_of_year\"])\n    new_df = pd.get_dummies(new_df, columns = [\"important_dates\",\"day_of_week\"], drop_first=True)\n\n    new_df['combine_feat1']=new_df.important_dates_4*new_df.month_sin\n    return new_df", "metadata": {"execution": {"iopub.status.busy": "2025-01-20T15:28:10.632608Z", "iopub.execute_input": "2025-01-20T15:28:10.633066Z", "iopub.status.idle": "2025-01-20T15:28:10.649375Z", "shell.execute_reply.started": "2025-01-20T15:28:10.633009Z", "shell.execute_reply": "2025-01-20T15:28:10.648133Z"}, "trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "train_total_sales_df = feature_engineer(train_df_imputed)\ntest_total_sales_df = feature_engineer(test_total_sales_df)", "metadata": {"execution": {"iopub.status.busy": "2025-01-20T15:28:10.650845Z", "iopub.execute_input": "2025-01-20T15:28:10.651255Z", "iopub.status.idle": "2025-01-20T15:28:10.724479Z", "shell.execute_reply.started": "2025-01-20T15:28:10.651221Z", "shell.execute_reply": "2025-01-20T15:28:10.723318Z"}, "trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "import holidays\ntrain_df_tmp = train_df.copy()\ntest_df_tmp = test_df.copy()\nalpha2 = dict(zip(np.sort(train_df.country.unique()), ['CA', 'FI', 'IT', 'KE', 'NO', 'SG']))\nh = {c: holidays.country_holidays(a, years=range(2010, 2020)) for c, a in alpha2.items()}\ntrain_df_tmp['is_holiday'] = 0\ntest_df_tmp['is_holiday'] = 0\nfor c in alpha2:\n    train_df_tmp.loc[train_df_tmp.country==c, 'is_holiday'] = train_df_tmp.date.isin(h[c]).astype(int)\n    test_df_tmp.loc[test_df_tmp.country==c, 'is_holiday'] = test_df_tmp.date.isin(h[c]).astype(int)\n\n\ntrain_total_sales_df['is_holiday'] = (train_df_tmp.groupby([\"date\"])[\"is_holiday\"].sum().reset_index())[\"is_holiday\"]\ntest_total_sales_df['is_holiday'] = (test_df_tmp.groupby([\"date\"])[\"is_holiday\"].sum().reset_index())[\"is_holiday\"]", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-20T15:28:10.72602Z", "iopub.execute_input": "2025-01-20T15:28:10.726366Z", "iopub.status.idle": "2025-01-20T15:28:10.982788Z", "shell.execute_reply.started": "2025-01-20T15:28:10.726335Z", "shell.execute_reply": "2025-01-20T15:28:10.981804Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "y = train_total_sales_df[\"num_sold\"]\nX = train_total_sales_df.drop(columns=\"num_sold\")\nX_test = test_total_sales_df", "metadata": {"execution": {"iopub.status.busy": "2025-01-20T15:28:10.984259Z", "iopub.execute_input": "2025-01-20T15:28:10.984673Z", "iopub.status.idle": "2025-01-20T15:28:10.993127Z", "shell.execute_reply.started": "2025-01-20T15:28:10.984632Z", "shell.execute_reply": "2025-01-20T15:28:10.991773Z"}, "trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "X.head()", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-20T15:28:10.994685Z", "iopub.execute_input": "2025-01-20T15:28:10.995166Z", "iopub.status.idle": "2025-01-20T15:28:11.023566Z", "shell.execute_reply.started": "2025-01-20T15:28:10.995122Z", "shell.execute_reply": "2025-01-20T15:28:11.022086Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "def linear_function(X):\n    a = np.zeros(X.shape[1])\n    a[0] = 210.148\n    a[12] = 19472.7071\n    a[34] = 19472.7071\n    a[35] = 19472.7071\n    a[36] = 19472.7071\n    a[37] = 19472.7071\n    a[41] = 1.0000\n    b = 65141.2073\n    return X @ a + b", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-20T15:28:11.025141Z", "iopub.execute_input": "2025-01-20T15:28:11.025475Z", "iopub.status.idle": "2025-01-20T15:28:11.035932Z", "shell.execute_reply.started": "2025-01-20T15:28:11.025444Z", "shell.execute_reply": "2025-01-20T15:28:11.034667Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "from sklearn.metrics import mean_absolute_percentage_error\npreds = linear_function(X_test)\ntest_total_sales_dates[\"num_sold\"] = preds\nmean_absolute_percentage_error(y,linear_function(X))\n", "metadata": {"execution": {"iopub.status.busy": "2025-01-20T15:28:11.037416Z", "iopub.execute_input": "2025-01-20T15:28:11.037789Z", "iopub.status.idle": "2025-01-20T15:28:11.063292Z", "shell.execute_reply.started": "2025-01-20T15:28:11.037757Z", "shell.execute_reply": "2025-01-20T15:28:11.061082Z"}, "trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "product_ratio_2017_df = product_ratio_df.loc[product_ratio_df[\"date\"].dt.year == 2015].copy()\nproduct_ratio_2018_df = product_ratio_df.loc[product_ratio_df[\"date\"].dt.year == 2016].copy()\nproduct_ratio_2019_df = product_ratio_df.loc[product_ratio_df[\"date\"].dt.year == 2015].copy()\n\nproduct_ratio_2017_df[\"date\"] = product_ratio_2017_df[\"date\"] + pd.DateOffset(years=2)\nproduct_ratio_2018_df[\"date\"] = product_ratio_2018_df[\"date\"] + pd.DateOffset(years=2)\nproduct_ratio_2019_df[\"date\"] =  product_ratio_2019_df[\"date\"] + pd.DateOffset(years=4)\n\nforecasted_ratios_df = pd.concat([product_ratio_2017_df, product_ratio_2018_df, product_ratio_2019_df])", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-20T15:28:11.067603Z", "iopub.execute_input": "2025-01-20T15:28:11.069973Z", "iopub.status.idle": "2025-01-20T15:28:11.108034Z", "shell.execute_reply.started": "2025-01-20T15:28:11.069882Z", "shell.execute_reply": "2025-01-20T15:28:11.106414Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "store_weights_df = store_weights.reset_index()\ntest_sub_df = pd.merge(test_df, test_total_sales_dates, how=\"left\", on=\"date\")\ntest_sub_df = test_sub_df.rename(columns = {\"num_sold\":\"day_num_sold\"})\n# Adding in the product ratios\ntest_sub_df = pd.merge(test_sub_df, store_weights_df, how=\"left\", on=\"store\")\ntest_sub_df = test_sub_df.rename(columns = {\"num_sold\":\"store_ratio\"})\n# Adding in the country ratios\ntest_sub_df[\"year\"] = test_sub_df[\"date\"].dt.year\ntest_sub_df = pd.merge(test_sub_df, gdp_per_capita_filtered_ratios_df_2, how=\"left\", on=[\"year\", \"country\"])\ntest_sub_df = test_sub_df.rename(columns = {\"ratio\":\"country_ratio\"})\n# Adding in the product ratio\ntest_sub_df = pd.merge(test_sub_df, forecasted_ratios_df, how=\"left\", on=[\"date\", \"product\"])\ntest_sub_df = test_sub_df.rename(columns = {\"ratios\":\"product_ratio\"})\n\n# Adding in the week ratio\ntest_sub_df[\"day_of_week\"] = test_sub_df[\"date\"].dt.dayofweek\ntest_sub_df = pd.merge(test_sub_df, day_of_week_ratio.reset_index(), how=\"left\", on=\"day_of_week\")\n\n\n# Disaggregating the forecast\ntest_sub_df.loc[test_sub_df['country'] == 'Kenya', 'country_ratio'] -= 0.0007/2\n\ntest_sub_df[\"num_sold\"] = test_sub_df[\"day_num_sold\"] *test_sub_df[\"day_of_week_ratios\"]* test_sub_df[\"store_ratio\"] * test_sub_df[\"country_ratio\"] * test_sub_df[\"product_ratio\"]\ntest_sub_df[\"num_sold\"] = test_sub_df[\"num_sold\"].round()\ndisplay(test_sub_df.head(2))", "metadata": {"execution": {"iopub.status.busy": "2025-01-20T15:28:11.109834Z", "iopub.execute_input": "2025-01-20T15:28:11.11134Z", "iopub.status.idle": "2025-01-20T15:28:11.313378Z", "shell.execute_reply.started": "2025-01-20T15:28:11.111293Z", "shell.execute_reply": "2025-01-20T15:28:11.312134Z"}, "trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "submission = pd.read_csv(\"./input/playground-series-s5e1/sample_submission.csv\")\nsubmission[\"num_sold\"] = test_sub_df[\"num_sold\"]\ndisplay(submission.head(2))", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-20T15:28:11.31511Z", "iopub.execute_input": "2025-01-20T15:28:11.315458Z", "iopub.status.idle": "2025-01-20T15:28:11.358933Z", "shell.execute_reply.started": "2025-01-20T15:28:11.315426Z", "shell.execute_reply": "2025-01-20T15:28:11.357769Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "submission.to_csv('submission.csv', index = False)", "metadata": {"execution": {"iopub.status.busy": "2025-01-20T15:28:11.360296Z", "iopub.execute_input": "2025-01-20T15:28:11.360624Z", "iopub.status.idle": "2025-01-20T15:28:11.522789Z", "shell.execute_reply.started": "2025-01-20T15:28:11.360594Z", "shell.execute_reply": "2025-01-20T15:28:11.521826Z"}, "trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "", "metadata": {"trusted": true}, "outputs": [], "execution_count": null}]}