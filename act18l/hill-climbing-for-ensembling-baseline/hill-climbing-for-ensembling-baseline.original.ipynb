{"metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "language_info": {"name": "python", "version": "3.7.12", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}, "kaggle": {"accelerator": "none", "dataSources": [{"sourceId": 85723, "databundleVersionId": 10652996, "sourceType": "competition"}, {"sourceId": 3325325, "sourceType": "datasetVersion", "datasetId": 2007861}, {"sourceId": 219013464, "sourceType": "kernelVersion"}, {"sourceId": 219539450, "sourceType": "kernelVersion"}, {"sourceId": 219621257, "sourceType": "kernelVersion"}], "dockerImageVersionId": 30235, "isInternetEnabled": true, "language": "python", "sourceType": "notebook", "isGpuEnabled": false}}, "nbformat_minor": 4, "nbformat": 4, "cells": [{"cell_type": "markdown", "source": "Credit to  @Cabaxiom's [notebook](https://www.kaggle.com/code/cabaxiom/s5e1-eda-and-linear-regression-baseline) , @Konstantin Dmitriev 's [noetbook](https://www.kaggle.com/code/kdmitrie/pgs501-model-2-additional-country-doy-factor) , @ Mikhail Naumov's [notebook](https://www.kaggle.com/code/mikhailnaumov/forecasting-sticker-sales-ensemble), @cdeotte 's [transformer notebook](https://www.kaggle.com/code/cdeotte/transformer-starter-lb-0-052)\n\n[**Based on my single model baseline**.](https://www.kaggle.com/code/act18l/linearregression-subtract-bias-makes-lb-0-054)\n\nFix typo.Update CNN.\n\nsubmission = {[hill_climbling(2linear_regression +3lgbm)\\*0.5+nonlinear\\*0.5]\\*0.5+(cnn\\*0.5+transformer\\*0.5)\\*0.5}\\*0.5+Konstantin's model*0.5\n\nsubmission = ((0.05387+0.05275)\\*0.5+(0.05298\\*0.5+0.05260\\*0.5)\\*0.5)+0.05034\\*0.5", "metadata": {"_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5", "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19", "execution": {"iopub.status.busy": "2025-01-09T13:24:57.642736Z", "iopub.execute_input": "2025-01-09T13:24:57.643201Z", "iopub.status.idle": "2025-01-09T13:24:58.539568Z", "shell.execute_reply.started": "2025-01-09T13:24:57.64309Z", "shell.execute_reply": "2025-01-09T13:24:58.538351Z"}}}, {"cell_type": "code", "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.linear_model import Ridge,Lasso\nfrom sklearn.metrics import mean_absolute_percentage_error\nfrom lightgbm import log_evaluation, early_stopping, LGBMRegressor\n\nsns.set_style('darkgrid')", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-28T13:48:36.607765Z", "iopub.execute_input": "2025-01-28T13:48:36.608183Z", "iopub.status.idle": "2025-01-28T13:48:36.615184Z", "shell.execute_reply.started": "2025-01-28T13:48:36.60815Z", "shell.execute_reply": "2025-01-28T13:48:36.613935Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "train_df = pd.read_csv(\"./input/playground-series-s5e1/train.csv\", parse_dates=[\"date\"])\noriginal_train_df = train_df.copy()\ntest_df = pd.read_csv(\"./input/playground-series-s5e1/test.csv\", parse_dates=[\"date\"])\nsub = pd.read_csv(\"./input/playground-series-s5e1/sample_submission.csv\")\n", "metadata": {"execution": {"iopub.status.busy": "2025-01-28T13:48:36.61754Z", "iopub.execute_input": "2025-01-28T13:48:36.61808Z", "iopub.status.idle": "2025-01-28T13:48:36.948758Z", "shell.execute_reply.started": "2025-01-28T13:48:36.618037Z", "shell.execute_reply": "2025-01-28T13:48:36.94757Z"}, "trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "gdp_per_capita_df = pd.read_csv(\"./input/world-gdpgdp-gdp-per-capita-and-annual-growths/gdp_per_capita.csv\")\n\nyears =  [\"2010\", \"2011\", \"2012\", \"2013\", \"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020\"]\ngdp_per_capita_filtered_df = gdp_per_capita_df.loc[gdp_per_capita_df[\"Country Name\"].isin(train_df[\"country\"].unique()), [\"Country Name\"] + years].set_index(\"Country Name\")\nfor year in years:\n    gdp_per_capita_filtered_df[f\"{year}_ratio\"] = gdp_per_capita_filtered_df[year] / gdp_per_capita_filtered_df.sum()[year]\ngdp_per_capita_filtered_ratios_df = gdp_per_capita_filtered_df[[i+\"_ratio\" for i in years]]\ngdp_per_capita_filtered_ratios_df.columns = [int(i) for i in years]\ngdp_per_capita_filtered_ratios_df = gdp_per_capita_filtered_ratios_df.unstack().reset_index().rename(columns = {\"level_0\": \"year\", 0: \"ratio\", \"Country Name\": \"country\"})\ngdp_per_capita_filtered_ratios_df['year'] = pd.to_datetime(gdp_per_capita_filtered_ratios_df['year'], format='%Y')\n\n# For plotting purposes\ngdp_per_capita_filtered_ratios_df_2 = gdp_per_capita_filtered_ratios_df.copy()\ngdp_per_capita_filtered_ratios_df_2[\"year\"] = pd.to_datetime(gdp_per_capita_filtered_ratios_df_2['year'].astype(str)) + pd.offsets.YearEnd(1)\ngdp_per_capita_filtered_ratios_df = pd.concat([gdp_per_capita_filtered_ratios_df, gdp_per_capita_filtered_ratios_df_2]).reset_index()", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-28T13:48:36.950556Z", "iopub.execute_input": "2025-01-28T13:48:36.95089Z", "iopub.status.idle": "2025-01-28T13:48:37.01219Z", "shell.execute_reply.started": "2025-01-28T13:48:36.950861Z", "shell.execute_reply": "2025-01-28T13:48:37.010979Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "gdp_per_capita_filtered_ratios_df_2[\"year\"] = gdp_per_capita_filtered_ratios_df_2[\"year\"].dt.year", "metadata": {"execution": {"iopub.status.busy": "2025-01-28T13:48:37.013346Z", "iopub.execute_input": "2025-01-28T13:48:37.013876Z", "iopub.status.idle": "2025-01-28T13:48:37.0203Z", "shell.execute_reply.started": "2025-01-28T13:48:37.013833Z", "shell.execute_reply": "2025-01-28T13:48:37.019103Z"}, "trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "## imputing", "metadata": {}}, {"cell_type": "code", "source": "train_df_imputed = train_df.copy()\nmissing_value_ids = train_df.loc[train_df[\"num_sold\"].isna(), \"id\"].values\nprint(f\"Missing values remaining: {train_df_imputed['num_sold'].isna().sum()}\")\n\ntrain_df_imputed[\"year\"] = train_df_imputed[\"date\"].dt.year\nfor year in train_df_imputed[\"year\"].unique():\n    # Impute Time Series 1 (Canada, Discount Stickers, Holographic Goose)\n    target_ratio = gdp_per_capita_filtered_ratios_df_2.loc[(gdp_per_capita_filtered_ratios_df_2[\"year\"] == year) & (gdp_per_capita_filtered_ratios_df_2[\"country\"] == \"Norway\"), \"ratio\"].values[0] # Using Norway as should have the best precision\n    current_raito = gdp_per_capita_filtered_ratios_df_2.loc[(gdp_per_capita_filtered_ratios_df_2[\"year\"] == year) & (gdp_per_capita_filtered_ratios_df_2[\"country\"] == \"Canada\"), \"ratio\"].values[0]\n    ratio_can = current_raito / target_ratio\n    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Canada\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year), \"num_sold\"] * ratio_can).values\n    \n    # Impute Time Series 2 (Only Missing Values)\n    current_ts =  train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Canada\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year)]\n    missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Canada\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] * ratio_can).values\n\n    # Impute Time Series 3 (Only Missing Values)\n    current_ts =  train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Canada\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year)]\n    missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Canada\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] * ratio_can).values\n    \n    # Impute Time Series 4 (Kenya, Discount Stickers, Holographic Goose)\n    current_raito = gdp_per_capita_filtered_ratios_df_2.loc[(gdp_per_capita_filtered_ratios_df_2[\"year\"] == year) & (gdp_per_capita_filtered_ratios_df_2[\"country\"] == \"Kenya\"), \"ratio\"].values[0]\n    ratio_ken = current_raito / target_ratio\n    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Holographic Goose\")& (train_df_imputed[\"year\"] == year), \"num_sold\"] * ratio_ken).values\n\n    # Impute Time Series 5 (Only Missing Values)\n    current_ts = train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year)]\n    missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] * ratio_ken).values\n\n    # Impute Time Series 6 (Only Missing Values)\n    current_ts = train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year)]\n    missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] * ratio_ken).values\n\n    # Impute Time Series 7 (Only Missing Values)\n    current_ts = train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Kerneler\") & (train_df_imputed[\"year\"] == year)]\n    missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Kerneler\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Kerneler\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] * ratio_ken).values\n    \nprint(f\"Missing values remaining: {train_df_imputed['num_sold'].isna().sum()}\")", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-28T13:48:37.024142Z", "iopub.execute_input": "2025-01-28T13:48:37.024644Z", "iopub.status.idle": "2025-01-28T13:48:43.875251Z", "shell.execute_reply.started": "2025-01-28T13:48:37.024601Z", "shell.execute_reply": "2025-01-28T13:48:43.874056Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "missing_rows = train_df_imputed.loc[train_df_imputed[\"num_sold\"].isna()]\ndisplay(missing_rows)\ntrain_df_imputed.loc[train_df_imputed[\"id\"] == 23719, \"num_sold\"] = 4\ntrain_df_imputed.loc[train_df_imputed[\"id\"] == 207003, \"num_sold\"] = 195\n\nprint(f\"Missing values remaining: {train_df_imputed['num_sold'].isna().sum()}\")", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-28T13:48:43.876697Z", "iopub.execute_input": "2025-01-28T13:48:43.877013Z", "iopub.status.idle": "2025-01-28T13:48:43.898048Z", "shell.execute_reply.started": "2025-01-28T13:48:43.876985Z", "shell.execute_reply": "2025-01-28T13:48:43.896874Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "store_weights = train_df_imputed.groupby(\"store\")[\"num_sold\"].sum()/train_df_imputed[\"num_sold\"].sum()\nstore_weights", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-28T13:48:43.899263Z", "iopub.execute_input": "2025-01-28T13:48:43.89961Z", "iopub.status.idle": "2025-01-28T13:48:43.931734Z", "shell.execute_reply.started": "2025-01-28T13:48:43.899581Z", "shell.execute_reply": "2025-01-28T13:48:43.930633Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "product_df = train_df_imputed.groupby([\"date\",\"product\"])[\"num_sold\"].sum().reset_index()\nproduct_ratio_df = product_df.pivot(index=\"date\", columns=\"product\", values=\"num_sold\")\nproduct_ratio_df = product_ratio_df.apply(lambda x: x/x.sum(),axis=1)\nproduct_ratio_df = product_ratio_df.stack().rename(\"ratios\").reset_index()\nproduct_ratio_df.head(4)", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-28T13:48:43.932939Z", "iopub.execute_input": "2025-01-28T13:48:43.933221Z", "iopub.status.idle": "2025-01-28T13:48:44.525691Z", "shell.execute_reply.started": "2025-01-28T13:48:43.933196Z", "shell.execute_reply": "2025-01-28T13:48:44.524371Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "train_df_imputed.head()", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-28T13:48:44.527061Z", "iopub.execute_input": "2025-01-28T13:48:44.527419Z", "iopub.status.idle": "2025-01-28T13:48:44.541695Z", "shell.execute_reply.started": "2025-01-28T13:48:44.527379Z", "shell.execute_reply": "2025-01-28T13:48:44.540554Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "original_train_df_imputed = train_df_imputed.copy()\ntrain_df_imputed = train_df_imputed.groupby([\"date\"])[\"num_sold\"].sum().reset_index()", "metadata": {"execution": {"iopub.status.busy": "2025-01-28T13:48:44.543228Z", "iopub.execute_input": "2025-01-28T13:48:44.543702Z", "iopub.status.idle": "2025-01-28T13:48:44.568951Z", "shell.execute_reply.started": "2025-01-28T13:48:44.543654Z", "shell.execute_reply": "2025-01-28T13:48:44.567799Z"}, "trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "train_df_imputed.head()", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-28T13:48:44.574036Z", "iopub.execute_input": "2025-01-28T13:48:44.574356Z", "iopub.status.idle": "2025-01-28T13:48:44.58533Z", "shell.execute_reply.started": "2025-01-28T13:48:44.574329Z", "shell.execute_reply": "2025-01-28T13:48:44.58419Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "train_df_imputed[\"day_of_week\"] = train_df_imputed[\"date\"].dt.dayofweek\nday_of_week_ratio = (train_df_imputed.groupby(\"day_of_week\")[\"num_sold\"].mean() / train_df_imputed.groupby(\"day_of_week\")[\"num_sold\"].mean().mean()).rename(\"day_of_week_ratios\")\ndisplay(day_of_week_ratio)\ntrain_df_imputed = pd.merge(train_df_imputed, day_of_week_ratio, how=\"left\", on=\"day_of_week\")\ntrain_df_imputed[\"num_sold\"] = train_df_imputed[\"num_sold\"] / train_df_imputed[\"day_of_week_ratios\"]\ntrain_df_imputed = train_df_imputed.drop(\"day_of_week_ratios\",axis=1)# we don't need it in training.", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-28T13:48:44.586708Z", "iopub.execute_input": "2025-01-28T13:48:44.587045Z", "iopub.status.idle": "2025-01-28T13:48:44.609493Z", "shell.execute_reply.started": "2025-01-28T13:48:44.587016Z", "shell.execute_reply": "2025-01-28T13:48:44.608339Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "test_total_sales_df = test_df.groupby([\"date\"])[\"id\"].first().reset_index().drop(columns=\"id\")\ntest_total_sales_dates = test_total_sales_df[[\"date\"]]", "metadata": {"execution": {"iopub.status.busy": "2025-01-28T13:48:44.610538Z", "iopub.execute_input": "2025-01-28T13:48:44.610914Z", "iopub.status.idle": "2025-01-28T13:48:44.624856Z", "shell.execute_reply.started": "2025-01-28T13:48:44.610877Z", "shell.execute_reply": "2025-01-28T13:48:44.623817Z"}, "trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "## feature engineering", "metadata": {}}, {"cell_type": "code", "source": "def feature_engineer(df):\n    new_df = df.copy()\n    new_df[\"month\"] = df[\"date\"].dt.month\n    new_df[\"month_sin\"] = np.sin(new_df['month'] * (2 * np.pi / 12))\n    new_df[\"month_cos\"] = np.cos(new_df['month'] * (2 * np.pi / 12))\n    new_df[\"day_of_week\"] = df[\"date\"].dt.dayofweek\n    new_df[\"day_of_week\"] = new_df[\"day_of_week\"].apply(lambda x: 0 if x<=3 else(1 if x==4 else (2 if x==5 else (3))))    \n    new_df[\"day_of_year\"] = df['date'].apply(\n        lambda x: x.timetuple().tm_yday if not (x.is_leap_year and x.month > 2) else x.timetuple().tm_yday - 1\n    )\n\n    new_df['day_sin4'] = np.sin(new_df['day_of_year'] * (8 * np.pi /  365.0))\n    new_df['day_cos4'] = np.cos(new_df['day_of_year'] * (8 * np.pi /  365.0))\n    new_df['day_sin3'] = np.sin(new_df['day_of_year'] * (6 * np.pi /  365.0))\n    new_df['day_cos3'] = np.cos(new_df['day_of_year'] * (6 * np.pi /  365.0))\n    new_df['day_sin2'] = np.sin(new_df['day_of_year'] * (4 * np.pi /  365.0))\n    new_df['day_cos2'] = np.cos(new_df['day_of_year'] * (4 * np.pi /  365.0))\n    new_df['day_sin'] = np.sin(new_df['day_of_year'] * (2 * np.pi /  365.0))\n    new_df['day_cos'] = np.cos(new_df['day_of_year'] * (2 * np.pi /  365.0)) \n    new_df['day_sin_0.5'] = np.sin(new_df['day_of_year'] * (1 * np.pi /  365.0))\n    new_df['day_cos_0.5'] = np.cos(new_df['day_of_year'] * (1 * np.pi /  365.0))    \n    new_df[\"important_dates\"] = new_df[\"day_of_year\"].apply(lambda x: x if x in [1,2,3,4,5,6,7,8,9,10,99, 100, 101, 125,126,355,256,357,358,359,360,361,362,363,364,365] else 0)\n    \n    new_df = new_df.drop(columns=[\"date\",\"month\",\"day_of_year\"])\n    new_df = pd.get_dummies(new_df, columns = [\"important_dates\",\"day_of_week\"], drop_first=True)\n    \n    return new_df", "metadata": {"execution": {"iopub.status.busy": "2025-01-28T13:48:44.626202Z", "iopub.execute_input": "2025-01-28T13:48:44.626567Z", "iopub.status.idle": "2025-01-28T13:48:44.644139Z", "shell.execute_reply.started": "2025-01-28T13:48:44.626521Z", "shell.execute_reply": "2025-01-28T13:48:44.64295Z"}, "trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "train_total_sales_df = feature_engineer(train_df_imputed)\ntest_total_sales_df = feature_engineer(test_total_sales_df)", "metadata": {"execution": {"iopub.status.busy": "2025-01-28T13:48:44.645443Z", "iopub.execute_input": "2025-01-28T13:48:44.645926Z", "iopub.status.idle": "2025-01-28T13:48:44.715726Z", "shell.execute_reply.started": "2025-01-28T13:48:44.64588Z", "shell.execute_reply": "2025-01-28T13:48:44.71471Z"}, "trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "import holidays\ntrain_df_tmp = train_df.copy()\ntest_df_tmp = test_df.copy()\nalpha2 = dict(zip(np.sort(train_df.country.unique()), ['CA', 'FI', 'IT', 'KE', 'NO', 'SG']))\nh = {c: holidays.country_holidays(a, years=range(2010, 2020)) for c, a in alpha2.items()}\ntrain_df_tmp['is_holiday'] = 0\ntest_df_tmp['is_holiday'] = 0\nfor c in alpha2:\n    train_df_tmp.loc[train_df_tmp.country==c, 'is_holiday'] = train_df_tmp.date.isin(h[c]).astype(int)\n    test_df_tmp.loc[test_df_tmp.country==c, 'is_holiday'] = test_df_tmp.date.isin(h[c]).astype(int)\n\n\ntrain_total_sales_df['is_holiday'] = (train_df_tmp.groupby([\"date\"])[\"is_holiday\"].sum().reset_index())[\"is_holiday\"]\ntest_total_sales_df['is_holiday'] = (test_df_tmp.groupby([\"date\"])[\"is_holiday\"].sum().reset_index())[\"is_holiday\"]", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-28T13:48:44.716981Z", "iopub.execute_input": "2025-01-28T13:48:44.717299Z", "iopub.status.idle": "2025-01-28T13:48:44.973111Z", "shell.execute_reply.started": "2025-01-28T13:48:44.717256Z", "shell.execute_reply": "2025-01-28T13:48:44.971951Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "y = train_total_sales_df[\"num_sold\"]\nX = train_total_sales_df.drop(columns=\"num_sold\")\nX_test = test_total_sales_df", "metadata": {"execution": {"iopub.status.busy": "2025-01-28T13:48:44.974415Z", "iopub.execute_input": "2025-01-28T13:48:44.974771Z", "iopub.status.idle": "2025-01-28T13:48:44.981569Z", "shell.execute_reply.started": "2025-01-28T13:48:44.974741Z", "shell.execute_reply": "2025-01-28T13:48:44.980517Z"}, "trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "X.head()", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-28T13:48:44.983275Z", "iopub.execute_input": "2025-01-28T13:48:44.983716Z", "iopub.status.idle": "2025-01-28T13:48:45.013363Z", "shell.execute_reply.started": "2025-01-28T13:48:44.983664Z", "shell.execute_reply": "2025-01-28T13:48:45.012089Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "## hill climbing", "metadata": {}}, {"cell_type": "code", "source": "import numpy as np\n\nclass LinearModel1:\n    def fit(self, X, y=None):\n        pass\n\n    def predict(self, X):\n        return self.nonlinear(X)\n\n    def nonlinear(self, X):\n        y = (\n            X.important_dates_1 * 14514.153142903406 +\n            X.important_dates_6 +\n            X.important_dates_99 -\n            X.important_dates_359 +\n            X.important_dates_363 * 23245.947607750375 +\n            64795.970182750174 -\n            ((X.important_dates_362 + np.sqrt(X.important_dates_364 + X.important_dates_365)) *\n             ((X.important_dates_2 - X.important_dates_10) * (X.important_dates_101 + X.month_sin) +\n              X.important_dates_362 - 19142.637752625015))\n        )\n        return y\n\nclass LinearModel2:\n    def fit(self, X, y=None):\n        pass\n\n    def predict(self, X):\n        return self.nonlinear(X)\n\n    def nonlinear(self, X):\n        y = X.month_sin*210.148+\\\n            (X.important_dates_1+X.important_dates_362+X.important_dates_363+X.important_dates_364+X.important_dates_365)*19472.7071+\\\n            X.important_dates_4*X.month_sin+\\\n            65141.2073\n            \n        return y\n", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-28T13:48:45.015008Z", "iopub.execute_input": "2025-01-28T13:48:45.015472Z", "iopub.status.idle": "2025-01-28T13:48:45.025656Z", "shell.execute_reply.started": "2025-01-28T13:48:45.01541Z", "shell.execute_reply": "2025-01-28T13:48:45.024477Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "regressors = {\n    \"Ridge\" : Ridge(tol=1e-2, max_iter=1000000, random_state=0),\n    \"Lasso\" : Lasso(tol=1e-2, max_iter=1000000, random_state=0),\n    \"nonlinear1\" : LinearModel1(),\n    \"nonlinear2\" : LinearModel2(),\n    \"LGBM1\": LGBMRegressor(**{'objective': 'regression_l2',\n                               'metric': 'mape', \n                               'max_depth': 7,\n                               'num_leaves': 123, \n                               'min_child_samples': 21,\n                               'min_child_weight': 24,\n                               'colsample_bytree': 0.3641261996760593, \n                               'reg_alpha': 0.03632800166349373, \n                               'reg_lambda': 0.5287861861476272,\n                               'random_state': 42,\n                               'early_stopping_round':200,\n                               'verbose': -1,\n                               'boosting_type': 'gbdt',\n                               'n_estimators': 3000,\n                               'learning_rate': 0.01,\n                               }),\n    \"LGBM2\": LGBMRegressor(**{'objective': 'regression_l2',\n                               'metric': 'mape',\n                               'max_depth': 6,\n                               'num_leaves': 502,\n                               'min_child_samples': 23,\n                               'min_child_weight': 18, \n                               'colsample_bytree': 0.4714820876493163, \n                               'reg_alpha': 0.054972003081022576, \n                               'reg_lambda': 0.5774608955362155,\n                               'random_state': 42,\n                               'early_stopping_round': 200,\n                               'verbose': -1,\n                               'boosting_type': 'goss',\n                               'n_estimators': 3000,\n                               'learning_rate': 0.01,\n                              }),\n    \"LGBM3\": LGBMRegressor(**{'objective': 'regression_l2', \n                               'metric': 'mape',\n                               'max_depth': 14,\n                               'num_leaves': 279,\n                               'min_child_samples': 7,\n                               'min_child_weight': 24, \n                               'colsample_bytree': 0.43218993309765835,\n                               'reg_alpha': 0.42757392987472964,\n                               'reg_lambda': 0.9039762787446107,\n                               'random_state': 42,\n                               'early_stopping_round': 200,\n                               'verbose': -1,\n                               'boosting_type': 'goss',\n                               'n_estimators': 3000,\n                               'learning_rate': 0.01,\n                               }),\n    \n\n}", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-28T13:48:45.027575Z", "iopub.execute_input": "2025-01-28T13:48:45.028017Z", "iopub.status.idle": "2025-01-28T13:48:45.047427Z", "shell.execute_reply.started": "2025-01-28T13:48:45.027974Z", "shell.execute_reply": "2025-01-28T13:48:45.046401Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "import time\nfrom sklearn.model_selection import KFold\nn_folds = 5\n\nfor key, reg in regressors.items():\n    test_preds = np.zeros(len(X_test))\n    oof_full = y.copy()\n\n    start = time.time()\n\n    cv = KFold(n_splits=n_folds, shuffle=False)\n    score=0\n    for fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n        X_train, X_valid = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_valid = y.iloc[train_idx], y.iloc[val_idx]\n        start = time.time()\n        if \"LGBM\" in key:\n            reg.fit(X_train, y_train,\n                   eval_set = [(X_valid, y_valid)], \n                                       callbacks = [log_evaluation(0),\n                                                    early_stopping(200, verbose = False)\n                                                   ])\n            \n        else:\n            reg.fit(X_train, y_train)\n        oof_preds = reg.predict(X_valid)\n        score += mean_absolute_percentage_error(y_valid, oof_preds)/n_folds\n        oof_full[val_idx] = oof_preds\n        \n        test_preds += reg.predict(X_test)/n_folds\n    \n    # Stop timer\n    stop = time.time()\n    \n    print('Model:', key)\n    print('Average validation MAPE:', score)\n    print('Training time (mins):', np.round((stop - start)/60,2))\n    print('')\n    \n\n    oof_full.to_csv(f\"{key}_oof_preds.csv\", index=False)\n    ss = pd.DataFrame()\n    ss[\"num_sold\"] = test_preds\n    ss.to_csv(f\"{key}_test_preds.csv\", index=False)", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-28T13:48:45.04901Z", "iopub.execute_input": "2025-01-28T13:48:45.049423Z", "iopub.status.idle": "2025-01-28T13:49:16.684137Z", "shell.execute_reply.started": "2025-01-28T13:48:45.049383Z", "shell.execute_reply": "2025-01-28T13:49:16.68271Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "oof_df = pd.DataFrame(index=np.arange(len(y)))\nfor i in regressors.keys():\n    df = pd.read_csv(f\"/kaggle/working/{i}_oof_preds.csv\")\n    df.rename(columns={\"num_sold\": i}, inplace=True)\n    oof_df = pd.concat([oof_df,df], axis=1)\n    \n# Join test preds\ntest_preds = pd.DataFrame(index=np.arange(len(X_test)))\nfor i in regressors.keys():\n    df = pd.read_csv(f\"/kaggle/working/{i}_test_preds.csv\")\n    df.rename(columns={\"num_sold\": i}, inplace=True)\n    test_preds = pd.concat([test_preds,df], axis=1)\n    \noof_df.head(3)", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-28T13:49:16.685645Z", "iopub.execute_input": "2025-01-28T13:49:16.686699Z", "iopub.status.idle": "2025-01-28T13:49:16.750202Z", "shell.execute_reply.started": "2025-01-28T13:49:16.686654Z", "shell.execute_reply": "2025-01-28T13:49:16.749038Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "# Evaluate oof preds\nscores = {}\nfor col in oof_df.columns:\n    scores[col] = mean_absolute_percentage_error(y, oof_df[col])\n\n# Sort scores\n    scores = {k: v for k, v in sorted(scores.items(), key=lambda item: item[1], reverse=False)}\n\n# Sort oof_df and test_preds\noof_df = oof_df[list(scores.keys())]\ntest_preds = test_preds[list(scores.keys())]\n\nscores", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-28T13:49:16.751531Z", "iopub.execute_input": "2025-01-28T13:49:16.751872Z", "iopub.status.idle": "2025-01-28T13:49:16.770301Z", "shell.execute_reply.started": "2025-01-28T13:49:16.751839Z", "shell.execute_reply": "2025-01-28T13:49:16.769205Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "# Initialise\nSTOP = False\ncurrent_best_ensemble = oof_df.iloc[:,0]\ncurrent_best_test_preds = test_preds.iloc[:,0]\nMODELS = oof_df.iloc[:,1:]\nweight_range = np.arange(-1,1,0.001)  \nhistory = [mean_absolute_percentage_error(y, current_best_ensemble)]\ni=0\n\n# Hill climbing\nwhile not STOP:\n    i+=1\n    potential_new_best_cv_score = mean_absolute_percentage_error(y, current_best_ensemble)\n    k_best, wgt_best = None, None\n    for k in MODELS:\n        for wgt in weight_range:\n            potential_ensemble = (1-wgt) * current_best_ensemble + wgt * MODELS[k]\n            cv_score = mean_absolute_percentage_error(y, potential_ensemble)\n            if cv_score < potential_new_best_cv_score:\n                potential_new_best_cv_score = cv_score\n                k_best, wgt_best = k, wgt\n            \n    if k_best is not None:\n        current_best_ensemble = (1-wgt_best) * current_best_ensemble + wgt_best * MODELS[k_best]\n        current_best_test_preds = (1-wgt_best) * current_best_test_preds + wgt_best * test_preds[k_best]\n        MODELS.drop(k_best, axis=1, inplace=True)\n        if MODELS.shape[1]==0:\n            STOP = True\n        print(f'Iteration: {i}, Model added: {k_best}, Best weight: {wgt_best:}, Best MAPE: {potential_new_best_cv_score:.5f}')\n        history.append(potential_new_best_cv_score)\n    else:\n        STOP = True", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-28T13:49:16.771601Z", "iopub.execute_input": "2025-01-28T13:49:16.771894Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "plt.figure(figsize=(10,4))\nplt.plot(np.arange(len(history))+1, history, marker=\"x\")\nplt.title(\"CV MAPE vs. Number of Models with Hill Climbing\")\nplt.xlabel(\"Number of models\")\nplt.ylabel(\"MAPE\")\nplt.show()", "metadata": {"trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "test_total_sales_dates[\"num_sold\"] = current_best_test_preds*0.5+LinearModel2().predict(X_test)*0.5", "metadata": {"trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "product_ratio_2017_df = product_ratio_df.loc[product_ratio_df[\"date\"].dt.year == 2015].copy()\nproduct_ratio_2018_df = product_ratio_df.loc[product_ratio_df[\"date\"].dt.year == 2016].copy()\nproduct_ratio_2019_df = product_ratio_df.loc[product_ratio_df[\"date\"].dt.year == 2015].copy()\n\nproduct_ratio_2017_df[\"date\"] = product_ratio_2017_df[\"date\"] + pd.DateOffset(years=2)\nproduct_ratio_2018_df[\"date\"] = product_ratio_2018_df[\"date\"] + pd.DateOffset(years=2)\nproduct_ratio_2019_df[\"date\"] =  product_ratio_2019_df[\"date\"] + pd.DateOffset(years=4)\n\nforecasted_ratios_df = pd.concat([product_ratio_2017_df, product_ratio_2018_df, product_ratio_2019_df])", "metadata": {"trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "store_weights_df = store_weights.reset_index()\ntest_sub_df = pd.merge(test_df, test_total_sales_dates, how=\"left\", on=\"date\")\ntest_sub_df = test_sub_df.rename(columns = {\"num_sold\":\"day_num_sold\"})\n# Adding in the product ratios\ntest_sub_df = pd.merge(test_sub_df, store_weights_df, how=\"left\", on=\"store\")\ntest_sub_df = test_sub_df.rename(columns = {\"num_sold\":\"store_ratio\"})\n# Adding in the country ratios\ntest_sub_df[\"year\"] = test_sub_df[\"date\"].dt.year\ntest_sub_df = pd.merge(test_sub_df, gdp_per_capita_filtered_ratios_df_2, how=\"left\", on=[\"year\", \"country\"])\ntest_sub_df = test_sub_df.rename(columns = {\"ratio\":\"country_ratio\"})\n# Adding in the product ratio\ntest_sub_df = pd.merge(test_sub_df, forecasted_ratios_df, how=\"left\", on=[\"date\", \"product\"])\ntest_sub_df = test_sub_df.rename(columns = {\"ratios\":\"product_ratio\"})\n\n# Adding in the week ratio\ntest_sub_df[\"day_of_week\"] = test_sub_df[\"date\"].dt.dayofweek\ntest_sub_df = pd.merge(test_sub_df, day_of_week_ratio.reset_index(), how=\"left\", on=\"day_of_week\")\n\n\n# Disaggregating the forecast\ntest_sub_df.loc[test_sub_df['country'] == 'Kenya', 'country_ratio'] -= 0.0007/2\n\ntest_sub_df[\"num_sold\"] = test_sub_df[\"day_num_sold\"] *test_sub_df[\"day_of_week_ratios\"]* test_sub_df[\"store_ratio\"] * test_sub_df[\"country_ratio\"] * test_sub_df[\"product_ratio\"]\n#test_sub_df[\"num_sold\"] = test_sub_df[\"num_sold\"].round()\ndisplay(test_sub_df.head(2))", "metadata": {"trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "The blending coefficient comes from [HERE](https://www.kaggle.com/code/zyh1104/sticker-sales-solution-ensembling).", "metadata": {}}, {"cell_type": "code", "source": "submission = pd.read_csv(\"./input/playground-series-s5e1/sample_submission.csv\")\nlb_best = pd.read_csv(\"./input/pgs501-model-2-additional-country-doy-factor/submission.csv\")\ntf_best = pd.read_csv(\"./input/transformer-starter-lb-0-052/submission_v1.csv\")\ncnn_best = pd.read_csv(\"./input/convnet-starter-lb-0-052/submission_v1.csv\")\nsubmission[\"num_sold\"] = (test_sub_df[\"num_sold\"]*0.2403 + (tf_best.num_sold*0.55+cnn_best.num_sold*0.45)*0.2587 + lb_best.num_sold*0.5102).round()\ndisplay(submission.head(2))", "metadata": {"trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "submission.to_csv('submission.csv', index = False)", "metadata": {"trusted": true}, "outputs": [], "execution_count": null}]}