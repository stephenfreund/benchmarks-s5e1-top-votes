{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deb6",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-09T13:24:57.643201Z",
     "iopub.status.busy": "2025-01-09T13:24:57.642736Z",
     "iopub.status.idle": "2025-01-09T13:24:58.539568Z",
     "shell.execute_reply": "2025-01-09T13:24:58.538351Z",
     "shell.execute_reply.started": "2025-01-09T13:24:57.64309Z"
    }
   },
   "source": [
    "Credit to  @Cabaxiom's [notebook](https://www.kaggle.com/code/cabaxiom/s5e1-eda-and-linear-regression-baseline) , @Konstantin Dmitriev 's [noetbook](https://www.kaggle.com/code/kdmitrie/pgs501-model-2-additional-country-doy-factor) , @ Mikhail Naumov's [notebook](https://www.kaggle.com/code/mikhailnaumov/forecasting-sticker-sales-ensemble), @cdeotte 's [transformer notebook](https://www.kaggle.com/code/cdeotte/transformer-starter-lb-0-052)\n",
    "\n",
    "[**Based on my single model baseline**.](https://www.kaggle.com/code/act18l/linearregression-subtract-bias-makes-lb-0-054)\n",
    "\n",
    "Fix typo.Update CNN.\n",
    "\n",
    "submission = {[hill_climbling(2linear_regression +3lgbm)\\*0.5+nonlinear\\*0.5]\\*0.5+(cnn\\*0.5+transformer\\*0.5)\\*0.5}\\*0.5+Konstantin's model*0.5\n",
    "\n",
    "submission = ((0.05387+0.05275)\\*0.5+(0.05298\\*0.5+0.05260\\*0.5)\\*0.5)+0.05034\\*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T13:48:36.608183Z",
     "iopub.status.busy": "2025-01-28T13:48:36.607765Z",
     "iopub.status.idle": "2025-01-28T13:48:36.615184Z",
     "shell.execute_reply": "2025-01-28T13:48:36.613935Z",
     "shell.execute_reply.started": "2025-01-28T13:48:36.60815Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import Ridge,Lasso\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from lightgbm import log_evaluation, early_stopping, LGBMRegressor\n",
    "\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T13:48:36.61808Z",
     "iopub.status.busy": "2025-01-28T13:48:36.61754Z",
     "iopub.status.idle": "2025-01-28T13:48:36.948758Z",
     "shell.execute_reply": "2025-01-28T13:48:36.94757Z",
     "shell.execute_reply.started": "2025-01-28T13:48:36.618037Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./input/playground-series-s5e1/train.csv\", parse_dates=[\"date\"])\n",
    "original_train_df = train_df.copy()\n",
    "test_df = pd.read_csv(\"./input/playground-series-s5e1/test.csv\", parse_dates=[\"date\"])\n",
    "sub = pd.read_csv(\"./input/playground-series-s5e1/sample_submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T13:48:36.95089Z",
     "iopub.status.busy": "2025-01-28T13:48:36.950556Z",
     "iopub.status.idle": "2025-01-28T13:48:37.01219Z",
     "shell.execute_reply": "2025-01-28T13:48:37.010979Z",
     "shell.execute_reply.started": "2025-01-28T13:48:36.950861Z"
    }
   },
   "outputs": [],
   "source": [
    "gdp_per_capita_df = pd.read_csv(\"./input/world-gdpgdp-gdp-per-capita-and-annual-growths/gdp_per_capita.csv\")\n",
    "\n",
    "years =  [\"2010\", \"2011\", \"2012\", \"2013\", \"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020\"]\n",
    "gdp_per_capita_filtered_df = gdp_per_capita_df.loc[gdp_per_capita_df[\"Country Name\"].isin(train_df[\"country\"].unique()), [\"Country Name\"] + years].set_index(\"Country Name\")\n",
    "for year in years:\n",
    "    gdp_per_capita_filtered_df[f\"{year}_ratio\"] = gdp_per_capita_filtered_df[year] / gdp_per_capita_filtered_df.sum()[year]\n",
    "gdp_per_capita_filtered_ratios_df = gdp_per_capita_filtered_df[[i+\"_ratio\" for i in years]]\n",
    "gdp_per_capita_filtered_ratios_df.columns = [int(i) for i in years]\n",
    "gdp_per_capita_filtered_ratios_df = gdp_per_capita_filtered_ratios_df.unstack().reset_index().rename(columns = {\"level_0\": \"year\", 0: \"ratio\", \"Country Name\": \"country\"})\n",
    "gdp_per_capita_filtered_ratios_df['year'] = pd.to_datetime(gdp_per_capita_filtered_ratios_df['year'], format='%Y')\n",
    "\n",
    "# For plotting purposes\n",
    "gdp_per_capita_filtered_ratios_df_2 = gdp_per_capita_filtered_ratios_df.copy()\n",
    "gdp_per_capita_filtered_ratios_df_2[\"year\"] = pd.to_datetime(gdp_per_capita_filtered_ratios_df_2['year'].astype(str)) + pd.offsets.YearEnd(1)\n",
    "gdp_per_capita_filtered_ratios_df = pd.concat([gdp_per_capita_filtered_ratios_df, gdp_per_capita_filtered_ratios_df_2]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T13:48:37.013876Z",
     "iopub.status.busy": "2025-01-28T13:48:37.013346Z",
     "iopub.status.idle": "2025-01-28T13:48:37.0203Z",
     "shell.execute_reply": "2025-01-28T13:48:37.019103Z",
     "shell.execute_reply.started": "2025-01-28T13:48:37.013833Z"
    }
   },
   "outputs": [],
   "source": [
    "gdp_per_capita_filtered_ratios_df_2[\"year\"] = gdp_per_capita_filtered_ratios_df_2[\"year\"].dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b8",
   "metadata": {},
   "source": [
    "## imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T13:48:37.024644Z",
     "iopub.status.busy": "2025-01-28T13:48:37.024142Z",
     "iopub.status.idle": "2025-01-28T13:48:43.875251Z",
     "shell.execute_reply": "2025-01-28T13:48:43.874056Z",
     "shell.execute_reply.started": "2025-01-28T13:48:37.024601Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df_imputed = train_df.copy()\n",
    "missing_value_ids = train_df.loc[train_df[\"num_sold\"].isna(), \"id\"].values\n",
    "print(f\"Missing values remaining: {train_df_imputed['num_sold'].isna().sum()}\")\n",
    "\n",
    "train_df_imputed[\"year\"] = train_df_imputed[\"date\"].dt.year\n",
    "for year in train_df_imputed[\"year\"].unique():\n",
    "    # Impute Time Series 1 (Canada, Discount Stickers, Holographic Goose)\n",
    "    target_ratio = gdp_per_capita_filtered_ratios_df_2.loc[(gdp_per_capita_filtered_ratios_df_2[\"year\"] == year) & (gdp_per_capita_filtered_ratios_df_2[\"country\"] == \"Norway\"), \"ratio\"].values[0] # Using Norway as should have the best precision\n",
    "    current_raito = gdp_per_capita_filtered_ratios_df_2.loc[(gdp_per_capita_filtered_ratios_df_2[\"year\"] == year) & (gdp_per_capita_filtered_ratios_df_2[\"country\"] == \"Canada\"), \"ratio\"].values[0]\n",
    "    ratio_can = current_raito / target_ratio\n",
    "    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Canada\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year), \"num_sold\"] * ratio_can).values\n",
    "    \n",
    "    # Impute Time Series 2 (Only Missing Values)\n",
    "    current_ts =  train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Canada\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year)]\n",
    "    missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n",
    "    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Canada\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] * ratio_can).values\n",
    "\n",
    "    # Impute Time Series 3 (Only Missing Values)\n",
    "    current_ts =  train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Canada\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year)]\n",
    "    missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n",
    "    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Canada\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] * ratio_can).values\n",
    "    \n",
    "    # Impute Time Series 4 (Kenya, Discount Stickers, Holographic Goose)\n",
    "    current_raito = gdp_per_capita_filtered_ratios_df_2.loc[(gdp_per_capita_filtered_ratios_df_2[\"year\"] == year) & (gdp_per_capita_filtered_ratios_df_2[\"country\"] == \"Kenya\"), \"ratio\"].values[0]\n",
    "    ratio_ken = current_raito / target_ratio\n",
    "    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Holographic Goose\")& (train_df_imputed[\"year\"] == year), \"num_sold\"] * ratio_ken).values\n",
    "\n",
    "    # Impute Time Series 5 (Only Missing Values)\n",
    "    current_ts = train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year)]\n",
    "    missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n",
    "    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] * ratio_ken).values\n",
    "\n",
    "    # Impute Time Series 6 (Only Missing Values)\n",
    "    current_ts = train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year)]\n",
    "    missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n",
    "    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] * ratio_ken).values\n",
    "\n",
    "    # Impute Time Series 7 (Only Missing Values)\n",
    "    current_ts = train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Kerneler\") & (train_df_imputed[\"year\"] == year)]\n",
    "    missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n",
    "    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Kerneler\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Kerneler\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] * ratio_ken).values\n",
    "    \n",
    "print(f\"Missing values remaining: {train_df_imputed['num_sold'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5693",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T13:48:43.877013Z",
     "iopub.status.busy": "2025-01-28T13:48:43.876697Z",
     "iopub.status.idle": "2025-01-28T13:48:43.898048Z",
     "shell.execute_reply": "2025-01-28T13:48:43.896874Z",
     "shell.execute_reply.started": "2025-01-28T13:48:43.876985Z"
    }
   },
   "outputs": [],
   "source": [
    "missing_rows = train_df_imputed.loc[train_df_imputed[\"num_sold\"].isna()]\n",
    "display(missing_rows)\n",
    "train_df_imputed.loc[train_df_imputed[\"id\"] == 23719, \"num_sold\"] = 4\n",
    "train_df_imputed.loc[train_df_imputed[\"id\"] == 207003, \"num_sold\"] = 195\n",
    "\n",
    "print(f\"Missing values remaining: {train_df_imputed['num_sold'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b746",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T13:48:43.89961Z",
     "iopub.status.busy": "2025-01-28T13:48:43.899263Z",
     "iopub.status.idle": "2025-01-28T13:48:43.931734Z",
     "shell.execute_reply": "2025-01-28T13:48:43.930633Z",
     "shell.execute_reply.started": "2025-01-28T13:48:43.899581Z"
    }
   },
   "outputs": [],
   "source": [
    "store_weights = train_df_imputed.groupby(\"store\")[\"num_sold\"].sum()/train_df_imputed[\"num_sold\"].sum()\n",
    "store_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d008",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T13:48:43.933221Z",
     "iopub.status.busy": "2025-01-28T13:48:43.932939Z",
     "iopub.status.idle": "2025-01-28T13:48:44.525691Z",
     "shell.execute_reply": "2025-01-28T13:48:44.524371Z",
     "shell.execute_reply.started": "2025-01-28T13:48:43.933196Z"
    }
   },
   "outputs": [],
   "source": [
    "product_df = train_df_imputed.groupby([\"date\",\"product\"])[\"num_sold\"].sum().reset_index()\n",
    "product_ratio_df = product_df.pivot(index=\"date\", columns=\"product\", values=\"num_sold\")\n",
    "product_ratio_df = product_ratio_df.apply(lambda x: x/x.sum(),axis=1)\n",
    "product_ratio_df = product_ratio_df.stack().rename(\"ratios\").reset_index()\n",
    "product_ratio_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9626",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T13:48:44.527419Z",
     "iopub.status.busy": "2025-01-28T13:48:44.527061Z",
     "iopub.status.idle": "2025-01-28T13:48:44.541695Z",
     "shell.execute_reply": "2025-01-28T13:48:44.540554Z",
     "shell.execute_reply.started": "2025-01-28T13:48:44.527379Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df_imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T13:48:44.543702Z",
     "iopub.status.busy": "2025-01-28T13:48:44.543228Z",
     "iopub.status.idle": "2025-01-28T13:48:44.568951Z",
     "shell.execute_reply": "2025-01-28T13:48:44.567799Z",
     "shell.execute_reply.started": "2025-01-28T13:48:44.543654Z"
    }
   },
   "outputs": [],
   "source": [
    "original_train_df_imputed = train_df_imputed.copy()\n",
    "train_df_imputed = train_df_imputed.groupby([\"date\"])[\"num_sold\"].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7355",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T13:48:44.574356Z",
     "iopub.status.busy": "2025-01-28T13:48:44.574036Z",
     "iopub.status.idle": "2025-01-28T13:48:44.58533Z",
     "shell.execute_reply": "2025-01-28T13:48:44.58419Z",
     "shell.execute_reply.started": "2025-01-28T13:48:44.574329Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df_imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7163",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T13:48:44.587045Z",
     "iopub.status.busy": "2025-01-28T13:48:44.586708Z",
     "iopub.status.idle": "2025-01-28T13:48:44.609493Z",
     "shell.execute_reply": "2025-01-28T13:48:44.608339Z",
     "shell.execute_reply.started": "2025-01-28T13:48:44.587016Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df_imputed[\"day_of_week\"] = train_df_imputed[\"date\"].dt.dayofweek\n",
    "day_of_week_ratio = (train_df_imputed.groupby(\"day_of_week\")[\"num_sold\"].mean() / train_df_imputed.groupby(\"day_of_week\")[\"num_sold\"].mean().mean()).rename(\"day_of_week_ratios\")\n",
    "display(day_of_week_ratio)\n",
    "train_df_imputed = pd.merge(train_df_imputed, day_of_week_ratio, how=\"left\", on=\"day_of_week\")\n",
    "train_df_imputed[\"num_sold\"] = train_df_imputed[\"num_sold\"] / train_df_imputed[\"day_of_week_ratios\"]\n",
    "train_df_imputed = train_df_imputed.drop(\"day_of_week_ratios\",axis=1)# we don't need it in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1820",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T13:48:44.610914Z",
     "iopub.status.busy": "2025-01-28T13:48:44.610538Z",
     "iopub.status.idle": "2025-01-28T13:48:44.624856Z",
     "shell.execute_reply": "2025-01-28T13:48:44.623817Z",
     "shell.execute_reply.started": "2025-01-28T13:48:44.610877Z"
    }
   },
   "outputs": [],
   "source": [
    "test_total_sales_df = test_df.groupby([\"date\"])[\"id\"].first().reset_index().drop(columns=\"id\")\n",
    "test_total_sales_dates = test_total_sales_df[[\"date\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d08",
   "metadata": {},
   "source": [
    "## feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T13:48:44.626567Z",
     "iopub.status.busy": "2025-01-28T13:48:44.626202Z",
     "iopub.status.idle": "2025-01-28T13:48:44.644139Z",
     "shell.execute_reply": "2025-01-28T13:48:44.64295Z",
     "shell.execute_reply.started": "2025-01-28T13:48:44.626521Z"
    }
   },
   "outputs": [],
   "source": [
    "def feature_engineer(df):\n",
    "    new_df = df.copy()\n",
    "    new_df[\"month\"] = df[\"date\"].dt.month\n",
    "    new_df[\"month_sin\"] = np.sin(new_df['month'] * (2 * np.pi / 12))\n",
    "    new_df[\"month_cos\"] = np.cos(new_df['month'] * (2 * np.pi / 12))\n",
    "    new_df[\"day_of_week\"] = df[\"date\"].dt.dayofweek\n",
    "    new_df[\"day_of_week\"] = new_df[\"day_of_week\"].apply(lambda x: 0 if x<=3 else(1 if x==4 else (2 if x==5 else (3))))    \n",
    "    new_df[\"day_of_year\"] = df['date'].apply(\n",
    "        lambda x: x.timetuple().tm_yday if not (x.is_leap_year and x.month > 2) else x.timetuple().tm_yday - 1\n",
    "    )\n",
    "\n",
    "    new_df['day_sin4'] = np.sin(new_df['day_of_year'] * (8 * np.pi /  365.0))\n",
    "    new_df['day_cos4'] = np.cos(new_df['day_of_year'] * (8 * np.pi /  365.0))\n",
    "    new_df['day_sin3'] = np.sin(new_df['day_of_year'] * (6 * np.pi /  365.0))\n",
    "    new_df['day_cos3'] = np.cos(new_df['day_of_year'] * (6 * np.pi /  365.0))\n",
    "    new_df['day_sin2'] = np.sin(new_df['day_of_year'] * (4 * np.pi /  365.0))\n",
    "    new_df['day_cos2'] = np.cos(new_df['day_of_year'] * (4 * np.pi /  365.0))\n",
    "    new_df['day_sin'] = np.sin(new_df['day_of_year'] * (2 * np.pi /  365.0))\n",
    "    new_df['day_cos'] = np.cos(new_df['day_of_year'] * (2 * np.pi /  365.0)) \n",
    "    new_df['day_sin_0.5'] = np.sin(new_df['day_of_year'] * (1 * np.pi /  365.0))\n",
    "    new_df['day_cos_0.5'] = np.cos(new_df['day_of_year'] * (1 * np.pi /  365.0))    \n",
    "    new_df[\"important_dates\"] = new_df[\"day_of_year\"].apply(lambda x: x if x in [1,2,3,4,5,6,7,8,9,10,99, 100, 101, 125,126,355,256,357,358,359,360,361,362,363,364,365] else 0)\n",
    "    \n",
    "    new_df = new_df.drop(columns=[\"date\",\"month\",\"day_of_year\"])\n",
    "    new_df = pd.get_dummies(new_df, columns = [\"important_dates\",\"day_of_week\"], drop_first=True)\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d377",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T13:48:44.645926Z",
     "iopub.status.busy": "2025-01-28T13:48:44.645443Z",
     "iopub.status.idle": "2025-01-28T13:48:44.715726Z",
     "shell.execute_reply": "2025-01-28T13:48:44.71471Z",
     "shell.execute_reply.started": "2025-01-28T13:48:44.64588Z"
    }
   },
   "outputs": [],
   "source": [
    "train_total_sales_df = feature_engineer(train_df_imputed)\n",
    "test_total_sales_df = feature_engineer(test_total_sales_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T13:48:44.717299Z",
     "iopub.status.busy": "2025-01-28T13:48:44.716981Z",
     "iopub.status.idle": "2025-01-28T13:48:44.973111Z",
     "shell.execute_reply": "2025-01-28T13:48:44.971951Z",
     "shell.execute_reply.started": "2025-01-28T13:48:44.717256Z"
    }
   },
   "outputs": [],
   "source": [
    "import holidays\n",
    "train_df_tmp = train_df.copy()\n",
    "test_df_tmp = test_df.copy()\n",
    "alpha2 = dict(zip(np.sort(train_df.country.unique()), ['CA', 'FI', 'IT', 'KE', 'NO', 'SG']))\n",
    "h = {c: holidays.country_holidays(a, years=range(2010, 2020)) for c, a in alpha2.items()}\n",
    "train_df_tmp['is_holiday'] = 0\n",
    "test_df_tmp['is_holiday'] = 0\n",
    "for c in alpha2:\n",
    "    train_df_tmp.loc[train_df_tmp.country==c, 'is_holiday'] = train_df_tmp.date.isin(h[c]).astype(int)\n",
    "    test_df_tmp.loc[test_df_tmp.country==c, 'is_holiday'] = test_df_tmp.date.isin(h[c]).astype(int)\n",
    "\n",
    "\n",
    "train_total_sales_df['is_holiday'] = (train_df_tmp.groupby([\"date\"])[\"is_holiday\"].sum().reset_index())[\"is_holiday\"]\n",
    "test_total_sales_df['is_holiday'] = (test_df_tmp.groupby([\"date\"])[\"is_holiday\"].sum().reset_index())[\"is_holiday\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T13:48:44.974771Z",
     "iopub.status.busy": "2025-01-28T13:48:44.974415Z",
     "iopub.status.idle": "2025-01-28T13:48:44.981569Z",
     "shell.execute_reply": "2025-01-28T13:48:44.980517Z",
     "shell.execute_reply.started": "2025-01-28T13:48:44.974741Z"
    }
   },
   "outputs": [],
   "source": [
    "y = train_total_sales_df[\"num_sold\"]\n",
    "X = train_total_sales_df.drop(columns=\"num_sold\")\n",
    "X_test = test_total_sales_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T13:48:44.983716Z",
     "iopub.status.busy": "2025-01-28T13:48:44.983275Z",
     "iopub.status.idle": "2025-01-28T13:48:45.013363Z",
     "shell.execute_reply": "2025-01-28T13:48:45.012089Z",
     "shell.execute_reply.started": "2025-01-28T13:48:44.983664Z"
    }
   },
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf9",
   "metadata": {},
   "source": [
    "## hill climbing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T13:48:45.015472Z",
     "iopub.status.busy": "2025-01-28T13:48:45.015008Z",
     "iopub.status.idle": "2025-01-28T13:48:45.025656Z",
     "shell.execute_reply": "2025-01-28T13:48:45.024477Z",
     "shell.execute_reply.started": "2025-01-28T13:48:45.01541Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LinearModel1:\n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.nonlinear(X)\n",
    "\n",
    "    def nonlinear(self, X):\n",
    "        y = (\n",
    "            X.important_dates_1 * 14514.153142903406 +\n",
    "            X.important_dates_6 +\n",
    "            X.important_dates_99 -\n",
    "            X.important_dates_359 +\n",
    "            X.important_dates_363 * 23245.947607750375 +\n",
    "            64795.970182750174 -\n",
    "            ((X.important_dates_362 + np.sqrt(X.important_dates_364 + X.important_dates_365)) *\n",
    "             ((X.important_dates_2 - X.important_dates_10) * (X.important_dates_101 + X.month_sin) +\n",
    "              X.important_dates_362 - 19142.637752625015))\n",
    "        )\n",
    "        return y\n",
    "\n",
    "class LinearModel2:\n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.nonlinear(X)\n",
    "\n",
    "    def nonlinear(self, X):\n",
    "        y = X.month_sin*210.148+\\\n",
    "            (X.important_dates_1+X.important_dates_362+X.important_dates_363+X.important_dates_364+X.important_dates_365)*19472.7071+\\\n",
    "            X.important_dates_4*X.month_sin+\\\n",
    "            65141.2073\n",
    "            \n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T13:48:45.028017Z",
     "iopub.status.busy": "2025-01-28T13:48:45.027575Z",
     "iopub.status.idle": "2025-01-28T13:48:45.047427Z",
     "shell.execute_reply": "2025-01-28T13:48:45.046401Z",
     "shell.execute_reply.started": "2025-01-28T13:48:45.027974Z"
    }
   },
   "outputs": [],
   "source": [
    "regressors = {\n",
    "    \"Ridge\" : Ridge(tol=1e-2, max_iter=1000000, random_state=0),\n",
    "    \"Lasso\" : Lasso(tol=1e-2, max_iter=1000000, random_state=0),\n",
    "    \"nonlinear1\" : LinearModel1(),\n",
    "    \"nonlinear2\" : LinearModel2(),\n",
    "    \"LGBM1\": LGBMRegressor(**{'objective': 'regression_l2',\n",
    "                               'metric': 'mape', \n",
    "                               'max_depth': 7,\n",
    "                               'num_leaves': 123, \n",
    "                               'min_child_samples': 21,\n",
    "                               'min_child_weight': 24,\n",
    "                               'colsample_bytree': 0.3641261996760593, \n",
    "                               'reg_alpha': 0.03632800166349373, \n",
    "                               'reg_lambda': 0.5287861861476272,\n",
    "                               'random_state': 42,\n",
    "                               'early_stopping_round':200,\n",
    "                               'verbose': -1,\n",
    "                               'boosting_type': 'gbdt',\n",
    "                               'n_estimators': 3000,\n",
    "                               'learning_rate': 0.01,\n",
    "                               }),\n",
    "    \"LGBM2\": LGBMRegressor(**{'objective': 'regression_l2',\n",
    "                               'metric': 'mape',\n",
    "                               'max_depth': 6,\n",
    "                               'num_leaves': 502,\n",
    "                               'min_child_samples': 23,\n",
    "                               'min_child_weight': 18, \n",
    "                               'colsample_bytree': 0.4714820876493163, \n",
    "                               'reg_alpha': 0.054972003081022576, \n",
    "                               'reg_lambda': 0.5774608955362155,\n",
    "                               'random_state': 42,\n",
    "                               'early_stopping_round': 200,\n",
    "                               'verbose': -1,\n",
    "                               'boosting_type': 'goss',\n",
    "                               'n_estimators': 3000,\n",
    "                               'learning_rate': 0.01,\n",
    "                              }),\n",
    "    \"LGBM3\": LGBMRegressor(**{'objective': 'regression_l2', \n",
    "                               'metric': 'mape',\n",
    "                               'max_depth': 14,\n",
    "                               'num_leaves': 279,\n",
    "                               'min_child_samples': 7,\n",
    "                               'min_child_weight': 24, \n",
    "                               'colsample_bytree': 0.43218993309765835,\n",
    "                               'reg_alpha': 0.42757392987472964,\n",
    "                               'reg_lambda': 0.9039762787446107,\n",
    "                               'random_state': 42,\n",
    "                               'early_stopping_round': 200,\n",
    "                               'verbose': -1,\n",
    "                               'boosting_type': 'goss',\n",
    "                               'n_estimators': 3000,\n",
    "                               'learning_rate': 0.01,\n",
    "                               }),\n",
    "    \n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T13:48:45.049423Z",
     "iopub.status.busy": "2025-01-28T13:48:45.04901Z",
     "iopub.status.idle": "2025-01-28T13:49:16.684137Z",
     "shell.execute_reply": "2025-01-28T13:49:16.68271Z",
     "shell.execute_reply.started": "2025-01-28T13:48:45.049383Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.model_selection import KFold\n",
    "n_folds = 5\n",
    "\n",
    "for key, reg in regressors.items():\n",
    "    test_preds = np.zeros(len(X_test))\n",
    "    oof_full = y.copy()\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    cv = KFold(n_splits=n_folds, shuffle=False)\n",
    "    score=0\n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n",
    "        X_train, X_valid = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_valid = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        start = time.time()\n",
    "        if \"LGBM\" in key:\n",
    "            reg.fit(X_train, y_train,\n",
    "                   eval_set = [(X_valid, y_valid)], \n",
    "                                       callbacks = [log_evaluation(0),\n",
    "                                                    early_stopping(200, verbose = False)\n",
    "                                                   ])\n",
    "            \n",
    "        else:\n",
    "            reg.fit(X_train, y_train)\n",
    "        oof_preds = reg.predict(X_valid)\n",
    "        score += mean_absolute_percentage_error(y_valid, oof_preds)/n_folds\n",
    "        oof_full[val_idx] = oof_preds\n",
    "        \n",
    "        test_preds += reg.predict(X_test)/n_folds\n",
    "    \n",
    "    # Stop timer\n",
    "    stop = time.time()\n",
    "    \n",
    "    print('Model:', key)\n",
    "    print('Average validation MAPE:', score)\n",
    "    print('Training time (mins):', np.round((stop - start)/60,2))\n",
    "    print('')\n",
    "    \n",
    "\n",
    "    oof_full.to_csv(f\"{key}_oof_preds.csv\", index=False)\n",
    "    ss = pd.DataFrame()\n",
    "    ss[\"num_sold\"] = test_preds\n",
    "    ss.to_csv(f\"{key}_test_preds.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T13:49:16.686699Z",
     "iopub.status.busy": "2025-01-28T13:49:16.685645Z",
     "iopub.status.idle": "2025-01-28T13:49:16.750202Z",
     "shell.execute_reply": "2025-01-28T13:49:16.749038Z",
     "shell.execute_reply.started": "2025-01-28T13:49:16.686654Z"
    }
   },
   "outputs": [],
   "source": [
    "oof_df = pd.DataFrame(index=np.arange(len(y)))\n",
    "for i in regressors.keys():\n",
    "    df = pd.read_csv(f\"/kaggle/working/{i}_oof_preds.csv\")\n",
    "    df.rename(columns={\"num_sold\": i}, inplace=True)\n",
    "    oof_df = pd.concat([oof_df,df], axis=1)\n",
    "    \n",
    "# Join test preds\n",
    "test_preds = pd.DataFrame(index=np.arange(len(X_test)))\n",
    "for i in regressors.keys():\n",
    "    df = pd.read_csv(f\"/kaggle/working/{i}_test_preds.csv\")\n",
    "    df.rename(columns={\"num_sold\": i}, inplace=True)\n",
    "    test_preds = pd.concat([test_preds,df], axis=1)\n",
    "    \n",
    "oof_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T13:49:16.751872Z",
     "iopub.status.busy": "2025-01-28T13:49:16.751531Z",
     "iopub.status.idle": "2025-01-28T13:49:16.770301Z",
     "shell.execute_reply": "2025-01-28T13:49:16.769205Z",
     "shell.execute_reply.started": "2025-01-28T13:49:16.751839Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate oof preds\n",
    "scores = {}\n",
    "for col in oof_df.columns:\n",
    "    scores[col] = mean_absolute_percentage_error(y, oof_df[col])\n",
    "\n",
    "# Sort scores\n",
    "    scores = {k: v for k, v in sorted(scores.items(), key=lambda item: item[1], reverse=False)}\n",
    "\n",
    "# Sort oof_df and test_preds\n",
    "oof_df = oof_df[list(scores.keys())]\n",
    "test_preds = test_preds[list(scores.keys())]\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9944",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T13:49:16.771894Z",
     "iopub.status.busy": "2025-01-28T13:49:16.771601Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialise\n",
    "STOP = False\n",
    "current_best_ensemble = oof_df.iloc[:,0]\n",
    "current_best_test_preds = test_preds.iloc[:,0]\n",
    "MODELS = oof_df.iloc[:,1:]\n",
    "weight_range = np.arange(-1,1,0.001)  \n",
    "history = [mean_absolute_percentage_error(y, current_best_ensemble)]\n",
    "i=0\n",
    "\n",
    "# Hill climbing\n",
    "while not STOP:\n",
    "    i+=1\n",
    "    potential_new_best_cv_score = mean_absolute_percentage_error(y, current_best_ensemble)\n",
    "    k_best, wgt_best = None, None\n",
    "    for k in MODELS:\n",
    "        for wgt in weight_range:\n",
    "            potential_ensemble = (1-wgt) * current_best_ensemble + wgt * MODELS[k]\n",
    "            cv_score = mean_absolute_percentage_error(y, potential_ensemble)\n",
    "            if cv_score < potential_new_best_cv_score:\n",
    "                potential_new_best_cv_score = cv_score\n",
    "                k_best, wgt_best = k, wgt\n",
    "            \n",
    "    if k_best is not None:\n",
    "        current_best_ensemble = (1-wgt_best) * current_best_ensemble + wgt_best * MODELS[k_best]\n",
    "        current_best_test_preds = (1-wgt_best) * current_best_test_preds + wgt_best * test_preds[k_best]\n",
    "        MODELS.drop(k_best, axis=1, inplace=True)\n",
    "        if MODELS.shape[1]==0:\n",
    "            STOP = True\n",
    "        print(f'Iteration: {i}, Model added: {k_best}, Best weight: {wgt_best:}, Best MAPE: {potential_new_best_cv_score:.5f}')\n",
    "        history.append(potential_new_best_cv_score)\n",
    "    else:\n",
    "        STOP = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(np.arange(len(history))+1, history, marker=\"x\")\n",
    "plt.title(\"CV MAPE vs. Number of Models with Hill Climbing\")\n",
    "plt.xlabel(\"Number of models\")\n",
    "plt.ylabel(\"MAPE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_total_sales_dates[\"num_sold\"] = current_best_test_preds*0.5+LinearModel2().predict(X_test)*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_ratio_2017_df = product_ratio_df.loc[product_ratio_df[\"date\"].dt.year == 2015].copy()\n",
    "product_ratio_2018_df = product_ratio_df.loc[product_ratio_df[\"date\"].dt.year == 2016].copy()\n",
    "product_ratio_2019_df = product_ratio_df.loc[product_ratio_df[\"date\"].dt.year == 2015].copy()\n",
    "\n",
    "product_ratio_2017_df[\"date\"] = product_ratio_2017_df[\"date\"] + pd.DateOffset(years=2)\n",
    "product_ratio_2018_df[\"date\"] = product_ratio_2018_df[\"date\"] + pd.DateOffset(years=2)\n",
    "product_ratio_2019_df[\"date\"] =  product_ratio_2019_df[\"date\"] + pd.DateOffset(years=4)\n",
    "\n",
    "forecasted_ratios_df = pd.concat([product_ratio_2017_df, product_ratio_2018_df, product_ratio_2019_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_weights_df = store_weights.reset_index()\n",
    "test_sub_df = pd.merge(test_df, test_total_sales_dates, how=\"left\", on=\"date\")\n",
    "test_sub_df = test_sub_df.rename(columns = {\"num_sold\":\"day_num_sold\"})\n",
    "# Adding in the product ratios\n",
    "test_sub_df = pd.merge(test_sub_df, store_weights_df, how=\"left\", on=\"store\")\n",
    "test_sub_df = test_sub_df.rename(columns = {\"num_sold\":\"store_ratio\"})\n",
    "# Adding in the country ratios\n",
    "test_sub_df[\"year\"] = test_sub_df[\"date\"].dt.year\n",
    "test_sub_df = pd.merge(test_sub_df, gdp_per_capita_filtered_ratios_df_2, how=\"left\", on=[\"year\", \"country\"])\n",
    "test_sub_df = test_sub_df.rename(columns = {\"ratio\":\"country_ratio\"})\n",
    "# Adding in the product ratio\n",
    "test_sub_df = pd.merge(test_sub_df, forecasted_ratios_df, how=\"left\", on=[\"date\", \"product\"])\n",
    "test_sub_df = test_sub_df.rename(columns = {\"ratios\":\"product_ratio\"})\n",
    "\n",
    "# Adding in the week ratio\n",
    "test_sub_df[\"day_of_week\"] = test_sub_df[\"date\"].dt.dayofweek\n",
    "test_sub_df = pd.merge(test_sub_df, day_of_week_ratio.reset_index(), how=\"left\", on=\"day_of_week\")\n",
    "\n",
    "\n",
    "# Disaggregating the forecast\n",
    "test_sub_df.loc[test_sub_df['country'] == 'Kenya', 'country_ratio'] -= 0.0007/2\n",
    "\n",
    "test_sub_df[\"num_sold\"] = test_sub_df[\"day_num_sold\"] *test_sub_df[\"day_of_week_ratios\"]* test_sub_df[\"store_ratio\"] * test_sub_df[\"country_ratio\"] * test_sub_df[\"product_ratio\"]\n",
    "#test_sub_df[\"num_sold\"] = test_sub_df[\"num_sold\"].round()\n",
    "display(test_sub_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcd",
   "metadata": {},
   "source": [
    "The blending coefficient comes from [HERE](https://www.kaggle.com/code/zyh1104/sticker-sales-solution-ensembling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2325",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"./input/playground-series-s5e1/sample_submission.csv\")\n",
    "lb_best = pd.read_csv(\"./input/pgs501-model-2-additional-country-doy-factor/submission.csv\")\n",
    "tf_best = pd.read_csv(\"./input/transformer-starter-lb-0-052/submission_v1.csv\")\n",
    "cnn_best = pd.read_csv(\"./input/convnet-starter-lb-0-052/submission_v1.csv\")\n",
    "submission[\"num_sold\"] = (test_sub_df[\"num_sold\"]*0.2403 + (tf_best.num_sold*0.55+cnn_best.num_sold*0.45)*0.2587 + lb_best.num_sold*0.5102).round()\n",
    "display(submission.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10652996,
     "sourceId": 85723,
     "sourceType": "competition"
    },
    {
     "datasetId": 2007861,
     "sourceId": 3325325,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 219013464,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 219539450,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 219621257,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30235,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
