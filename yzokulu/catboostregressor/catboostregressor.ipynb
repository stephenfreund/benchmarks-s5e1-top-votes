{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-20T18:23:57.277704Z",
     "iopub.status.busy": "2025-01-20T18:23:57.277319Z",
     "iopub.status.idle": "2025-01-20T18:24:02.945926Z",
     "shell.execute_reply": "2025-01-20T18:24:02.94469Z",
     "shell.execute_reply.started": "2025-01-20T18:23:57.277661Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor \n",
    "from xgboost import XGBRegressor \n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('./input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T18:24:02.948084Z",
     "iopub.status.busy": "2025-01-20T18:24:02.947363Z",
     "iopub.status.idle": "2025-01-20T18:24:03.474081Z",
     "shell.execute_reply": "2025-01-20T18:24:03.472779Z",
     "shell.execute_reply.started": "2025-01-20T18:24:02.948019Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./input/playground-series-s5e1/train.csv')\n",
    "test = pd.read_csv('./input/playground-series-s5e1/test.csv')\n",
    "sub = pd.read_csv('./input/playground-series-s5e1/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1232",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T18:24:03.475569Z",
     "iopub.status.busy": "2025-01-20T18:24:03.47526Z",
     "iopub.status.idle": "2025-01-20T18:24:03.549215Z",
     "shell.execute_reply": "2025-01-20T18:24:03.548045Z",
     "shell.execute_reply.started": "2025-01-20T18:24:03.475543Z"
    }
   },
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9149",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T18:24:03.552203Z",
     "iopub.status.busy": "2025-01-20T18:24:03.551855Z",
     "iopub.status.idle": "2025-01-20T18:24:03.959474Z",
     "shell.execute_reply": "2025-01-20T18:24:03.957984Z",
     "shell.execute_reply.started": "2025-01-20T18:24:03.552175Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sorted = train.sort_values(by=['country', 'store', 'product', 'date'])\n",
    "\n",
    "# Group by 'country', 'store', and 'product' to fill missing 'num_sold' values\n",
    "df_sorted['num_sold'] = df_sorted.groupby(['country', 'store', 'product'])['num_sold'].fillna(method='ffill')\n",
    "\n",
    "# Then fill remaining missing values using backward fill\n",
    "df_sorted['num_sold'] = df_sorted.groupby(['country', 'store', 'product'])['num_sold'].fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T18:24:03.961486Z",
     "iopub.status.busy": "2025-01-20T18:24:03.961193Z",
     "iopub.status.idle": "2025-01-20T18:24:04.016537Z",
     "shell.execute_reply": "2025-01-20T18:24:04.015133Z",
     "shell.execute_reply.started": "2025-01-20T18:24:03.961453Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sorted.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T18:24:04.017855Z",
     "iopub.status.busy": "2025-01-20T18:24:04.017552Z",
     "iopub.status.idle": "2025-01-20T18:24:04.160689Z",
     "shell.execute_reply": "2025-01-20T18:24:04.159633Z",
     "shell.execute_reply.started": "2025-01-20T18:24:04.017829Z"
    }
   },
   "outputs": [],
   "source": [
    "grouped = df_sorted.groupby(['country', 'store', 'product'])['num_sold'].sum().reset_index()\n",
    "\n",
    "\n",
    "# 2. Calculate total sales for each country-store combination\n",
    "total_sales = df_sorted.groupby(['country', 'store'], as_index=False)['num_sold'].sum()\n",
    "\n",
    "# 3. Merge to get total sales for each country-store combination into the grouped DataFrame\n",
    "result = pd.merge(grouped, total_sales, on=['country', 'store'], suffixes=('', '_total'))\n",
    "\n",
    "# 4. Calculate the percentage contribution for each country-store-product combination\n",
    "result['percentage'] = (result['num_sold'] / result['num_sold_total']) * 100\n",
    "\n",
    "# Display the result\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T18:24:04.162377Z",
     "iopub.status.busy": "2025-01-20T18:24:04.161925Z",
     "iopub.status.idle": "2025-01-20T18:24:04.166971Z",
     "shell.execute_reply": "2025-01-20T18:24:04.165968Z",
     "shell.execute_reply.started": "2025-01-20T18:24:04.162338Z"
    }
   },
   "outputs": [],
   "source": [
    "#def plot_total_sales_by_date(country, store, df):\n",
    "#    # Filter the data for the given country and store\n",
    "#    filtered_df = df[(df['country'] == country) & (df['store'] == store)]\n",
    "#    \n",
    "#    # Group by date and sum the num_sold for all products\n",
    "#    total_sales_by_date = filtered_df.groupby('date')['num_sold'].sum().reset_index()\n",
    "#    \n",
    "#    # Check if data exists for the given combination\n",
    "#    if total_sales_by_date.empty:\n",
    "#        print(f\"No data available for {country} - {store}.\")\n",
    "#        return\n",
    "    # Create a line plot for total num_sold by date\n",
    "##    \n",
    "#    plt.figure(figsize=(8, 6))\n",
    "#    sns.lineplot(data=total_sales_by_date, x='date', y='num_sold')\n",
    "\n",
    "    # Set plot labels and title\n",
    "#   plt.xlabel('Date')\n",
    "#   plt.ylabel('Total Number Sold')\n",
    "#   plt.title(f'Total Sales in {store} ({country}) by Date')\n",
    "\n",
    "    # Rotate the date labels for better readability\n",
    "#    plt.xticks(rotation=45)\n",
    "\n",
    "#    # Show the plot\n",
    "#    plt.tight_layout()\n",
    "#    plt.show()\n",
    "\n",
    "\n",
    "#plot_total_sales_by_date('Finland', 'Discount Stickers', df_sorted)  # You can change this to an\n",
    "#plot_total_sales_by_date('Norway', 'Discount Stickers', df_sorted)  # You can change this to an\n",
    "#plot_total_sales_by_date('Singapore', 'Discount Stickers', df_sorted)  # You can change this to an\n",
    "#plot_total_sales_by_date('Italy', 'Discount Stickers', df_sorted)  # You can change this to an\n",
    "#plot_total_sales_by_date('Canada', 'Discount Stickers', df_sorted)  # You can change this to an\n",
    "#plot_total_sales_by_date('Kenya', 'Discount Stickers', df_sorted)  # You can change this to an\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T18:24:04.16867Z",
     "iopub.status.busy": "2025-01-20T18:24:04.16832Z",
     "iopub.status.idle": "2025-01-20T18:24:04.189153Z",
     "shell.execute_reply": "2025-01-20T18:24:04.187874Z",
     "shell.execute_reply.started": "2025-01-20T18:24:04.168644Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_sales_for_specific_product_with_ratio(df, target_country, target_store, target_product, source_country, source_store, source_product):\n",
    "    \"\"\"\n",
    "    This method sets the num_sold for a specific product in a source country-store combination to \n",
    "    the num_sold values from a target country-store combination, \n",
    "    scaled by a ratio calculated as the total num_sold at the target country-store combination \n",
    "    divided by the total num_sold at the source country-store combination (for all products).\n",
    "    \n",
    "    Parameters:\n",
    "    - df: The DataFrame containing the sales data.\n",
    "    - target_country: The country from which to take the target sales values.\n",
    "    - target_store: The store from which to take the target sales values.\n",
    "    - target_product: The product whose sales will be used to scale the source product's sales.\n",
    "    - source_country: The country to which the sales values will be updated.\n",
    "    - source_store: The store to which the sales values will be updated.\n",
    "    - source_product: The product whose sales will be updated.\n",
    "    \n",
    "    Returns:\n",
    "    - The updated DataFrame with num_sold for the specific product scaled by the ratio for the source combination.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter the target data for the target country-store combination and the specific product\n",
    "    target_data = df[(df['country'] == target_country) & \n",
    "                     (df['store'] == target_store) & \n",
    "                     (df['product'] == target_product)]\n",
    "    \n",
    "    # Filter the source data for the source country-store combination and the specific product\n",
    "    source_data = df[(df['country'] == source_country) & \n",
    "                     (df['store'] == source_store) & \n",
    "                     (df['product'] == source_product)]\n",
    "    \n",
    "    # Calculate total num_sold for all products in the target and source country-store combinations\n",
    "    total_target_sales = df[(df['country'] == target_country) & \n",
    "                            (df['store'] == target_store)]['num_sold'].sum()\n",
    "    \n",
    "    total_source_sales = df[(df['country'] == source_country) & \n",
    "                            (df['store'] == source_store)]['num_sold'].sum()\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if total_source_sales == 0:\n",
    "        raise ValueError(f\"Total sales for the source combination ({source_country}-{source_store}) is zero.\")\n",
    "    \n",
    "    # Calculate the ratio (total num_sold at target / total num_sold at source)\n",
    "    ratio = total_target_sales / total_source_sales\n",
    "\n",
    "    print('total_target_sales', total_target_sales)\n",
    "    print('total_source_sales', total_source_sales)\n",
    "    print('ratio:', ratio)\n",
    "   \n",
    "    df = df.merge(source_data[['date', 'num_sold']], on='date', how='left', suffixes=('', '_source'))\n",
    "\n",
    "    # Set the num_sold for the source product based on the target num_sold scaled by the ratio\n",
    "    df.loc[(df['country'] == target_country) & \n",
    "           (df['store'] == target_store) & \n",
    "           (df['product'] == target_product), 'num_sold'] = df['num_sold_source'] * ratio\n",
    "    \n",
    "    # Drop the extra merged column\n",
    "    df.drop(columns=['num_sold_source'], inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T18:24:04.190795Z",
     "iopub.status.busy": "2025-01-20T18:24:04.19036Z",
     "iopub.status.idle": "2025-01-20T18:24:04.538399Z",
     "shell.execute_reply": "2025-01-20T18:24:04.537165Z",
     "shell.execute_reply.started": "2025-01-20T18:24:04.190735Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "train = set_sales_for_specific_product_with_ratio(df_sorted, 'Canada', 'Discount Stickers', 'Holographic Goose', 'Italy', 'Discount Stickers', 'Holographic Goose')\n",
    "#train = set_sales_for_specific_product_with_ratio(train, 'Kenya', 'Discount Stickers', 'Holographic Goose', 'Singapore', 'Discount Stickers', 'Holographic Goose')\n",
    "\n",
    "train[train['country'] == 'Canada'].head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T18:24:04.540194Z",
     "iopub.status.busy": "2025-01-20T18:24:04.539767Z",
     "iopub.status.idle": "2025-01-20T18:24:04.757175Z",
     "shell.execute_reply": "2025-01-20T18:24:04.755972Z",
     "shell.execute_reply.started": "2025-01-20T18:24:04.540152Z"
    }
   },
   "outputs": [],
   "source": [
    "def fill_nulls_for_Kenya(df):\n",
    "    \"\"\"\n",
    "    This method updates the num_sold for each product in the target country-store combination\n",
    "    by scaling the num_sold of the corresponding product in the source country-store combination.\n",
    "    The scaling ratio is calculated as the total num_sold at target country-store divided by the \n",
    "    total num_sold at source country-store (for all products).\n",
    "\n",
    "    Parameters:\n",
    "    - df: The DataFrame containing the sales data.\n",
    "    - target_country: The country from which to take the target sales values.\n",
    "    - target_store: The store from which to take the target sales values.\n",
    "    - source_country: The country from which to take the source sales values.\n",
    "    - source_store: The store from which to take the source sales values.\n",
    "    - source_product: The specific product in the source store-country combination to scale.\n",
    "    \n",
    "    Returns:\n",
    "    - The updated DataFrame with num_sold for the target country-store-product combination.\n",
    "    \"\"\"\n",
    "\n",
    "    #total_sales_per_day = df[df['country'] == 'Kenya'].groupby('date')['num_sold'].transform('sum')\n",
    "\n",
    "    total_sales_per_day = df[(df['country'] == 'Kenya') & (df['store'] == 'Discount Stickers')].groupby('date')['num_sold'].sum().reset_index()\n",
    "    #total_sales_per_day.rename(columns={'num_sold': 'total_num_sold'}, inplace=True)\n",
    "\n",
    "    print(total_sales_per_day.head(20))\n",
    "    \n",
    "    ratio = 4.75 / 95.25\n",
    "    print(f\"Calculated ratio: {ratio:.2f}\")\n",
    "\n",
    "    # Merge source_data with df to get the corresponding target product for the target country-store combination\n",
    "    df_merged = df.merge(total_sales_per_day[['date', 'num_sold']], on='date', how='left', suffixes=('', '_source'))\n",
    "    \n",
    "    # Now, update num_sold for the target country-store-product combination\n",
    "    df_merged.loc[(df_merged['country'] == 'Kenya') & \n",
    "                  (df_merged['store'] == 'Discount Stickers') & \n",
    "                  (df_merged['product'] == 'Holographic Goose'), 'num_sold'] = df_merged['num_sold_source'] * ratio\n",
    "    \n",
    "    # Debug: Check if num_sold has been updated\n",
    "    print(\"After update:\")\n",
    "    print(df_merged[df_merged['country'] == 'Kenya'][['country', 'store', 'product', 'date', 'num_sold']].head())\n",
    "\n",
    "    # Drop the extra merged column\n",
    "    df_merged.drop(columns=['num_sold_source'], inplace=True)\n",
    "    \n",
    "    return df_merged\n",
    "\n",
    "\n",
    "train = fill_nulls_for_Kenya(train)\n",
    "#train = set_sales_for_specific_product_with_ratio(train, 'Kenya', 'Discount Stickers', 'Holographic Goose', 'Singapore', 'Discount Stickers', 'Holographic Goose')\n",
    "\n",
    "train[train['country'] == 'Kenya'].head(200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T18:24:04.758395Z",
     "iopub.status.busy": "2025-01-20T18:24:04.758129Z",
     "iopub.status.idle": "2025-01-20T18:24:04.81353Z",
     "shell.execute_reply": "2025-01-20T18:24:04.812352Z",
     "shell.execute_reply.started": "2025-01-20T18:24:04.758373Z"
    }
   },
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T18:24:04.815201Z",
     "iopub.status.busy": "2025-01-20T18:24:04.814867Z",
     "iopub.status.idle": "2025-01-20T18:24:05.007825Z",
     "shell.execute_reply": "2025-01-20T18:24:05.006951Z",
     "shell.execute_reply.started": "2025-01-20T18:24:04.815173Z"
    }
   },
   "outputs": [],
   "source": [
    "# Feature engineering function\n",
    "def feature_engineering(df):\n",
    "    # Generate new columns from date\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['week'] = df['date'].dt.isocalendar().week\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['day_of_week'] = df['date'].dt.dayofweek\n",
    "    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "    df['sine_day'] = np.sin(2 * np.pi * df['day'] / 31)\n",
    "    df['cos_day'] = np.cos(2 * np.pi * df['day'] / 31)\n",
    "    df['sine_month'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['cos_month'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    df['sine_year'] = np.sin(2 * np.pi * df['year'])\n",
    "    df['cos_year'] = np.cos(2 * np.pi * df['year'])\n",
    "\n",
    "    # Convert categorical columns to category dtype\n",
    "    for col in ['country', 'store', 'product']:\n",
    "        df[col] = df[col].astype('category')\n",
    "\n",
    "    return df\n",
    "\n",
    "train = feature_engineering(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T18:24:05.010643Z",
     "iopub.status.busy": "2025-01-20T18:24:05.010353Z",
     "iopub.status.idle": "2025-01-20T18:24:05.032554Z",
     "shell.execute_reply": "2025-01-20T18:24:05.031507Z",
     "shell.execute_reply.started": "2025-01-20T18:24:05.01062Z"
    }
   },
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7234",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T18:24:05.03413Z",
     "iopub.status.busy": "2025-01-20T18:24:05.033814Z",
     "iopub.status.idle": "2025-01-20T18:24:05.216052Z",
     "shell.execute_reply": "2025-01-20T18:24:05.21515Z",
     "shell.execute_reply.started": "2025-01-20T18:24:05.034104Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply feature engineering and preprocessing\n",
    "train = train.dropna() #feature_engineering(train)\n",
    "train.info()\n",
    "#train = preprocess_data(train)\n",
    "\n",
    "# Prepare data for training\n",
    "X = train.drop(columns=['id', 'date', 'num_sold'])\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "y = np.log1p(train['num_sold'])\n",
    "\n",
    "# Load test data\n",
    "test = feature_engineering(test)\n",
    "X_test = test.drop(columns=['id', 'date'])\n",
    "X_test = pd.get_dummies(X_test, drop_first=True)\n",
    "X_test = X_test.reindex(columns=X.columns, fill_value=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3342",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T18:24:05.217526Z",
     "iopub.status.busy": "2025-01-20T18:24:05.217195Z",
     "iopub.status.idle": "2025-01-20T18:24:13.144089Z",
     "shell.execute_reply": "2025-01-20T18:24:13.142827Z",
     "shell.execute_reply.started": "2025-01-20T18:24:05.2175Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "train_predictions = np.zeros(len(train))\n",
    "mape_scores = []\n",
    "test_predictions_list = []\n",
    "\n",
    "# CatBoost Hyperparameters\n",
    "params = {\n",
    "    'iterations': 1000,             # Number of trees to build\n",
    "    'learning_rate': 0.1,           # Learning rate\n",
    "    'depth': 6,                     # Depth of the tree\n",
    "    'loss_function': 'MAPE',        # Loss function for regression\n",
    "    'cat_features': [],             # List of categorical features (if applicable)\n",
    "    'random_seed': 42,              # Random seed for reproducibility\n",
    "    'verbose': 200,                 # Print information every 200 iterations\n",
    "}\n",
    "\n",
    "\n",
    "# Train and validate model using 5-fold cross-validation\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X), start=1):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    # Train CatBoostRegressor\n",
    "    model = LGBMRegressor()#XGBRegressor()#ExtraTreesRegressor(random_state=42, n_jobs=-1)  # RandomForestRegressor(random_state=42, n_jobs=-1) # CatBoostRegressor(**params) # #\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    train_predictions[val_idx] = y_val_pred\n",
    "\n",
    "    # Calculate MAPE\n",
    "    mape = mean_absolute_percentage_error(y_val, y_val_pred)\n",
    "    mape_scores.append(mape)\n",
    "    print(f\"Fold {fold}: MAPE = {mape:.4f}\")\n",
    "\n",
    "    # Predict on test data for this fold\n",
    "    test_pred_fold = model.predict(X_test)\n",
    "    test_predictions_list.append(test_pred_fold)\n",
    "\n",
    "# Average test predictions across folds\n",
    "test_predictions_avg = np.mean(test_predictions_list, axis=0)\n",
    "test_predictions_avg = np.expm1(test_predictions_avg) \n",
    "\n",
    "# Print training MAPE score\n",
    "print(f\"Training MAPE score (5-fold average): {np.mean(mape_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T18:24:13.145608Z",
     "iopub.status.busy": "2025-01-20T18:24:13.1452Z",
     "iopub.status.idle": "2025-01-20T18:24:13.150573Z",
     "shell.execute_reply": "2025-01-20T18:24:13.149434Z",
     "shell.execute_reply.started": "2025-01-20T18:24:13.145578Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot actual vs. predicted values (daily, weekly, monthly)\n",
    "#def plot_predictions(data, predictions, freq, title):\n",
    "#    data['predicted'] = np.expm1(predictions)\n",
    "#    aggregated = data.groupby(pd.Grouper(key='date', freq=freq))[['num_sold', 'predicted']].sum()\n",
    "\n",
    "#    plt.figure(figsize=(12, 6))\n",
    "#    plt.plot(aggregated.index, aggregated['num_sold'], label='Actual')\n",
    "#    plt.plot(aggregated.index, aggregated['predicted'], label='Predicted')\n",
    "#    plt.title(title)\n",
    "#    plt.xlabel('Date')\n",
    "#    plt.ylabel('Num Sold')\n",
    "#    plt.legend()\n",
    "#    plt.show()\n",
    "\n",
    "#plot_predictions(train, train_predictions, 'D', 'Daily Actual vs Predicted')\n",
    "#plot_predictions(train, train_predictions, 'W', 'Weekly Actual vs Predicted')\n",
    "#plot_predictions(train, train_predictions, 'M', 'Monthly Actual vs Predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4449",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T18:24:13.152075Z",
     "iopub.status.busy": "2025-01-20T18:24:13.151647Z",
     "iopub.status.idle": "2025-01-20T18:24:13.388518Z",
     "shell.execute_reply": "2025-01-20T18:24:13.387367Z",
     "shell.execute_reply.started": "2025-01-20T18:24:13.152009Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print test predictions\n",
    "test['predicted_num_sold'] = test_predictions_avg\n",
    "print(test[['id', 'predicted_num_sold']])\n",
    "\n",
    "# Save predictions\n",
    "test[['id', 'predicted_num_sold']].to_csv('submission.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10652996,
     "sourceId": 85723,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
