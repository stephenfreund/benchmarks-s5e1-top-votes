{"metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "language_info": {"name": "python", "version": "3.10.12", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}, "kaggle": {"accelerator": "none", "dataSources": [{"sourceId": 85723, "databundleVersionId": 10652996, "sourceType": "competition"}], "dockerImageVersionId": 30822, "isInternetEnabled": true, "language": "python", "sourceType": "notebook", "isGpuEnabled": false}}, "nbformat_minor": 4, "nbformat": 4, "cells": [{"cell_type": "code", "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor \nfrom xgboost import XGBRegressor \nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.metrics import mean_absolute_percentage_error\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('./input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session", "metadata": {"_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5", "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19", "trusted": true, "execution": {"iopub.status.busy": "2025-01-20T18:23:57.277319Z", "iopub.execute_input": "2025-01-20T18:23:57.277704Z", "iopub.status.idle": "2025-01-20T18:24:02.945926Z", "shell.execute_reply.started": "2025-01-20T18:23:57.277661Z", "shell.execute_reply": "2025-01-20T18:24:02.94469Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "train = pd.read_csv('./input/playground-series-s5e1/train.csv')\ntest = pd.read_csv('./input/playground-series-s5e1/test.csv')\nsub = pd.read_csv('./input/playground-series-s5e1/sample_submission.csv')", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-20T18:24:02.947363Z", "iopub.execute_input": "2025-01-20T18:24:02.948084Z", "iopub.status.idle": "2025-01-20T18:24:03.474081Z", "shell.execute_reply.started": "2025-01-20T18:24:02.948019Z", "shell.execute_reply": "2025-01-20T18:24:03.472779Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "train.info()", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-20T18:24:03.47526Z", "iopub.execute_input": "2025-01-20T18:24:03.475569Z", "iopub.status.idle": "2025-01-20T18:24:03.549215Z", "shell.execute_reply.started": "2025-01-20T18:24:03.475543Z", "shell.execute_reply": "2025-01-20T18:24:03.548045Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "df_sorted = train.sort_values(by=['country', 'store', 'product', 'date'])\n\n# Group by 'country', 'store', and 'product' to fill missing 'num_sold' values\ndf_sorted['num_sold'] = df_sorted.groupby(['country', 'store', 'product'])['num_sold'].fillna(method='ffill')\n\n# Then fill remaining missing values using backward fill\ndf_sorted['num_sold'] = df_sorted.groupby(['country', 'store', 'product'])['num_sold'].fillna(method='bfill')", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-20T18:24:03.551855Z", "iopub.execute_input": "2025-01-20T18:24:03.552203Z", "iopub.status.idle": "2025-01-20T18:24:03.959474Z", "shell.execute_reply.started": "2025-01-20T18:24:03.552175Z", "shell.execute_reply": "2025-01-20T18:24:03.957984Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "df_sorted.info()", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-20T18:24:03.961193Z", "iopub.execute_input": "2025-01-20T18:24:03.961486Z", "iopub.status.idle": "2025-01-20T18:24:04.016537Z", "shell.execute_reply.started": "2025-01-20T18:24:03.961453Z", "shell.execute_reply": "2025-01-20T18:24:04.015133Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "grouped = df_sorted.groupby(['country', 'store', 'product'])['num_sold'].sum().reset_index()\n\n\n# 2. Calculate total sales for each country-store combination\ntotal_sales = df_sorted.groupby(['country', 'store'], as_index=False)['num_sold'].sum()\n\n# 3. Merge to get total sales for each country-store combination into the grouped DataFrame\nresult = pd.merge(grouped, total_sales, on=['country', 'store'], suffixes=('', '_total'))\n\n# 4. Calculate the percentage contribution for each country-store-product combination\nresult['percentage'] = (result['num_sold'] / result['num_sold_total']) * 100\n\n# Display the result\nresult", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-20T18:24:04.017552Z", "iopub.execute_input": "2025-01-20T18:24:04.017855Z", "iopub.status.idle": "2025-01-20T18:24:04.160689Z", "shell.execute_reply.started": "2025-01-20T18:24:04.017829Z", "shell.execute_reply": "2025-01-20T18:24:04.159633Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "#def plot_total_sales_by_date(country, store, df):\n#    # Filter the data for the given country and store\n#    filtered_df = df[(df['country'] == country) & (df['store'] == store)]\n#    \n#    # Group by date and sum the num_sold for all products\n#    total_sales_by_date = filtered_df.groupby('date')['num_sold'].sum().reset_index()\n#    \n#    # Check if data exists for the given combination\n#    if total_sales_by_date.empty:\n#        print(f\"No data available for {country} - {store}.\")\n#        return\n    # Create a line plot for total num_sold by date\n##    \n#    plt.figure(figsize=(8, 6))\n#    sns.lineplot(data=total_sales_by_date, x='date', y='num_sold')\n\n    # Set plot labels and title\n#   plt.xlabel('Date')\n#   plt.ylabel('Total Number Sold')\n#   plt.title(f'Total Sales in {store} ({country}) by Date')\n\n    # Rotate the date labels for better readability\n#    plt.xticks(rotation=45)\n\n#    # Show the plot\n#    plt.tight_layout()\n#    plt.show()\n\n\n#plot_total_sales_by_date('Finland', 'Discount Stickers', df_sorted)  # You can change this to an\n#plot_total_sales_by_date('Norway', 'Discount Stickers', df_sorted)  # You can change this to an\n#plot_total_sales_by_date('Singapore', 'Discount Stickers', df_sorted)  # You can change this to an\n#plot_total_sales_by_date('Italy', 'Discount Stickers', df_sorted)  # You can change this to an\n#plot_total_sales_by_date('Canada', 'Discount Stickers', df_sorted)  # You can change this to an\n#plot_total_sales_by_date('Kenya', 'Discount Stickers', df_sorted)  # You can change this to an\n", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-20T18:24:04.161925Z", "iopub.execute_input": "2025-01-20T18:24:04.162377Z", "iopub.status.idle": "2025-01-20T18:24:04.166971Z", "shell.execute_reply.started": "2025-01-20T18:24:04.162338Z", "shell.execute_reply": "2025-01-20T18:24:04.165968Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "def set_sales_for_specific_product_with_ratio(df, target_country, target_store, target_product, source_country, source_store, source_product):\n    \"\"\"\n    This method sets the num_sold for a specific product in a source country-store combination to \n    the num_sold values from a target country-store combination, \n    scaled by a ratio calculated as the total num_sold at the target country-store combination \n    divided by the total num_sold at the source country-store combination (for all products).\n    \n    Parameters:\n    - df: The DataFrame containing the sales data.\n    - target_country: The country from which to take the target sales values.\n    - target_store: The store from which to take the target sales values.\n    - target_product: The product whose sales will be used to scale the source product's sales.\n    - source_country: The country to which the sales values will be updated.\n    - source_store: The store to which the sales values will be updated.\n    - source_product: The product whose sales will be updated.\n    \n    Returns:\n    - The updated DataFrame with num_sold for the specific product scaled by the ratio for the source combination.\n    \"\"\"\n    \n    # Filter the target data for the target country-store combination and the specific product\n    target_data = df[(df['country'] == target_country) & \n                     (df['store'] == target_store) & \n                     (df['product'] == target_product)]\n    \n    # Filter the source data for the source country-store combination and the specific product\n    source_data = df[(df['country'] == source_country) & \n                     (df['store'] == source_store) & \n                     (df['product'] == source_product)]\n    \n    # Calculate total num_sold for all products in the target and source country-store combinations\n    total_target_sales = df[(df['country'] == target_country) & \n                            (df['store'] == target_store)]['num_sold'].sum()\n    \n    total_source_sales = df[(df['country'] == source_country) & \n                            (df['store'] == source_store)]['num_sold'].sum()\n    \n    # Avoid division by zero\n    if total_source_sales == 0:\n        raise ValueError(f\"Total sales for the source combination ({source_country}-{source_store}) is zero.\")\n    \n    # Calculate the ratio (total num_sold at target / total num_sold at source)\n    ratio = total_target_sales / total_source_sales\n\n    print('total_target_sales', total_target_sales)\n    print('total_source_sales', total_source_sales)\n    print('ratio:', ratio)\n   \n    df = df.merge(source_data[['date', 'num_sold']], on='date', how='left', suffixes=('', '_source'))\n\n    # Set the num_sold for the source product based on the target num_sold scaled by the ratio\n    df.loc[(df['country'] == target_country) & \n           (df['store'] == target_store) & \n           (df['product'] == target_product), 'num_sold'] = df['num_sold_source'] * ratio\n    \n    # Drop the extra merged column\n    df.drop(columns=['num_sold_source'], inplace=True)\n\n    return df", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-20T18:24:04.16832Z", "iopub.execute_input": "2025-01-20T18:24:04.16867Z", "iopub.status.idle": "2025-01-20T18:24:04.189153Z", "shell.execute_reply.started": "2025-01-20T18:24:04.168644Z", "shell.execute_reply": "2025-01-20T18:24:04.187874Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "\ntrain = set_sales_for_specific_product_with_ratio(df_sorted, 'Canada', 'Discount Stickers', 'Holographic Goose', 'Italy', 'Discount Stickers', 'Holographic Goose')\n#train = set_sales_for_specific_product_with_ratio(train, 'Kenya', 'Discount Stickers', 'Holographic Goose', 'Singapore', 'Discount Stickers', 'Holographic Goose')\n\ntrain[train['country'] == 'Canada'].head(200)", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-20T18:24:04.19036Z", "iopub.execute_input": "2025-01-20T18:24:04.190795Z", "iopub.status.idle": "2025-01-20T18:24:04.538399Z", "shell.execute_reply.started": "2025-01-20T18:24:04.190735Z", "shell.execute_reply": "2025-01-20T18:24:04.537165Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "def fill_nulls_for_Kenya(df):\n    \"\"\"\n    This method updates the num_sold for each product in the target country-store combination\n    by scaling the num_sold of the corresponding product in the source country-store combination.\n    The scaling ratio is calculated as the total num_sold at target country-store divided by the \n    total num_sold at source country-store (for all products).\n\n    Parameters:\n    - df: The DataFrame containing the sales data.\n    - target_country: The country from which to take the target sales values.\n    - target_store: The store from which to take the target sales values.\n    - source_country: The country from which to take the source sales values.\n    - source_store: The store from which to take the source sales values.\n    - source_product: The specific product in the source store-country combination to scale.\n    \n    Returns:\n    - The updated DataFrame with num_sold for the target country-store-product combination.\n    \"\"\"\n\n    #total_sales_per_day = df[df['country'] == 'Kenya'].groupby('date')['num_sold'].transform('sum')\n\n    total_sales_per_day = df[(df['country'] == 'Kenya') & (df['store'] == 'Discount Stickers')].groupby('date')['num_sold'].sum().reset_index()\n    #total_sales_per_day.rename(columns={'num_sold': 'total_num_sold'}, inplace=True)\n\n    print(total_sales_per_day.head(20))\n    \n    ratio = 4.75 / 95.25\n    print(f\"Calculated ratio: {ratio:.2f}\")\n\n    # Merge source_data with df to get the corresponding target product for the target country-store combination\n    df_merged = df.merge(total_sales_per_day[['date', 'num_sold']], on='date', how='left', suffixes=('', '_source'))\n    \n    # Now, update num_sold for the target country-store-product combination\n    df_merged.loc[(df_merged['country'] == 'Kenya') & \n                  (df_merged['store'] == 'Discount Stickers') & \n                  (df_merged['product'] == 'Holographic Goose'), 'num_sold'] = df_merged['num_sold_source'] * ratio\n    \n    # Debug: Check if num_sold has been updated\n    print(\"After update:\")\n    print(df_merged[df_merged['country'] == 'Kenya'][['country', 'store', 'product', 'date', 'num_sold']].head())\n\n    # Drop the extra merged column\n    df_merged.drop(columns=['num_sold_source'], inplace=True)\n    \n    return df_merged\n\n\ntrain = fill_nulls_for_Kenya(train)\n#train = set_sales_for_specific_product_with_ratio(train, 'Kenya', 'Discount Stickers', 'Holographic Goose', 'Singapore', 'Discount Stickers', 'Holographic Goose')\n\ntrain[train['country'] == 'Kenya'].head(200)\n", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-20T18:24:04.539767Z", "iopub.execute_input": "2025-01-20T18:24:04.540194Z", "iopub.status.idle": "2025-01-20T18:24:04.757175Z", "shell.execute_reply.started": "2025-01-20T18:24:04.540152Z", "shell.execute_reply": "2025-01-20T18:24:04.755972Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "train.info()", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-20T18:24:04.758129Z", "iopub.execute_input": "2025-01-20T18:24:04.758395Z", "iopub.status.idle": "2025-01-20T18:24:04.81353Z", "shell.execute_reply.started": "2025-01-20T18:24:04.758373Z", "shell.execute_reply": "2025-01-20T18:24:04.812352Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "# Feature engineering function\ndef feature_engineering(df):\n    # Generate new columns from date\n    df['date'] = pd.to_datetime(df['date'])\n    df['year'] = df['date'].dt.year\n    df['month'] = df['date'].dt.month\n    df['week'] = df['date'].dt.isocalendar().week\n    df['day'] = df['date'].dt.day\n    df['day_of_week'] = df['date'].dt.dayofweek\n    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n    df['sine_day'] = np.sin(2 * np.pi * df['day'] / 31)\n    df['cos_day'] = np.cos(2 * np.pi * df['day'] / 31)\n    df['sine_month'] = np.sin(2 * np.pi * df['month'] / 12)\n    df['cos_month'] = np.cos(2 * np.pi * df['month'] / 12)\n    df['sine_year'] = np.sin(2 * np.pi * df['year'])\n    df['cos_year'] = np.cos(2 * np.pi * df['year'])\n\n    # Convert categorical columns to category dtype\n    for col in ['country', 'store', 'product']:\n        df[col] = df[col].astype('category')\n\n    return df\n\ntrain = feature_engineering(train)", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-20T18:24:04.814867Z", "iopub.execute_input": "2025-01-20T18:24:04.815201Z", "iopub.status.idle": "2025-01-20T18:24:05.007825Z", "shell.execute_reply.started": "2025-01-20T18:24:04.815173Z", "shell.execute_reply": "2025-01-20T18:24:05.006951Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "train.info()", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-20T18:24:05.010353Z", "iopub.execute_input": "2025-01-20T18:24:05.010643Z", "iopub.status.idle": "2025-01-20T18:24:05.032554Z", "shell.execute_reply.started": "2025-01-20T18:24:05.01062Z", "shell.execute_reply": "2025-01-20T18:24:05.031507Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "# Apply feature engineering and preprocessing\ntrain = train.dropna() #feature_engineering(train)\ntrain.info()\n#train = preprocess_data(train)\n\n# Prepare data for training\nX = train.drop(columns=['id', 'date', 'num_sold'])\nX = pd.get_dummies(X, drop_first=True)\ny = np.log1p(train['num_sold'])\n\n# Load test data\ntest = feature_engineering(test)\nX_test = test.drop(columns=['id', 'date'])\nX_test = pd.get_dummies(X_test, drop_first=True)\nX_test = X_test.reindex(columns=X.columns, fill_value=0)\n", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-20T18:24:05.033814Z", "iopub.execute_input": "2025-01-20T18:24:05.03413Z", "iopub.status.idle": "2025-01-20T18:24:05.216052Z", "shell.execute_reply.started": "2025-01-20T18:24:05.034104Z", "shell.execute_reply": "2025-01-20T18:24:05.21515Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "# Initialize variables\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\ntrain_predictions = np.zeros(len(train))\nmape_scores = []\ntest_predictions_list = []\n\n# CatBoost Hyperparameters\nparams = {\n    'iterations': 1000,             # Number of trees to build\n    'learning_rate': 0.1,           # Learning rate\n    'depth': 6,                     # Depth of the tree\n    'loss_function': 'MAPE',        # Loss function for regression\n    'cat_features': [],             # List of categorical features (if applicable)\n    'random_seed': 42,              # Random seed for reproducibility\n    'verbose': 200,                 # Print information every 200 iterations\n}\n\n\n# Train and validate model using 5-fold cross-validation\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X), start=1):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n\n    # Train CatBoostRegressor\n    model = LGBMRegressor()#XGBRegressor()#ExtraTreesRegressor(random_state=42, n_jobs=-1)  # RandomForestRegressor(random_state=42, n_jobs=-1) # CatBoostRegressor(**params) # #\n    model.fit(X_train, y_train)\n\n    # Make predictions\n    y_val_pred = model.predict(X_val)\n    train_predictions[val_idx] = y_val_pred\n\n    # Calculate MAPE\n    mape = mean_absolute_percentage_error(y_val, y_val_pred)\n    mape_scores.append(mape)\n    print(f\"Fold {fold}: MAPE = {mape:.4f}\")\n\n    # Predict on test data for this fold\n    test_pred_fold = model.predict(X_test)\n    test_predictions_list.append(test_pred_fold)\n\n# Average test predictions across folds\ntest_predictions_avg = np.mean(test_predictions_list, axis=0)\ntest_predictions_avg = np.expm1(test_predictions_avg) \n\n# Print training MAPE score\nprint(f\"Training MAPE score (5-fold average): {np.mean(mape_scores):.4f}\")\n", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-20T18:24:05.217195Z", "iopub.execute_input": "2025-01-20T18:24:05.217526Z", "iopub.status.idle": "2025-01-20T18:24:13.144089Z", "shell.execute_reply.started": "2025-01-20T18:24:05.2175Z", "shell.execute_reply": "2025-01-20T18:24:13.142827Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "# Plot actual vs. predicted values (daily, weekly, monthly)\n#def plot_predictions(data, predictions, freq, title):\n#    data['predicted'] = np.expm1(predictions)\n#    aggregated = data.groupby(pd.Grouper(key='date', freq=freq))[['num_sold', 'predicted']].sum()\n\n#    plt.figure(figsize=(12, 6))\n#    plt.plot(aggregated.index, aggregated['num_sold'], label='Actual')\n#    plt.plot(aggregated.index, aggregated['predicted'], label='Predicted')\n#    plt.title(title)\n#    plt.xlabel('Date')\n#    plt.ylabel('Num Sold')\n#    plt.legend()\n#    plt.show()\n\n#plot_predictions(train, train_predictions, 'D', 'Daily Actual vs Predicted')\n#plot_predictions(train, train_predictions, 'W', 'Weekly Actual vs Predicted')\n#plot_predictions(train, train_predictions, 'M', 'Monthly Actual vs Predicted')", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-20T18:24:13.1452Z", "iopub.execute_input": "2025-01-20T18:24:13.145608Z", "iopub.status.idle": "2025-01-20T18:24:13.150573Z", "shell.execute_reply.started": "2025-01-20T18:24:13.145578Z", "shell.execute_reply": "2025-01-20T18:24:13.149434Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "# Print test predictions\ntest['predicted_num_sold'] = test_predictions_avg\nprint(test[['id', 'predicted_num_sold']])\n\n# Save predictions\ntest[['id', 'predicted_num_sold']].to_csv('submission.csv', index=False)\n", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-20T18:24:13.151647Z", "iopub.execute_input": "2025-01-20T18:24:13.152075Z", "iopub.status.idle": "2025-01-20T18:24:13.388518Z", "shell.execute_reply.started": "2025-01-20T18:24:13.152009Z", "shell.execute_reply": "2025-01-20T18:24:13.387367Z"}}, "outputs": [], "execution_count": null}]}