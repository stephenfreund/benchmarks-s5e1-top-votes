{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-17T10:08:15.805488Z",
     "iopub.status.busy": "2025-01-17T10:08:15.804952Z",
     "iopub.status.idle": "2025-01-17T10:08:31.90648Z",
     "shell.execute_reply": "2025-01-17T10:08:31.905152Z",
     "shell.execute_reply.started": "2025-01-17T10:08:15.80544Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder, label_binarize, OrdinalEncoder\n",
    "from category_encoders import CatBoostEncoder, MEstimateEncoder\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, HistGradientBoostingClassifier, GradientBoostingClassifier, HistGradientBoostingRegressor\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression, LinearRegression, Ridge, BayesianRidge\n",
    "\n",
    "from sklearn import set_config\n",
    "import os\n",
    "\n",
    "import optuna\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, mean_squared_error, precision_recall_curve, make_scorer, confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay, matthews_corrcoef, mean_absolute_percentage_error\n",
    "from scipy.stats import norm, skew\n",
    "\n",
    "from colorama import Fore, Style, init\n",
    "from copy import deepcopy\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, StratifiedKFold, KFold, RepeatedKFold, cross_val_score, StratifiedGroupKFold, GroupKFold\n",
    "from xgboost import DMatrix, XGBClassifier, XGBRegressor\n",
    "from lightgbm import log_evaluation, early_stopping, LGBMClassifier, LGBMRegressor, Dataset\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor, Pool\n",
    "from tqdm.notebook import tqdm\n",
    "from optuna.samplers import TPESampler, CmaEsSampler\n",
    "from optuna.pruners import HyperbandPruner\n",
    "from functools import partial\n",
    "from IPython.display import display_html, clear_output\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import gc\n",
    "import re\n",
    "import holidays\n",
    "import requests\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ac",
   "metadata": {},
   "source": [
    "# <p style=\"border-radius: 40px; color: white; font-weight: bold; font-size: 150%; text-align: center; background-color:#3cb371; padding: 5px 5px 5px 5px;\">Configuration</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T10:08:31.908687Z",
     "iopub.status.busy": "2025-01-17T10:08:31.907844Z",
     "iopub.status.idle": "2025-01-17T10:08:32.57003Z",
     "shell.execute_reply": "2025-01-17T10:08:32.568659Z",
     "shell.execute_reply.started": "2025-01-17T10:08:31.908644Z"
    }
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \n",
    "    state = 42\n",
    "    n_splits = 5\n",
    "    early_stop = 200\n",
    "        \n",
    "    target = 'num_sold'\n",
    "    train = pd.read_csv('./input/playground-series-s5e1/train.csv', index_col='id')\n",
    "    test = pd.read_csv('./input/playground-series-s5e1/test.csv', index_col='id')\n",
    "    submission = pd.read_csv('./input/playground-series-s5e1/sample_submission.csv')\n",
    "\n",
    "    original_data = 'N'\n",
    "    outliers = 'N'\n",
    "    log_trf = 'Y'\n",
    "    scaler_trf = 'N'\n",
    "    feature_eng = 'Y'\n",
    "    missing = 'Y'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e8",
   "metadata": {},
   "source": [
    "# <p style=\"border-radius: 40px; color: white; font-weight: bold; font-size: 150%; text-align: center; background-color:#3cb371; padding: 5px 5px 5px 5px;\">EDA</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T10:08:32.57293Z",
     "iopub.status.busy": "2025-01-17T10:08:32.572506Z",
     "iopub.status.idle": "2025-01-17T10:08:32.590113Z",
     "shell.execute_reply": "2025-01-17T10:08:32.58856Z",
     "shell.execute_reply.started": "2025-01-17T10:08:32.572902Z"
    }
   },
   "outputs": [],
   "source": [
    "class EDA(Config):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cat_features = self.train.drop(self.target, axis=1).select_dtypes(include=['object']).columns.tolist()\n",
    "        self.num_features = self.train.drop(self.target, axis=1).select_dtypes(exclude=['object']).columns.tolist()\n",
    "        self.data_info()\n",
    "        self.cat_feature_plots()\n",
    "        self.target_plot()\n",
    "                \n",
    "    def data_info(self):\n",
    "        \n",
    "        for data, label in zip([self.train, self.test], ['Train', 'Test']):\n",
    "            table_style = [{'selector': 'th:not(.index_name)',\n",
    "                            'props': [('background-color', '#3cb371'),\n",
    "                                      ('color', '#FFFFFF'),\n",
    "                                      ('font-weight', 'bold'),\n",
    "                                      ('border', '1px solid #DCDCDC'),\n",
    "                                      ('text-align', 'center')]\n",
    "                            }, \n",
    "                            {'selector': 'tbody td',\n",
    "                             'props': [('border', '1px solid #DCDCDC'),\n",
    "                                       ('font-weight', 'normal')]\n",
    "                            }]\n",
    "            print(Style.BRIGHT+Fore.GREEN+f'\\n{label} head\\n')\n",
    "            display(data.head().style.set_table_styles(table_style))\n",
    "                           \n",
    "            print(Style.BRIGHT+Fore.GREEN+f'\\n{label} info\\n'+Style.RESET_ALL)               \n",
    "            display(data.info())\n",
    "            \n",
    "            print(Style.BRIGHT+Fore.GREEN+f'\\n{label} missing values\\n'+Style.RESET_ALL)               \n",
    "            display(data.isna().sum())\n",
    "        return self\n",
    "        \n",
    "    def cat_feature_plots(self):\n",
    "        fig, axes = plt.subplots(len(self.cat_features), 2 ,figsize = (18, len(self.cat_features) * 6), \n",
    "                                 gridspec_kw = {'hspace': 0.5, \n",
    "                                                'wspace': 0.2,\n",
    "                                               }\n",
    "                                )\n",
    "\n",
    "        for i, col in enumerate(self.cat_features):\n",
    "            \n",
    "            ax = axes[i,0]\n",
    "            sns.barplot(data=self.train[col].value_counts().nlargest(10).reset_index(), x=col, y='count', ax=ax, color='#3cb371')\n",
    "            ax.set(xlabel = '', ylabel = '')\n",
    "            ax.set_title(f\"\\n{col} Train\")\n",
    "            \n",
    "            ax = axes[i,1]\n",
    "            sns.barplot(data=self.train[col].value_counts().nlargest(10).reset_index(), x=col, y='count', ax=ax, color='r')\n",
    "            ax.set(xlabel = '', ylabel = '')\n",
    "            ax.set_title(f\"\\n{col} Test\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def target_plot(self):\n",
    "        print(Style.BRIGHT+Fore.GREEN+f\"\\nTarget feature distribution\\n\")\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2 ,figsize = (14, 6), \n",
    "                                 gridspec_kw = {'hspace': 0.3, \n",
    "                                                'wspace': 0.2, \n",
    "                                                'width_ratios': [0.70, 0.30]\n",
    "                                               }\n",
    "                                )\n",
    "        ax = axes[0]\n",
    "        sns.kdeplot(data = self.train[self.target], \n",
    "                    color = '#3cb371', ax = ax, linewidth = 2\n",
    "                   )\n",
    "        ax.set(xlabel = '', ylabel = '')\n",
    "        ax.set_title(f\"\\n{self.target}\")\n",
    "        ax.grid()\n",
    "\n",
    "        ax = axes[1]\n",
    "        sns.boxplot(data = self.train, y = self.target, width = 0.5,\n",
    "                    linewidth = 1, fliersize= 1,\n",
    "                    ax = ax, color = '#3cb371'\n",
    "                   )\n",
    "        ax.set_title(f\"\\n{self.target}\")\n",
    "        ax.set(xlabel = '', ylabel = '')\n",
    "        ax.tick_params(axis='both', which='major')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T10:08:32.592666Z",
     "iopub.status.busy": "2025-01-17T10:08:32.592197Z",
     "iopub.status.idle": "2025-01-17T10:08:35.993625Z",
     "shell.execute_reply": "2025-01-17T10:08:35.992547Z",
     "shell.execute_reply.started": "2025-01-17T10:08:32.59263Z"
    }
   },
   "outputs": [],
   "source": [
    "eda = EDA()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094a",
   "metadata": {},
   "source": [
    "# <p style=\"border-radius: 40px; color: white; font-weight: bold; font-size: 150%; text-align: center; background-color:#3cb371; padding: 5px 5px 5px 5px;\">Data Transformation</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b753",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T10:08:35.994911Z",
     "iopub.status.busy": "2025-01-17T10:08:35.994634Z",
     "iopub.status.idle": "2025-01-17T10:08:36.045533Z",
     "shell.execute_reply": "2025-01-17T10:08:36.044383Z",
     "shell.execute_reply.started": "2025-01-17T10:08:35.994888Z"
    }
   },
   "outputs": [],
   "source": [
    "class Transform(Config):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        if Config.original_data == 'Y':\n",
    "            self.train = pd.concat([self.train, self.train_org], ignore_index=True).drop_duplicates()\n",
    "            self.train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        self.num_features = self.train.drop(self.target, axis=1).select_dtypes(exclude=['object', 'bool']).columns.tolist()\n",
    "        self.cat_features = self.train.drop(self.target, axis=1).select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "     \n",
    "        self.train_raw = self.train.copy()\n",
    "        \n",
    "        if self.feature_eng == 'Y':\n",
    "            self.train = self.new_features(self.train)\n",
    "            self.test = self.new_features(self.test)\n",
    "            self.train_raw = self.new_features(self.train_raw)\n",
    "        \n",
    "        if self.missing == 'Y':\n",
    "            self.missing_values()\n",
    "            \n",
    "        self.num_features = self.train.drop(self.target, axis=1).select_dtypes(exclude=['object', 'bool']).columns.tolist()\n",
    "        self.cat_features = self.train.drop(self.target, axis=1).select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "            \n",
    "        if self.outliers == 'Y':    \n",
    "            self.remove_outliers()\n",
    "            \n",
    "        if self.log_trf == 'Y':\n",
    "            self.log_transformation()\n",
    "            \n",
    "        if self.scaler_trf == 'Y':\n",
    "            self.scaler()\n",
    "            \n",
    "        self.train_enc = self.train.copy()\n",
    "        self.test_enc = self.test.copy()\n",
    "        self.encode()\n",
    "        \n",
    "        if self.outliers == 'Y' or self.log_trf == 'Y' or self.scaler_trf =='Y':\n",
    "            self.distribution()\n",
    "        \n",
    "    def __call__(self):\n",
    "\n",
    "        self.train[self.cat_features] = self.train[self.cat_features].astype('category')\n",
    "        self.test[self.cat_features] = self.test[self.cat_features].astype('category')\n",
    "\n",
    "        self.cat_features_card = []\n",
    "        for f in self.cat_features:\n",
    "            self.cat_features_card.append(self.train[f].nunique())\n",
    "\n",
    "        self.train = self.reduce_mem(self.train)\n",
    "        self.test = self.reduce_mem(self.test)\n",
    "  \n",
    "        self.y = self.train[self.target]\n",
    "        self.train = self.train.drop(self.target, axis=1)\n",
    "        self.train_enc = self.train_enc.drop(self.target, axis=1)\n",
    "        \n",
    "        return self.train, self.train_enc, self.y, self.test, self.test_enc, self.cat_features\n",
    "    \n",
    "    def encode(self):\n",
    "        data = pd.concat([self.test, self.train])\n",
    "        oe = OrdinalEncoder()\n",
    "        data[self.cat_features] = oe.fit_transform(data[self.cat_features]).astype('int')\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        data[self.num_features + [self.target]] = scaler.fit_transform(data[self.num_features + [self.target]])\n",
    "        \n",
    "        self.train_enc = data[~data[self.target].isna()]\n",
    "        self.test_enc = data[data[self.target].isna()].drop(self.target, axis=1)\n",
    "            \n",
    "    def new_features(self, data):\n",
    " \n",
    "        data['date'] = pd.to_datetime(data['date'])\n",
    "        data[\"quarter\"] = data[\"date\"].dt.quarter.astype('object')\n",
    "        data['month'] = data['date'].dt.month\n",
    "        data['month_sin'] = np.sin(data['month'] * (2 * np.pi / 12))\n",
    "        data['month_cos'] = np.cos(data['month'] * (2 * np.pi / 12))\n",
    "        data['day'] = data['date'].dt.day\n",
    "        data['day_sin'] = np.sin(data['day'] * (2 * np.pi / 31))\n",
    "        data['day_cos'] = np.cos(data['day'] * (2 * np.pi / 31))\n",
    "        data['day_of_week'] = data['date'].dt.dayofweek.astype('object')\n",
    "        data['week'] = data['date'].dt.isocalendar().week\n",
    "        data['week_sin'] = np.sin(data['week'] * (2 * np.pi/ 53))\n",
    "        data['week_cos'] = np.cos(data['week'] * (2 * np.pi/ 53))       \n",
    "        data['year'] = data['date'].dt.year\n",
    "        data['cos_year'] = np.cos(data['year'] * (2 * np.pi) / 100)\n",
    "        data['sin_year'] = np.sin(data['year'] * (2 * np.pi) / 100)\n",
    "        data[['month', 'day', 'week', 'year']] = data[['month', 'day', 'week', 'year']].astype('object')\n",
    "        data['group'] = (data['year'] - 2020 )*48 + data['month']*4 + data['day'] // 7\n",
    "        data = pd.get_dummies(data, columns = [\"day_of_week\"], drop_first=True)\n",
    "        \n",
    "        for day in range(24, 32):\n",
    "            data[f'dec{day}'] = (data.date.dt.day.eq(day) & data.date.dt.month.eq(12)).astype(np.uint8).astype('object')\n",
    "\n",
    "        alpha2 = dict(zip(np.sort(data.country.unique()), ['CA', 'FI', 'IT', 'KE', 'NO', 'SG']))\n",
    "        h = {c: holidays.country_holidays(a, years=range(2010, 2020)) for c, a in alpha2.items()}\n",
    "        data['is_holiday'] = 0\n",
    "        for c in alpha2:\n",
    "            data.loc[data.country==c, 'is_holiday'] = data.date.isin(h[c]).astype(int).astype('object')\n",
    "            \n",
    "        gdp = []\n",
    "        for country in data.country.unique():\n",
    "            row = []\n",
    "            for year in range(2010,2020):\n",
    "                row.append(self.get_gdp(country,year))\n",
    "            gdp.append(row)\n",
    "        \n",
    "        gdp = np.array(gdp)\n",
    "        gdp /= np.sum(gdp,axis=0)\n",
    "        \n",
    "        gdp_df = pd.DataFrame(gdp, index=data.country.unique(), columns=range(2010,2020))\n",
    "        gdp_df = gdp_df.reset_index().melt(id_vars = ['index']).rename({'index': 'country', 'variable': 'year', 'value': 'GDP'}, axis=1)\n",
    "        data = data.merge(gdp_df, how = \"left\", on = [\"year\", \"country\"])\n",
    "        return data\n",
    "\n",
    "    def log_transformation(self):\n",
    "        self.train[self.target] = np.log1p(self.train[self.target]) \n",
    "        return self\n",
    "    \n",
    "    def distribution(self):\n",
    "        print(Style.BRIGHT+Fore.GREEN+f'\\nHistograms of distribution\\n')\n",
    "        fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "\n",
    "        ax_r, ax_n = axes\n",
    "\n",
    "        ax_r.set_title(f'{self.target} ($\\mu=$ {self.train_raw[self.target].mean():.2f} and $\\sigma=$ {self.train_raw[self.target].std():.2f} )')\n",
    "        ax_r.hist(self.train_raw[self.target], bins=30, color='#3cb371')\n",
    "        ax_r.axvline(self.train_raw[self.target].mean(), color='r', label='Mean')\n",
    "        ax_r.axvline(self.train_raw[self.target].median(), color='y', linestyle='--', label='Median')\n",
    "        ax_r.legend()\n",
    "\n",
    "        ax_n.set_title(f'{self.target} Normalized ($\\mu=$ {self.train_enc[self.target].mean():.2f} and $\\sigma=$ {self.train_enc[self.target].std():.2f} )')\n",
    "        ax_n.hist(self.train_enc[self.target], bins=30, color='#3cb371')\n",
    "        ax_n.axvline(self.train_enc[self.target].mean(), color='r', label='Mean')\n",
    "        ax_n.axvline(self.train_enc[self.target].median(), color='y', linestyle='--', label='Median')\n",
    "        ax_n.legend()\n",
    "        \n",
    "    def remove_outliers(self):\n",
    "        Q1 = self.train[self.targets].quantile(0.25)\n",
    "        Q3 = self.train[self.targets].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_limit = Q1 - 1.5*IQR\n",
    "        upper_limit = Q3 + 1.5*IQR\n",
    "        self.train = self.train[(self.train[self.targets] >= lower_limit) & (self.train[self.targets] <= upper_limit)]\n",
    "        self.train.reset_index(drop=True, inplace=True) \n",
    "        \n",
    "    def scaler(self):\n",
    "        scaler = StandardScaler()\n",
    "        self.train[self.num_features] = scaler.fit_transform(self.train[self.num_features])\n",
    "        self.test[self.num_features] = scaler.transform(self.test[self.num_features])\n",
    "        return self\n",
    "    \n",
    "    def missing_values(self):\n",
    "        train_df_imputed = self.train.copy()\n",
    "        for year in train_df_imputed[\"year\"].unique():\n",
    "            target_ratio = train_df_imputed.loc[(train_df_imputed[\"year\"] == year) & (train_df_imputed[\"country\"] == \"Norway\"), \"GDP\"].values[0]\n",
    "            current_raito = train_df_imputed.loc[(train_df_imputed[\"year\"] == year) & (train_df_imputed[\"country\"] == \"Canada\"), \"GDP\"].values[0]\n",
    "            ratio_can = current_raito / target_ratio\n",
    "            train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Canada\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year), \"num_sold\"] * ratio_can).values\n",
    "\n",
    "            current_ts =  train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Canada\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year)]\n",
    "            missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n",
    "            train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Canada\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] * ratio_can).values\n",
    "\n",
    "            current_ts =  train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Canada\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year)]\n",
    "            missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n",
    "            train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Canada\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] * ratio_can).values\n",
    "\n",
    "            current_raito = train_df_imputed.loc[(train_df_imputed[\"year\"] == year) & (train_df_imputed[\"country\"] == \"Kenya\"), \"GDP\"].values[0]\n",
    "            ratio_ken = current_raito / target_ratio\n",
    "            train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Holographic Goose\")& (train_df_imputed[\"year\"] == year), \"num_sold\"] * ratio_ken).values\n",
    "\n",
    "            current_ts = train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year)]\n",
    "            missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n",
    "            train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] * ratio_ken).values\n",
    "\n",
    "            current_ts = train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year)]\n",
    "            missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n",
    "            train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] * ratio_ken).values\n",
    "\n",
    "            current_ts = train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Kerneler\") & (train_df_imputed[\"year\"] == year)]\n",
    "            missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n",
    "            train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Kerneler\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Kerneler\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] * ratio_ken).values\n",
    "        \n",
    "        train_df_imputed.loc[train_df_imputed.index == 23719, \"num_sold\"] = 4\n",
    "        train_df_imputed.loc[train_df_imputed.index == 207003, \"num_sold\"] = 195\n",
    "        self.train = train_df_imputed.drop('date', axis=1)\n",
    "        self.test.drop('date', axis=1, inplace=True)\n",
    "        return self\n",
    "\n",
    "    def reduce_mem(self, df):\n",
    "\n",
    "        numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64', \"uint16\", \"uint32\", \"uint64\"]\n",
    "        \n",
    "        for col in df.columns:\n",
    "            col_type = df[col].dtypes\n",
    "            \n",
    "            if col_type in numerics:\n",
    "                c_min = df[col].min()\n",
    "                c_max = df[col].max()\n",
    "\n",
    "                if \"int\" in str(col_type):\n",
    "                    if c_min >= np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                        df[col] = df[col].astype(np.int8)\n",
    "                    elif c_min >= np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                        df[col] = df[col].astype(np.int16)\n",
    "                    elif c_min >= np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                        df[col] = df[col].astype(np.int32)\n",
    "                    elif c_min >= np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                        df[col] = df[col].astype(np.int64)  \n",
    "                else:\n",
    "                    if c_min >= np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                        df[col] = df[col].astype(np.float16)\n",
    "                    if c_min >= np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                        df[col] = df[col].astype(np.float32)\n",
    "                    else:\n",
    "                        df[col] = df[col].astype(np.float64)  \n",
    "\n",
    "        return df\n",
    "\n",
    "    def get_gdp(self, country,year):\n",
    "        alpha3 = {'Canada'    :'CAN',\n",
    "                  'Finland'   :'FIN',\n",
    "                  'Italy'     :'ITA',\n",
    "                  'Kenya'     :'KEN',\n",
    "                  'Norway'    :'NOR',\n",
    "                  'Singapore' :'SGP',\n",
    "                 }\n",
    "                \n",
    "        url=\"https://api.worldbank.org/v2/country/{0}/indicator/NY.GDP.PCAP.CD?date={1}&format=json\".format(alpha3[country],year)\n",
    "        response = requests.get(url).json()\n",
    "        return response[1][0]['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T10:08:36.046773Z",
     "iopub.status.busy": "2025-01-17T10:08:36.046484Z",
     "iopub.status.idle": "2025-01-17T10:09:20.396762Z",
     "shell.execute_reply": "2025-01-17T10:09:20.395662Z",
     "shell.execute_reply.started": "2025-01-17T10:08:36.046749Z"
    }
   },
   "outputs": [],
   "source": [
    "t = Transform()\n",
    "X, X_enc, y, test, test_enc, cat_features = t()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1d",
   "metadata": {},
   "source": [
    "# <p style=\"border-radius: 40px; color: white; font-weight: bold; font-size: 150%; text-align: center; background-color:#3cb371; padding: 5px 5px 5px 5px;\">Model Training</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T10:09:20.398207Z",
     "iopub.status.busy": "2025-01-17T10:09:20.397873Z",
     "iopub.status.idle": "2025-01-17T10:09:20.776168Z",
     "shell.execute_reply": "2025-01-17T10:09:20.775091Z",
     "shell.execute_reply.started": "2025-01-17T10:09:20.398166Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "import contextlib, io\n",
    "import ydf; ydf.verbose(2)\n",
    "from ydf import GradientBoostedTreesLearner\n",
    "\n",
    "def YDFRegressor(learner_class):\n",
    "\n",
    "    class YDFXRegressor(BaseEstimator, RegressorMixin):\n",
    "\n",
    "        def __init__(self, params={}):\n",
    "            self.params = params\n",
    "\n",
    "        def fit(self, X, y):\n",
    "            assert isinstance(X, pd.DataFrame)\n",
    "            assert isinstance(y, pd.Series)\n",
    "            target = y.name\n",
    "            params = self.params.copy()\n",
    "            params['label'] = target\n",
    "            params['task'] = ydf.Task.REGRESSION\n",
    "            X = pd.concat([X, y], axis=1)\n",
    "            with contextlib.redirect_stderr(io.StringIO()), contextlib.redirect_stdout(io.StringIO()):\n",
    "                self.model = learner_class(**params).train(X)\n",
    "            return self\n",
    "\n",
    "        def predict(self, X):\n",
    "            assert isinstance(X, pd.DataFrame)\n",
    "            with contextlib.redirect_stderr(io.StringIO()), contextlib.redirect_stdout(io.StringIO()):\n",
    "                return self.model.predict(X)\n",
    "\n",
    "    return YDFXRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f128",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T10:09:20.779041Z",
     "iopub.status.busy": "2025-01-17T10:09:20.778752Z",
     "iopub.status.idle": "2025-01-17T10:09:20.786683Z",
     "shell.execute_reply": "2025-01-17T10:09:20.785409Z",
     "shell.execute_reply.started": "2025-01-17T10:09:20.779018Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \n",
    "    x_input_cats = layers.Input(shape=(len(cat_features),))\n",
    "    embs = []\n",
    "    for j in range(len(cat_features)):\n",
    "        e = layers.Embedding(t.cat_features_card[j], int(np.ceil(np.sqrt(t.cat_features_card[j]))))\n",
    "        x = e(x_input_cats[:,j])\n",
    "        x = layers.Flatten()(x)\n",
    "        embs.append(x)\n",
    "        \n",
    "    x_input_nums = layers.Input(shape=(len(t.num_features),))\n",
    "    \n",
    "    x = layers.Concatenate(axis=-1)(embs+[x_input_nums]) \n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(1)(x)\n",
    "    \n",
    "    model = keras.Model(inputs=[x_input_cats,x_input_nums], outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T10:09:20.788579Z",
     "iopub.status.busy": "2025-01-17T10:09:20.788175Z",
     "iopub.status.idle": "2025-01-17T10:09:20.82007Z",
     "shell.execute_reply": "2025-01-17T10:09:20.818595Z",
     "shell.execute_reply.started": "2025-01-17T10:09:20.788544Z"
    }
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "    'NN': [_,\n",
    "           False],\n",
    "    'CAT': [CatBoostRegressor(**{'verbose': 0,\n",
    "                                 'random_state': Config.state,\n",
    "                                 'cat_features': cat_features,\n",
    "                                 'early_stopping_rounds': Config.early_stop,\n",
    "                                 'eval_metric': \"RMSE\",\n",
    "                                 'n_estimators' : 2000,\n",
    "                                 'depth': 3,\n",
    "                                 'min_data_in_leaf': 96,\n",
    "                                 'l2_leaf_reg': 8.972890275248485,\n",
    "                                 'bagging_temperature': 0.18658249870341914, \n",
    "                                 'random_strength': 0.14106593468982453,\n",
    "                                 'learning_rate': 0.01,\n",
    "                                }),\n",
    "            False],\n",
    "    'CAT3': [CatBoostRegressor(**{'verbose': 0,\n",
    "                                  'random_state': Config.state,\n",
    "                                  'cat_features': cat_features,\n",
    "                                  'early_stopping_rounds': Config.early_stop,\n",
    "                                  'eval_metric': \"MAPE\",\n",
    "                                  'n_estimators' : 2000,\n",
    "                                  'learning_rate': 0.01,\n",
    "                                  'depth': 8,\n",
    "                                  'min_data_in_leaf': 99,\n",
    "                                  'l2_leaf_reg': 7.7324870113971125, \n",
    "                                  'bagging_temperature': 0.003232535109945575, \n",
    "                                  'random_strength': 0.12145610701952099,\n",
    "                                 }),\n",
    "             False],\n",
    "    'CAT5': [CatBoostRegressor(**{'depth': 7,\n",
    "                                  'min_data_in_leaf': 59,\n",
    "                                  'l2_leaf_reg': 6.485681470975604, \n",
    "                                  'bagging_temperature': 0.728613892125684,\n",
    "                                  'random_strength': 0.3565990691132947,\n",
    "                                  'verbose': 0,\n",
    "                                  'random_state': Config.state,\n",
    "                                  'cat_features': cat_features,\n",
    "                                  'early_stopping_rounds': Config.early_stop,\n",
    "                                  'eval_metric': \"MAPE\",\n",
    "                                  'n_estimators' : 2000,\n",
    "                                  'learning_rate': 0.01,\n",
    "                                  \"task_type\": \"GPU\",\n",
    "                                 }),\n",
    "             False],\n",
    "    'CAT6': [CatBoostRegressor(**{'depth': 10,\n",
    "                                  'min_data_in_leaf': 67,\n",
    "                                  'l2_leaf_reg': 0.010658988402410939,\n",
    "                                  'bagging_temperature': 0.7381549501573549,\n",
    "                                  'random_strength': 0.10057316762567874,\n",
    "                                  'verbose': 0,\n",
    "                                  'random_state': Config.state,\n",
    "                                  'cat_features': cat_features,\n",
    "                                  'early_stopping_rounds': Config.early_stop,\n",
    "                                  'eval_metric': \"MAPE\",\n",
    "                                  'n_estimators' : 2000,\n",
    "                                  'learning_rate': 0.01,\n",
    "                                  'bootstrap_type': 'Poisson',\n",
    "                                  \"task_type\": \"GPU\",\n",
    "                                 }),\n",
    "             False],\n",
    "    'XGB2': [XGBRegressor(**{'tree_method': 'hist',\n",
    "                             'n_estimators': 2000,\n",
    "                             'objective': 'reg:squarederror',\n",
    "                             'random_state': Config.state,\n",
    "                             'enable_categorical': True,\n",
    "                             'verbosity': 0,\n",
    "                             'early_stopping_rounds': Config.early_stop,\n",
    "                             'eval_metric': 'rmse',\n",
    "                             'booster': 'gbtree', \n",
    "                             'max_depth': 3,\n",
    "                             'min_child_weight': 16,\n",
    "                             'subsample': 0.8172380854733758, \n",
    "                             'reg_alpha': 0.2734696712123178, \n",
    "                             'reg_lambda': 0.5865768393479154,\n",
    "                             'colsample_bytree': 0.9766164536195251,\n",
    "                             'n_jobs': -1,\n",
    "                             'learning_rate': 0.01,\n",
    "                             'n_jobs': -1\n",
    "                            }),\n",
    "            False],\n",
    "    'XGB3': [XGBRegressor(**{'tree_method': 'hist',\n",
    "                             'n_estimators': 2000,\n",
    "                             'learning_rate': 0.01,\n",
    "                             'objective': 'reg:squarederror',\n",
    "                             'random_state': Config.state,\n",
    "                             'enable_categorical': True,\n",
    "                             'verbosity': 0,\n",
    "                             'early_stopping_rounds': Config.early_stop,\n",
    "                             'eval_metric': 'mape',\n",
    "                             'booster': 'gbtree',\n",
    "                             'max_depth': 3,\n",
    "                             'min_child_weight': 12,\n",
    "                             'subsample': 0.7720667996291699, \n",
    "                             'reg_alpha': 0.07869714859026081, \n",
    "                             'reg_lambda': 0.9577219578640989, \n",
    "                             'colsample_bytree': 0.9728085969282255, \n",
    "                             'n_jobs': -1\n",
    "                           }),\n",
    "        False],\n",
    "    'XGB4': [XGBRegressor(**{'booster': 'gbtree',\n",
    "                             'max_depth': 3,\n",
    "                             'min_child_weight': 12,\n",
    "                             'subsample': 0.800221370346261,\n",
    "                             'reg_alpha': 0.4571249607822852,\n",
    "                             'reg_lambda': 0.6572354640280187,\n",
    "                             'colsample_bytree': 0.9982441671154363,\n",
    "                             'n_jobs': -1,\n",
    "                             'tree_method': 'hist',\n",
    "                             'n_estimators': 3000,\n",
    "                             'learning_rate': 0.01,\n",
    "                             'objective': 'reg:squarederror',\n",
    "                             'random_state': Config.state,\n",
    "                             'enable_categorical': True,\n",
    "                             'verbosity': 0,\n",
    "                             'early_stopping_rounds': Config.early_stop,\n",
    "                             'eval_metric': 'mape',\n",
    "                             'booster': 'gbtree',\n",
    "                             \"device\": \"cuda\",\n",
    "                            }),\n",
    "             False],\n",
    "    'XGB5': [XGBRegressor(**{'booster': 'gbtree',\n",
    "                             'max_depth': 3,\n",
    "                             'min_child_weight': 19,\n",
    "                             'subsample': 0.8065343833518619,\n",
    "                             'reg_alpha': 0.3577049940509907,\n",
    "                             'reg_lambda': 0.8560297700871249,\n",
    "                             'colsample_bytree': 0.9866141987520272,\n",
    "                             'objective': 'reg:squarederror',\n",
    "                             'n_jobs': -1,\n",
    "                             'tree_method': 'hist',\n",
    "                             'n_estimators': 3000,\n",
    "                             'learning_rate': 0.01,\n",
    "                             'random_state': Config.state,\n",
    "                             'enable_categorical': True,\n",
    "                             'verbosity': 0,\n",
    "                             'early_stopping_rounds': Config.early_stop,\n",
    "                             'eval_metric': 'mape', \n",
    "                             \"device\": \"cuda\",\n",
    "                             }),\n",
    "             False],\n",
    "    'LGBM2': [LGBMRegressor(**{'random_state': Config.state,\n",
    "                               'early_stopping_round': Config.early_stop,\n",
    "                               'categorical_feature': cat_features,\n",
    "                               'verbose': -1,\n",
    "                               'boosting_type': 'gbdt',\n",
    "                               'eval_metric': 'rmse',\n",
    "                               'objective': 'regression_l2',\n",
    "                               'n_estimators': 5000,\n",
    "                               'max_depth': 13, \n",
    "                               'num_leaves': 891, \n",
    "                               'min_child_samples': 16,\n",
    "                               'min_child_weight': 11,\n",
    "                               'colsample_bytree': 0.48639630433139497,\n",
    "                               'reg_alpha': 0.45496760242817474,\n",
    "                               'reg_lambda': 0.9669296995303693,\n",
    "                               'learning_rate': 0.01\n",
    "                              }),\n",
    "             False],\n",
    "    'LGBM3': [LGBMRegressor(**{'random_state': Config.state,\n",
    "                               'early_stopping_round': Config.early_stop,\n",
    "                               'categorical_feature': cat_features,\n",
    "                               'verbose': -1,\n",
    "                               'boosting_type': 'gbdt',\n",
    "                               'eval_metric': 'rmse',\n",
    "                               'objective': 'regression_l2',\n",
    "                               'n_estimators': 2000,\n",
    "                               'max_depth': 6, \n",
    "                               'num_leaves': 328,\n",
    "                               'min_child_samples': 10,\n",
    "                               'min_child_weight': 16,\n",
    "                               'colsample_bytree': 0.4893394195489041,\n",
    "                               'reg_alpha': 0.18334253987924942,\n",
    "                               'reg_lambda': 0.8328414321738785,\n",
    "                               'learning_rate': 0.01\n",
    "                              }),\n",
    "             False],\n",
    "    'LGBM4': [LGBMRegressor(**{'objective': 'regression_l2',\n",
    "                               'metric': 'mape', \n",
    "                               'max_depth': 12, \n",
    "                               'num_leaves': 878,\n",
    "                               'min_child_samples': 29,\n",
    "                               'min_child_weight': 14,\n",
    "                               'colsample_bytree': 0.49788260207319734, \n",
    "                               'reg_alpha': 0.4747476308475839, \n",
    "                               'reg_lambda': 0.6960820486441526,\n",
    "                               'random_state': Config.state,\n",
    "                               'early_stopping_round': Config.early_stop,\n",
    "                               'categorical_feature': cat_features,\n",
    "                               'verbose': -1,\n",
    "                               'boosting_type': 'gbdt',\n",
    "                               'eval_metric': 'mape',\n",
    "                               'objective': 'regression_l2',\n",
    "                               'n_estimators': 3000,\n",
    "                               'learning_rate': 0.01,\n",
    "                               }),\n",
    "              False],\n",
    "    'LGBM5': [LGBMRegressor(**{'objective': 'regression_l2',\n",
    "                               'metric': 'mape', \n",
    "                               'max_depth': 7,\n",
    "                               'num_leaves': 123, \n",
    "                               'min_child_samples': 21,\n",
    "                               'min_child_weight': 24,\n",
    "                               'colsample_bytree': 0.3641261996760593, \n",
    "                               'reg_alpha': 0.03632800166349373, \n",
    "                               'reg_lambda': 0.5287861861476272,\n",
    "                               'random_state': Config.state,\n",
    "                               'early_stopping_round': Config.early_stop,\n",
    "                               'categorical_feature': cat_features,\n",
    "                               'verbose': -1,\n",
    "                               'boosting_type': 'gbdt',\n",
    "                               'n_estimators': 3000,\n",
    "                               'learning_rate': 0.01,\n",
    "                               }),\n",
    "              False],\n",
    "    'LGBM6': [LGBMRegressor(**{'objective': 'regression_l2',\n",
    "                               'metric': 'mape',\n",
    "                               'max_depth': 6,\n",
    "                               'num_leaves': 502,\n",
    "                               'min_child_samples': 23,\n",
    "                               'min_child_weight': 18, \n",
    "                               'colsample_bytree': 0.4714820876493163, \n",
    "                               'reg_alpha': 0.054972003081022576, \n",
    "                               'reg_lambda': 0.5774608955362155,\n",
    "                               'random_state': Config.state,\n",
    "                               'early_stopping_round': Config.early_stop,\n",
    "                               'categorical_feature': cat_features,\n",
    "                               'verbose': -1,\n",
    "                               'boosting_type': 'goss',\n",
    "                               'n_estimators': 3000,\n",
    "                               'learning_rate': 0.01,\n",
    "                              }),\n",
    "             False],\n",
    "    'LGBM7': [LGBMRegressor(**{'objective': 'regression_l2', \n",
    "                               'metric': 'mape',\n",
    "                               'max_depth': 14,\n",
    "                               'num_leaves': 279,\n",
    "                               'min_child_samples': 7,\n",
    "                               'min_child_weight': 24, \n",
    "                               'colsample_bytree': 0.43218993309765835,\n",
    "                               'reg_alpha': 0.42757392987472964,\n",
    "                               'reg_lambda': 0.9039762787446107,\n",
    "                               'random_state': Config.state,\n",
    "                               'early_stopping_round': Config.early_stop,\n",
    "                               'categorical_feature': cat_features,\n",
    "                               'verbose': -1,\n",
    "                               'boosting_type': 'goss',\n",
    "                               'n_estimators': 3000,\n",
    "                               'learning_rate': 0.01,\n",
    "                               }),\n",
    "              False],\n",
    "    'Ridge': [Ridge(tol=1e-2, max_iter=1000000,\n",
    "                    random_state=Config.state),\n",
    "              False],\n",
    "    'BRidge': [BayesianRidge(tol=1e-2, n_iter=1000000),\n",
    "              False],\n",
    "    'LR': [LinearRegression(),\n",
    "              False],\n",
    "    'HGB': [HistGradientBoostingRegressor(**{'max_depth': 4,\n",
    "                                             'loss': 'squared_error',\n",
    "                                             'l2_regularization': 0.014082438341668873,\n",
    "                                             'min_samples_leaf': 39,\n",
    "                                             'max_leaf_nodes': 25,\n",
    "                                             'learning_rate': 0.01,\n",
    "                                             'max_iter': 2000,\n",
    "                                             'random_state': Config.state,\n",
    "                                             'early_stopping': Config.early_stop,\n",
    "                                            }),\n",
    "              False],\n",
    "    'HGB2': [HistGradientBoostingRegressor(**{'max_depth': 4,\n",
    "                                              'loss': 'squared_error',\n",
    "                                              'l2_regularization': 1.0294569289519551e-05,\n",
    "                                              'min_samples_leaf': 12, \n",
    "                                              'max_leaf_nodes': 26,\n",
    "                                              'learning_rate': 0.01,\n",
    "                                              'max_iter': 2000,\n",
    "                                              'random_state': Config.state,\n",
    "                                              'early_stopping': Config.early_stop,\n",
    "                                             }),\n",
    "             False],\n",
    "    'HGB3': [HistGradientBoostingRegressor(**{'max_depth': 13, \n",
    "                                              'loss': 'squared_error',\n",
    "                                              'l2_regularization': 0.05253480068908677,\n",
    "                                              'min_samples_leaf': 19,\n",
    "                                              'max_leaf_nodes': 40,\n",
    "                                              'learning_rate': 0.01,\n",
    "                                              'max_iter': 3000,\n",
    "                                              'random_state': Config.state,\n",
    "                                              'early_stopping': Config.early_stop,\n",
    "                                             }),\n",
    "             False],\n",
    "    'HGB4': [HistGradientBoostingRegressor(**{'max_depth': 4, \n",
    "                                              'loss': 'squared_error', \n",
    "                                              'l2_regularization': 1.3248236291502028e-09,\n",
    "                                              'min_samples_leaf': 39,\n",
    "                                              'max_leaf_nodes': 29,\n",
    "                                              'learning_rate': 0.01,\n",
    "                                              'max_iter': 3000,\n",
    "                                              'random_state': Config.state,\n",
    "                                              'early_stopping': Config.early_stop,\n",
    "                                             }),\n",
    "             False],\n",
    "    'YDF': [YDFRegressor(GradientBoostedTreesLearner)({'num_trees': 1000,\n",
    "                                                       'max_depth': 13,\n",
    "                                                       }),\n",
    "            False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d120",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T10:12:01.486652Z",
     "iopub.status.busy": "2025-01-17T10:12:01.48615Z",
     "iopub.status.idle": "2025-01-17T10:12:01.524352Z",
     "shell.execute_reply": "2025-01-17T10:12:01.523344Z",
     "shell.execute_reply.started": "2025-01-17T10:12:01.486615Z"
    }
   },
   "outputs": [],
   "source": [
    "class Model(Config):\n",
    "    \n",
    "    def __init__(self, X, X_enc, y, test, test_enc, models):\n",
    "        self.y = y\n",
    "        self.models = models\n",
    "        self.scores = pd.DataFrame(columns=['Score'])\n",
    "        self.OOF_preds = pd.DataFrame()\n",
    "        self.TEST_preds = pd.DataFrame()\n",
    "        self.folds = GroupKFold(n_splits=self.n_splits)\n",
    "    def train(self):      \n",
    "        \n",
    "        for model_name, [model, training] in tqdm(self.models.items()):\n",
    "            \n",
    "            if training:\n",
    "                print('='*20)\n",
    "                print(model_name)\n",
    "                if any(model in model_name for model in ['LGBM', 'CAT', 'XGB']):\n",
    "                    self.X = X\n",
    "                    self.test = test\n",
    " \n",
    "                else:\n",
    "                    self.X = X_enc\n",
    "                    self.test = test_enc\n",
    "                    \n",
    "                if 'NN' in model_name:\n",
    "                    for n_fold, (train_id, valid_id) in enumerate(self.folds.split(self.X, self.y, groups = X['year'])):\n",
    "\n",
    "                        X_train_cats = self.X.loc[train_id, cat_features]\n",
    "                        X_train_nums = self.X.loc[train_id, t.num_features]\n",
    "                        y_train = self.y.loc[train_id].values\n",
    "\n",
    "                        X_val_cats = self.X.loc[valid_id, cat_features]\n",
    "                        X_val_nums = self.X.loc[valid_id, t.num_features]\n",
    "                        y_val = self.y.loc[valid_id]\n",
    "\n",
    "                        X_test_cats = self.test[cat_features]\n",
    "                        X_test_nums = self.test[t.num_features]\n",
    "        \n",
    "                        oof_preds = pd.DataFrame(columns=[model_name], index=X_val_cats.index)\n",
    "                        test_preds = pd.DataFrame(columns=[model_name], index=test.index)\n",
    "                        print(f'Fold {n_fold+1}')\n",
    "                        \n",
    "                        model = build_model()                        \n",
    "                        keras.utils.set_random_seed(self.state)\n",
    "                        optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "                        model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "                        model.fit([X_train_cats,X_train_nums], y_train, \n",
    "                                  validation_data=([X_val_cats, X_val_nums], y_val),\n",
    "                                  epochs=20,\n",
    "                                  batch_size=1000,\n",
    "                                  callbacks=[keras.callbacks.ReduceLROnPlateau(patience=1),\n",
    "                                             keras.callbacks.EarlyStopping(patience=3)\n",
    "                                            ])\n",
    "                        \n",
    "                        y_pred_val = model.predict([X_val_cats, X_val_nums])\n",
    "                        test_pred = model.predict([X_test_cats, X_test_nums])\n",
    "                        \n",
    "                        score = mean_absolute_percentage_error(y_val, y_pred_val)\n",
    "                        print(score)\n",
    "                        self.scores.loc[f'{model_name}', f'Fold {n_fold+1}'] = score\n",
    "                        \n",
    "                        oof_preds[model_name] = y_pred_val\n",
    "                        test_preds[model_name] = test_pred\n",
    "\n",
    "                        self.OOF_preds = pd.concat([self.OOF_preds, oof_preds], axis = 0, ignore_index = False)\n",
    "                        TEST_preds = pd.concat([self.TEST_preds, test_preds], axis = 0, ignore_index = False)\n",
    "                        \n",
    "                else:\n",
    "                    for n_fold, (train_id, valid_id) in enumerate(self.folds.split(self.X, self.y, groups = self.X['year'])):\n",
    "                        X_train, y_train = self.X.iloc[train_id], self.y.iloc[train_id]\n",
    "                        X_val, y_val = self.X.iloc[valid_id], self.y.iloc[valid_id]\n",
    "\n",
    "                        oof_preds = pd.DataFrame(columns=[model_name], index=X_val.index)\n",
    "                        test_preds = pd.DataFrame(columns=[model_name], index=test.index)\n",
    "                        print(f'Fold {n_fold+1}')\n",
    "\n",
    "                        if \"XGB\" in model_name:\n",
    "                            model.fit(X_train, y_train, \n",
    "                                      eval_set = [(X_val, y_val)], \n",
    "                                      verbose = False\n",
    "                                     )\n",
    "\n",
    "                        elif \"CAT\" in model_name:\n",
    "                            model.fit(X_train, y_train, \n",
    "                                      eval_set = [(X_val, y_val)],\n",
    "                                      verbose=False\n",
    "                                      ) \n",
    "\n",
    "                        elif \"LGBM\" in model_name:\n",
    "                             model.fit(X_train, y_train, \n",
    "                                       eval_set = [(X_val, y_val)], \n",
    "                                       callbacks = [log_evaluation(0),\n",
    "                                                    early_stopping(self.early_stop, verbose = False)\n",
    "                                                   ])  \n",
    "\n",
    "                        else:\n",
    "                            model.fit(X_train, y_train)\n",
    "\n",
    "                        y_pred_val = model.predict(X_val)\n",
    "                        test_pred = model.predict(self.test)\n",
    "                       \n",
    "                        score = mean_absolute_percentage_error(np.expm1(y_val), np.expm1(y_pred_val))\n",
    "                        print(score)\n",
    "                        self.scores.loc[f'{model_name}', f'Fold {n_fold+1}'] = score\n",
    "\n",
    "                        oof_preds[model_name] = y_pred_val\n",
    "                        test_preds[model_name] = test_pred\n",
    "                        self.OOF_preds = pd.concat([self.OOF_preds, oof_preds], axis = 0, ignore_index = False)\n",
    "                        self.TEST_preds = pd.concat([self.TEST_preds, test_preds], axis = 0, ignore_index = False)\n",
    "\n",
    "                self.OOF_preds = self.OOF_preds.groupby(level=0).mean()\n",
    "                self.TEST_preds = self.TEST_preds.groupby(level=0).mean()\n",
    "\n",
    "                self.OOF_preds[f'{model_name}'].to_csv(f'{model_name}_oof.csv', index=False)\n",
    "                self.TEST_preds[f'{model_name}'].to_csv(f'{model_name}_test.csv', index=False)\n",
    "            \n",
    "            else:\n",
    "                self.OOF_preds[f'{model_name}'] = pd.read_csv(f'./input/sticker-models/{model_name}_oof.csv')\n",
    "                self.TEST_preds[f'{model_name}'] = pd.read_csv(f'./input/sticker-models/{model_name}_test.csv')\n",
    "                \n",
    "                for n_fold, (train_id, valid_id) in enumerate(self.folds.split(self.OOF_preds[f'{model_name}'], self.y, groups = X['year'])):\n",
    "                    y_pred_val, y_val = self.OOF_preds[f'{model_name}'].iloc[valid_id], self.y.iloc[valid_id]\n",
    "                    self.scores.loc[f'{model_name}', f'Fold {n_fold+1}'] = mean_absolute_percentage_error(np.expm1(y_val), np.expm1(y_pred_val))\n",
    "                    \n",
    "            self.scores.loc[f'{model_name}', 'Score'] = self.scores.loc[f'{model_name}'][1:].mean()\n",
    "        self.scores.loc['Ensemble'], self.OOF_preds[\"Ensemble\"], self.TEST_preds[\"Ensemble\"] = self.ensemble(self.OOF_preds, self.y, self.TEST_preds)\n",
    "        self.scores = self.scores.sort_values('Score')\n",
    "\n",
    "        self.result()\n",
    "\n",
    "        return self.TEST_preds\n",
    "    \n",
    "    def ensemble(self, X, y, test):\n",
    "        scores = []\n",
    "        oof_pred = np.zeros(X.shape[0])\n",
    "        test_pred = np.zeros(test.shape[0])\n",
    "        model = BayesianRidge()\n",
    "        kf = KFold(n_splits=self.n_splits, random_state=self.state, shuffle=True)\n",
    "        \n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            y_pred_probs = model.predict(X_val)\n",
    "            oof_pred[val_idx] = y_pred_probs\n",
    "            test_pred += model.predict(test) / self.n_splits\n",
    "            \n",
    "            score = mean_absolute_percentage_error(np.expm1(y_val), np.expm1(y_pred_probs))\n",
    "            scores.append(score)\n",
    "                   \n",
    "        return np.mean(scores), oof_pred, test_pred\n",
    "    \n",
    "    def result(self):\n",
    "               \n",
    "        plt.figure(figsize=(14, 6))\n",
    "        colors = ['#3cb371' if i != 'Ensemble' else 'r' for i in self.scores.Score.index]\n",
    "        hbars = plt.barh(self.scores.index, self.scores.Score, color=colors, height=0.8)\n",
    "        plt.bar_label(hbars, fmt='%.5f')\n",
    "        plt.ylabel('Models')\n",
    "        plt.xlabel('Score')              \n",
    "        plt.show()\n",
    "\n",
    "        y = np.expm1(self.y).sort_index()\n",
    "        self.OOF_preds['Ensemble'] = np.expm1(self.OOF_preds['Ensemble']).sort_index()\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        axes[0].scatter(y, self.OOF_preds['Ensemble'], alpha=0.5, s=15, edgecolors='#3cb371')\n",
    "        axes[0].plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2)\n",
    "        axes[0].set_xlabel('Actual')\n",
    "        axes[0].set_ylabel('Predicted')\n",
    "        axes[0].set_title('Actual vs. Predicted')\n",
    "\n",
    "        axes[1].scatter(self.OOF_preds['Ensemble'], y - self.OOF_preds['Ensemble'], alpha=0.5, s=15, edgecolors='#3cb371')\n",
    "        axes[1].axhline(y=0, color='black', linestyle='--', lw=2)\n",
    "        axes[1].set_xlabel('Predicted Values')\n",
    "        axes[1].set_ylabel('Residuals')\n",
    "        axes[1].set_title('Residual Plot')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0196",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T10:16:06.410238Z",
     "iopub.status.busy": "2025-01-17T10:16:06.409871Z",
     "iopub.status.idle": "2025-01-17T10:16:12.82009Z",
     "shell.execute_reply": "2025-01-17T10:16:12.818903Z",
     "shell.execute_reply.started": "2025-01-17T10:16:06.410213Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Model(X, X_enc, y, test, test_enc, models)\n",
    "TEST_preds = model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363b",
   "metadata": {},
   "source": [
    "# <p style=\"border-radius: 40px; color: white; font-weight: bold; font-size: 150%; text-align: center; background-color:#3cb371; padding: 5px 5px 5px 5px;\">Submission</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T10:16:14.2685Z",
     "iopub.status.busy": "2025-01-17T10:16:14.268142Z",
     "iopub.status.idle": "2025-01-17T10:16:14.415851Z",
     "shell.execute_reply": "2025-01-17T10:16:14.414569Z",
     "shell.execute_reply.started": "2025-01-17T10:16:14.268472Z"
    }
   },
   "outputs": [],
   "source": [
    "submission = Config.submission\n",
    "submission[Config.target] = np.round(np.clip(1.01*np.expm1(TEST_preds['Ensemble'].values), 0, 6000))\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "display(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T10:16:16.919133Z",
     "iopub.status.busy": "2025-01-17T10:16:16.918784Z",
     "iopub.status.idle": "2025-01-17T10:16:17.23657Z",
     "shell.execute_reply": "2025-01-17T10:16:17.235437Z",
     "shell.execute_reply.started": "2025-01-17T10:16:16.919105Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "submission[Config.target].hist(color='#3cb371', bins=50)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10652996,
     "sourceId": 85723,
     "sourceType": "competition"
    },
    {
     "databundleVersionId": 10823830,
     "datasetId": 6413778,
     "sourceId": 10495653,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
