{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fbb",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 1.2em;\">\n",
    "    I used some techniques borrowed from these excellent notebooks, and they do a great job.\n",
    "</p>\n",
    "\n",
    "<p style=\"font-size: 1.1em;\">\n",
    "    @Chris Deotte <a href=\"https://www.kaggle.com/code/cdeotte/transformer-starter-lb-0-052\">Transformer Starter notebook</a>\n",
    "</p>\n",
    "\n",
    "<p style=\"font-size: 1.1em;\">\n",
    "    @Konstantin Dmitriev <a href=\"https://www.kaggle.com/code/kdmitrie/pg501-model-2-decomposition-country-doy-factor#4.-Blending-with-other-solutions\">this notebook PG501: Model 2. Decomposition + country-doy factor</a> \n",
    "</p>\n",
    "\n",
    "<p style=\"font-size: 1.1em;\">\n",
    "    @Spiritmilk <a href=\"https://www.kaggle.com/code/act18l/hill-climbing-for-ensembling-baseline\">Hill Climbing for ensembling baseline notebook</a>\n",
    "</p>\n",
    "\n",
    "<p style=\"font-size: 1.2em;\">\n",
    "    Thank you so much for everyone's contribution.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3e",
   "metadata": {},
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T02:55:18.254627Z",
     "iopub.status.busy": "2025-01-31T02:55:18.25413Z",
     "iopub.status.idle": "2025-01-31T02:55:19.940998Z",
     "shell.execute_reply": "2025-01-31T02:55:19.939849Z",
     "shell.execute_reply.started": "2025-01-31T02:55:18.254528Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import Ridge,Lasso\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from lightgbm import log_evaluation, early_stopping, LGBMRegressor\n",
    "\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab04",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3400",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T02:55:19.943811Z",
     "iopub.status.busy": "2025-01-31T02:55:19.943462Z",
     "iopub.status.idle": "2025-01-31T02:55:20.565825Z",
     "shell.execute_reply": "2025-01-31T02:55:20.564609Z",
     "shell.execute_reply.started": "2025-01-31T02:55:19.943778Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./input/playground-series-s5e1/train.csv\", parse_dates=[\"date\"])\n",
    "original_train_df = train_df.copy()\n",
    "test_df = pd.read_csv(\"./input/playground-series-s5e1/test.csv\", parse_dates=[\"date\"])\n",
    "sub = pd.read_csv(\"./input/playground-series-s5e1/sample_submission.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22d",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2796",
   "metadata": {},
   "source": [
    "## Import GDP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T02:55:20.567918Z",
     "iopub.status.busy": "2025-01-31T02:55:20.567363Z",
     "iopub.status.idle": "2025-01-31T02:55:20.640586Z",
     "shell.execute_reply": "2025-01-31T02:55:20.639517Z",
     "shell.execute_reply.started": "2025-01-31T02:55:20.567869Z"
    }
   },
   "outputs": [],
   "source": [
    "gdp_per_capita_df = pd.read_csv(\"./input/world-gdpgdp-gdp-per-capita-and-annual-growths/gdp_per_capita.csv\")\n",
    "\n",
    "years =  [\"2010\", \"2011\", \"2012\", \"2013\", \"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020\"]\n",
    "gdp_per_capita_filtered_df = gdp_per_capita_df.loc[gdp_per_capita_df[\"Country Name\"].isin(train_df[\"country\"].unique()), [\"Country Name\"] + years].set_index(\"Country Name\")\n",
    "for year in years:\n",
    "    gdp_per_capita_filtered_df[f\"{year}_ratio\"] = gdp_per_capita_filtered_df[year] / gdp_per_capita_filtered_df.sum()[year]\n",
    "gdp_per_capita_filtered_ratios_df = gdp_per_capita_filtered_df[[i+\"_ratio\" for i in years]]\n",
    "gdp_per_capita_filtered_ratios_df.columns = [int(i) for i in years]\n",
    "gdp_per_capita_filtered_ratios_df = gdp_per_capita_filtered_ratios_df.unstack().reset_index().rename(columns = {\"level_0\": \"year\", 0: \"ratio\", \"Country Name\": \"country\"})\n",
    "gdp_per_capita_filtered_ratios_df['year'] = pd.to_datetime(gdp_per_capita_filtered_ratios_df['year'], format='%Y')\n",
    "\n",
    "# For plotting purposes\n",
    "gdp_per_capita_filtered_ratios_df_2 = gdp_per_capita_filtered_ratios_df.copy()\n",
    "gdp_per_capita_filtered_ratios_df_2[\"year\"] = pd.to_datetime(gdp_per_capita_filtered_ratios_df_2['year'].astype(str)) + pd.offsets.YearEnd(1)\n",
    "gdp_per_capita_filtered_ratios_df = pd.concat([gdp_per_capita_filtered_ratios_df, gdp_per_capita_filtered_ratios_df_2]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e961",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T02:55:20.644339Z",
     "iopub.status.busy": "2025-01-31T02:55:20.643447Z",
     "iopub.status.idle": "2025-01-31T02:55:20.650321Z",
     "shell.execute_reply": "2025-01-31T02:55:20.649455Z",
     "shell.execute_reply.started": "2025-01-31T02:55:20.644293Z"
    }
   },
   "outputs": [],
   "source": [
    "gdp_per_capita_filtered_ratios_df_2[\"year\"] = gdp_per_capita_filtered_ratios_df_2[\"year\"].dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a78",
   "metadata": {},
   "source": [
    "## Process missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T02:55:20.653505Z",
     "iopub.status.busy": "2025-01-31T02:55:20.653056Z",
     "iopub.status.idle": "2025-01-31T02:55:27.703345Z",
     "shell.execute_reply": "2025-01-31T02:55:27.702206Z",
     "shell.execute_reply.started": "2025-01-31T02:55:20.653464Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df_imputed = train_df.copy()\n",
    "missing_value_ids = train_df.loc[train_df[\"num_sold\"].isna(), \"id\"].values\n",
    "print(f\"Missing values remaining: {train_df_imputed['num_sold'].isna().sum()}\")\n",
    "\n",
    "train_df_imputed[\"year\"] = train_df_imputed[\"date\"].dt.year\n",
    "for year in train_df_imputed[\"year\"].unique():\n",
    "    # Impute Time Series 1 (Canada, Discount Stickers, Holographic Goose)\n",
    "    target_ratio = gdp_per_capita_filtered_ratios_df_2.loc[(gdp_per_capita_filtered_ratios_df_2[\"year\"] == year) & (gdp_per_capita_filtered_ratios_df_2[\"country\"] == \"Norway\"), \"ratio\"].values[0] # Using Norway as should have the best precision\n",
    "    current_raito = gdp_per_capita_filtered_ratios_df_2.loc[(gdp_per_capita_filtered_ratios_df_2[\"year\"] == year) & (gdp_per_capita_filtered_ratios_df_2[\"country\"] == \"Canada\"), \"ratio\"].values[0]\n",
    "    ratio_can = current_raito / target_ratio\n",
    "    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Canada\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year), \"num_sold\"] * ratio_can).values\n",
    "    \n",
    "    # Impute Time Series 2 (Only Missing Values)\n",
    "    current_ts =  train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Canada\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year)]\n",
    "    missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n",
    "    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Canada\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] * ratio_can).values\n",
    "\n",
    "    # Impute Time Series 3 (Only Missing Values)\n",
    "    current_ts =  train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Canada\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year)]\n",
    "    missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n",
    "    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Canada\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] * ratio_can).values\n",
    "    \n",
    "    # Impute Time Series 4 (Kenya, Discount Stickers, Holographic Goose)\n",
    "    current_raito = gdp_per_capita_filtered_ratios_df_2.loc[(gdp_per_capita_filtered_ratios_df_2[\"year\"] == year) & (gdp_per_capita_filtered_ratios_df_2[\"country\"] == \"Kenya\"), \"ratio\"].values[0]\n",
    "    ratio_ken = current_raito / target_ratio\n",
    "    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Holographic Goose\")& (train_df_imputed[\"year\"] == year), \"num_sold\"] * ratio_ken).values\n",
    "\n",
    "    # Impute Time Series 5 (Only Missing Values)\n",
    "    current_ts = train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year)]\n",
    "    missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n",
    "    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] * ratio_ken).values\n",
    "\n",
    "    # Impute Time Series 6 (Only Missing Values)\n",
    "    current_ts = train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year)]\n",
    "    missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n",
    "    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] * ratio_ken).values\n",
    "\n",
    "    # Impute Time Series 7 (Only Missing Values)\n",
    "    current_ts = train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Kerneler\") & (train_df_imputed[\"year\"] == year)]\n",
    "    missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n",
    "    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Kerneler\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Kerneler\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] * ratio_ken).values\n",
    "    \n",
    "print(f\"Missing values remaining: {train_df_imputed['num_sold'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T02:55:27.705433Z",
     "iopub.status.busy": "2025-01-31T02:55:27.705082Z",
     "iopub.status.idle": "2025-01-31T02:55:27.729238Z",
     "shell.execute_reply": "2025-01-31T02:55:27.728062Z",
     "shell.execute_reply.started": "2025-01-31T02:55:27.705376Z"
    }
   },
   "outputs": [],
   "source": [
    "missing_rows = train_df_imputed.loc[train_df_imputed[\"num_sold\"].isna()]\n",
    "display(missing_rows)\n",
    "train_df_imputed.loc[train_df_imputed[\"id\"] == 23719, \"num_sold\"] = 4\n",
    "train_df_imputed.loc[train_df_imputed[\"id\"] == 207003, \"num_sold\"] = 195\n",
    "\n",
    "print(f\"Missing values remaining: {train_df_imputed['num_sold'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d781",
   "metadata": {},
   "source": [
    "## store weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c140",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T02:55:27.731218Z",
     "iopub.status.busy": "2025-01-31T02:55:27.730807Z",
     "iopub.status.idle": "2025-01-31T02:55:27.767537Z",
     "shell.execute_reply": "2025-01-31T02:55:27.76638Z",
     "shell.execute_reply.started": "2025-01-31T02:55:27.731174Z"
    }
   },
   "outputs": [],
   "source": [
    "store_weights = train_df_imputed.groupby(\"store\")[\"num_sold\"].sum()/train_df_imputed[\"num_sold\"].sum()\n",
    "store_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb06",
   "metadata": {},
   "source": [
    "## The proportion of product in daily sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T02:55:27.769201Z",
     "iopub.status.busy": "2025-01-31T02:55:27.768864Z",
     "iopub.status.idle": "2025-01-31T02:55:28.494695Z",
     "shell.execute_reply": "2025-01-31T02:55:28.493489Z",
     "shell.execute_reply.started": "2025-01-31T02:55:27.769173Z"
    }
   },
   "outputs": [],
   "source": [
    "product_df = train_df_imputed.groupby([\"date\",\"product\"])[\"num_sold\"].sum().reset_index()\n",
    "product_ratio_df = product_df.pivot(index=\"date\", columns=\"product\", values=\"num_sold\")\n",
    "product_ratio_df = product_ratio_df.apply(lambda x: x/x.sum(),axis=1)\n",
    "product_ratio_df = product_ratio_df.stack().rename(\"ratios\").reset_index()\n",
    "product_ratio_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d446",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T02:55:28.496353Z",
     "iopub.status.busy": "2025-01-31T02:55:28.496052Z",
     "iopub.status.idle": "2025-01-31T02:55:28.510305Z",
     "shell.execute_reply": "2025-01-31T02:55:28.509144Z",
     "shell.execute_reply.started": "2025-01-31T02:55:28.496325Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df_imputed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec7",
   "metadata": {},
   "source": [
    "## Group the dates and calculate the total sales num_sold per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T02:55:28.515309Z",
     "iopub.status.busy": "2025-01-31T02:55:28.51495Z",
     "iopub.status.idle": "2025-01-31T02:55:28.553886Z",
     "shell.execute_reply": "2025-01-31T02:55:28.552904Z",
     "shell.execute_reply.started": "2025-01-31T02:55:28.515272Z"
    }
   },
   "outputs": [],
   "source": [
    "original_train_df_imputed = train_df_imputed.copy()\n",
    "train_df_imputed = train_df_imputed.groupby([\"date\"])[\"num_sold\"].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T02:55:28.555526Z",
     "iopub.status.busy": "2025-01-31T02:55:28.55511Z",
     "iopub.status.idle": "2025-01-31T02:55:28.566833Z",
     "shell.execute_reply": "2025-01-31T02:55:28.565473Z",
     "shell.execute_reply.started": "2025-01-31T02:55:28.555379Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df_imputed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af4",
   "metadata": {},
   "source": [
    "## The weekly effect was normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T02:55:28.568845Z",
     "iopub.status.busy": "2025-01-31T02:55:28.568511Z",
     "iopub.status.idle": "2025-01-31T02:55:28.594536Z",
     "shell.execute_reply": "2025-01-31T02:55:28.593443Z",
     "shell.execute_reply.started": "2025-01-31T02:55:28.568816Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df_imputed[\"day_of_week\"] = train_df_imputed[\"date\"].dt.dayofweek\n",
    "day_of_week_ratio = (train_df_imputed.groupby(\"day_of_week\")[\"num_sold\"].mean() / train_df_imputed.groupby(\"day_of_week\")[\"num_sold\"].mean().mean()).rename(\"day_of_week_ratios\")\n",
    "display(day_of_week_ratio)\n",
    "train_df_imputed = pd.merge(train_df_imputed, day_of_week_ratio, how=\"left\", on=\"day_of_week\")\n",
    "train_df_imputed[\"num_sold\"] = train_df_imputed[\"num_sold\"] / train_df_imputed[\"day_of_week_ratios\"]\n",
    "train_df_imputed = train_df_imputed.drop(\"day_of_week_ratios\",axis=1)# we don't need it in training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f0",
   "metadata": {},
   "source": [
    "## Fetch date column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T02:55:28.596594Z",
     "iopub.status.busy": "2025-01-31T02:55:28.596243Z",
     "iopub.status.idle": "2025-01-31T02:55:28.60747Z",
     "shell.execute_reply": "2025-01-31T02:55:28.606475Z",
     "shell.execute_reply.started": "2025-01-31T02:55:28.596565Z"
    }
   },
   "outputs": [],
   "source": [
    "test_total_sales_df = test_df.groupby([\"date\"])[\"id\"].first().reset_index().drop(columns=\"id\")\n",
    "test_total_sales_dates = test_total_sales_df[[\"date\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea9",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T02:55:28.60936Z",
     "iopub.status.busy": "2025-01-31T02:55:28.609022Z",
     "iopub.status.idle": "2025-01-31T02:55:28.623695Z",
     "shell.execute_reply": "2025-01-31T02:55:28.622369Z",
     "shell.execute_reply.started": "2025-01-31T02:55:28.609332Z"
    }
   },
   "outputs": [],
   "source": [
    "def feature_engineer(df):\n",
    "    new_df = df.copy()\n",
    "    new_df[\"month\"] = df[\"date\"].dt.month\n",
    "    new_df[\"month_sin\"] = np.sin(new_df['month'] * (2 * np.pi / 12))\n",
    "    new_df[\"month_cos\"] = np.cos(new_df['month'] * (2 * np.pi / 12))\n",
    "    new_df[\"day_of_week\"] = df[\"date\"].dt.dayofweek\n",
    "    new_df[\"day_of_week\"] = new_df[\"day_of_week\"].apply(lambda x: 0 if x<=3 else(1 if x==4 else (2 if x==5 else (3))))    \n",
    "    new_df[\"day_of_year\"] = df['date'].apply(\n",
    "        lambda x: x.timetuple().tm_yday if not (x.is_leap_year and x.month > 2) else x.timetuple().tm_yday - 1\n",
    "    )\n",
    "\n",
    "    new_df['day_sin4'] = np.sin(new_df['day_of_year'] * (8 * np.pi /  365.0))\n",
    "    new_df['day_cos4'] = np.cos(new_df['day_of_year'] * (8 * np.pi /  365.0))\n",
    "    new_df['day_sin3'] = np.sin(new_df['day_of_year'] * (6 * np.pi /  365.0))\n",
    "    new_df['day_cos3'] = np.cos(new_df['day_of_year'] * (6 * np.pi /  365.0))\n",
    "    new_df['day_sin2'] = np.sin(new_df['day_of_year'] * (4 * np.pi /  365.0))\n",
    "    new_df['day_cos2'] = np.cos(new_df['day_of_year'] * (4 * np.pi /  365.0))\n",
    "    new_df['day_sin'] = np.sin(new_df['day_of_year'] * (2 * np.pi /  365.0))\n",
    "    new_df['day_cos'] = np.cos(new_df['day_of_year'] * (2 * np.pi /  365.0)) \n",
    "    new_df['day_sin_0.5'] = np.sin(new_df['day_of_year'] * (1 * np.pi /  365.0))\n",
    "    new_df['day_cos_0.5'] = np.cos(new_df['day_of_year'] * (1 * np.pi /  365.0))    \n",
    "    new_df[\"important_dates\"] = new_df[\"day_of_year\"].apply(lambda x: x if x in [1,2,3,4,5,6,7,8,9,10,99, 100, 101, 125,126,355,256,357,358,359,360,361,362,363,364,365] else 0)\n",
    "    \n",
    "    new_df = new_df.drop(columns=[\"date\",\"month\",\"day_of_year\"])\n",
    "    new_df = pd.get_dummies(new_df, columns = [\"important_dates\",\"day_of_week\"], drop_first=True)\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3588",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T02:55:28.62587Z",
     "iopub.status.busy": "2025-01-31T02:55:28.62534Z",
     "iopub.status.idle": "2025-01-31T02:55:28.694808Z",
     "shell.execute_reply": "2025-01-31T02:55:28.693866Z",
     "shell.execute_reply.started": "2025-01-31T02:55:28.625828Z"
    }
   },
   "outputs": [],
   "source": [
    "train_total_sales_df = feature_engineer(train_df_imputed)\n",
    "test_total_sales_df = feature_engineer(test_total_sales_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328e",
   "metadata": {},
   "source": [
    "## import holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T02:55:28.696733Z",
     "iopub.status.busy": "2025-01-31T02:55:28.696255Z",
     "iopub.status.idle": "2025-01-31T02:55:29.11598Z",
     "shell.execute_reply": "2025-01-31T02:55:29.114898Z",
     "shell.execute_reply.started": "2025-01-31T02:55:28.696692Z"
    }
   },
   "outputs": [],
   "source": [
    "import holidays\n",
    "train_df_tmp = train_df.copy()\n",
    "test_df_tmp = test_df.copy()\n",
    "alpha2 = dict(zip(np.sort(train_df.country.unique()), ['CA', 'FI', 'IT', 'KE', 'NO', 'SG']))\n",
    "h = {c: holidays.country_holidays(a, years=range(2010, 2020)) for c, a in alpha2.items()}\n",
    "train_df_tmp['is_holiday'] = 0\n",
    "test_df_tmp['is_holiday'] = 0\n",
    "for c in alpha2:\n",
    "    train_df_tmp.loc[train_df_tmp.country==c, 'is_holiday'] = train_df_tmp.date.isin(h[c]).astype(int)\n",
    "    test_df_tmp.loc[test_df_tmp.country==c, 'is_holiday'] = test_df_tmp.date.isin(h[c]).astype(int)\n",
    "\n",
    "\n",
    "train_total_sales_df['is_holiday'] = (train_df_tmp.groupby([\"date\"])[\"is_holiday\"].sum().reset_index())[\"is_holiday\"]\n",
    "test_total_sales_df['is_holiday'] = (test_df_tmp.groupby([\"date\"])[\"is_holiday\"].sum().reset_index())[\"is_holiday\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T02:55:29.117846Z",
     "iopub.status.busy": "2025-01-31T02:55:29.117415Z",
     "iopub.status.idle": "2025-01-31T02:55:29.125161Z",
     "shell.execute_reply": "2025-01-31T02:55:29.123946Z",
     "shell.execute_reply.started": "2025-01-31T02:55:29.117804Z"
    }
   },
   "outputs": [],
   "source": [
    "y = train_total_sales_df[\"num_sold\"]\n",
    "X = train_total_sales_df.drop(columns=\"num_sold\")\n",
    "X_test = test_total_sales_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f975",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T02:55:29.12719Z",
     "iopub.status.busy": "2025-01-31T02:55:29.12677Z",
     "iopub.status.idle": "2025-01-31T02:55:29.155382Z",
     "shell.execute_reply": "2025-01-31T02:55:29.154007Z",
     "shell.execute_reply.started": "2025-01-31T02:55:29.127152Z"
    }
   },
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f585",
   "metadata": {},
   "source": [
    "## hill climbing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T02:55:29.157633Z",
     "iopub.status.busy": "2025-01-31T02:55:29.157111Z",
     "iopub.status.idle": "2025-01-31T02:55:29.169286Z",
     "shell.execute_reply": "2025-01-31T02:55:29.168219Z",
     "shell.execute_reply.started": "2025-01-31T02:55:29.157582Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LinearModel1:\n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.nonlinear(X)\n",
    "\n",
    "    def nonlinear(self, X):\n",
    "        y = (\n",
    "            X.important_dates_1 * 14514.153142903406 +\n",
    "            X.important_dates_6 +\n",
    "            X.important_dates_99 -\n",
    "            X.important_dates_359 +\n",
    "            X.important_dates_363 * 23245.947607750375 +\n",
    "            64795.970182750174 -\n",
    "            ((X.important_dates_362 + np.sqrt(X.important_dates_364 + X.important_dates_365)) *\n",
    "             ((X.important_dates_2 - X.important_dates_10) * (X.important_dates_101 + X.month_sin) +\n",
    "              X.important_dates_362 - 19142.637752625015))\n",
    "        )\n",
    "        return y\n",
    "\n",
    "class LinearModel2:\n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.nonlinear(X)\n",
    "\n",
    "    def nonlinear(self, X):\n",
    "        y = X.month_sin*210.148+\\\n",
    "            (X.important_dates_1+X.important_dates_362+X.important_dates_363+X.important_dates_364+X.important_dates_365)*19472.7071+\\\n",
    "            X.important_dates_4*X.month_sin+\\\n",
    "            65141.2073\n",
    "            \n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c5",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006d",
   "metadata": {},
   "source": [
    "## Multi-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T02:55:29.171873Z",
     "iopub.status.busy": "2025-01-31T02:55:29.171415Z",
     "iopub.status.idle": "2025-01-31T02:55:29.184991Z",
     "shell.execute_reply": "2025-01-31T02:55:29.183857Z",
     "shell.execute_reply.started": "2025-01-31T02:55:29.17183Z"
    }
   },
   "outputs": [],
   "source": [
    "regressors = {\n",
    "    \"Ridge\" : Ridge(tol=1e-2, max_iter=1000000, random_state=0),\n",
    "    \"Lasso\" : Lasso(tol=1e-2, max_iter=1000000, random_state=0),\n",
    "    #\"nonlinear1\" : LinearModel1(),\n",
    "    #\"nonlinear2\" : LinearModel2(),\n",
    "    \"LGBM1\": LGBMRegressor(**{'objective': 'regression_l2',\n",
    "                               'metric': 'mape', \n",
    "                               'max_depth': 7,\n",
    "                               'num_leaves': 123, \n",
    "                               'min_child_samples': 21,\n",
    "                               'min_child_weight': 24,\n",
    "                               'colsample_bytree': 0.3641261996760593, \n",
    "                               'reg_alpha': 0.03632800166349373, \n",
    "                               'reg_lambda': 0.5287861861476272,\n",
    "                               'random_state': 42,\n",
    "                               'early_stopping_round':200,\n",
    "                               'verbose': -1,\n",
    "                               'boosting_type': 'gbdt',\n",
    "                               'n_estimators': 3000,\n",
    "                               'learning_rate': 0.01,\n",
    "                               }),\n",
    "    \"LGBM2\": LGBMRegressor(**{'objective': 'regression_l2',\n",
    "                               'metric': 'mape',\n",
    "                               'max_depth': 6,\n",
    "                               'num_leaves': 502,\n",
    "                               'min_child_samples': 23,\n",
    "                               'min_child_weight': 18, \n",
    "                               'colsample_bytree': 0.4714820876493163, \n",
    "                               'reg_alpha': 0.054972003081022576, \n",
    "                               'reg_lambda': 0.5774608955362155,\n",
    "                               'random_state': 42,\n",
    "                               'early_stopping_round': 200,\n",
    "                               'verbose': -1,\n",
    "                               'boosting_type': 'goss',\n",
    "                               'n_estimators': 3000,\n",
    "                               'learning_rate': 0.01,\n",
    "                              }),\n",
    "    \"LGBM3\": LGBMRegressor(**{'objective': 'regression_l2', \n",
    "                               'metric': 'mape',\n",
    "                               'max_depth': 14,\n",
    "                               'num_leaves': 279,\n",
    "                               'min_child_samples': 7,\n",
    "                               'min_child_weight': 24, \n",
    "                               'colsample_bytree': 0.43218993309765835,\n",
    "                               'reg_alpha': 0.42757392987472964,\n",
    "                               'reg_lambda': 0.9039762787446107,\n",
    "                               'random_state': 42,\n",
    "                               'early_stopping_round': 200,\n",
    "                               'verbose': -1,\n",
    "                               'boosting_type': 'goss',\n",
    "                               'n_estimators': 3000,\n",
    "                               'learning_rate': 0.01,\n",
    "                               }),\n",
    "   \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7877",
   "metadata": {},
   "source": [
    "## Cross-validation trains, validates, and generates predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T02:55:29.186994Z",
     "iopub.status.busy": "2025-01-31T02:55:29.186558Z",
     "iopub.status.idle": "2025-01-31T02:55:40.655834Z",
     "shell.execute_reply": "2025-01-31T02:55:40.654995Z",
     "shell.execute_reply.started": "2025-01-31T02:55:29.186952Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.model_selection import KFold\n",
    "n_folds = 5\n",
    "\n",
    "for key, reg in regressors.items():\n",
    "    test_preds = np.zeros(len(X_test))\n",
    "    oof_full = y.copy()\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    cv = KFold(n_splits=n_folds, shuffle=False)\n",
    "    score=0\n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n",
    "        X_train, X_valid = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_valid = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        start = time.time()\n",
    "        if \"LGBM\" in key:\n",
    "            reg.fit(X_train, y_train,\n",
    "                   eval_set = [(X_valid, y_valid)], \n",
    "                                       callbacks = [log_evaluation(0),\n",
    "                                                    early_stopping(200, verbose = False)\n",
    "                                                   ])\n",
    "            \n",
    "        else:\n",
    "            reg.fit(X_train, y_train)\n",
    "        oof_preds = reg.predict(X_valid)\n",
    "        score += mean_absolute_percentage_error(y_valid, oof_preds)/n_folds\n",
    "        oof_full[val_idx] = oof_preds\n",
    "        \n",
    "        test_preds += reg.predict(X_test)/n_folds\n",
    "    \n",
    "    # Stop timer\n",
    "    stop = time.time()\n",
    "    \n",
    "    print('Model:', key)\n",
    "    print('Average validation MAPE:', score)\n",
    "    print('Training time (mins):', np.round((stop - start)/60,2))\n",
    "    print('')\n",
    "    \n",
    "\n",
    "    oof_full.to_csv(f\"{key}_oof_preds.csv\", index=False)\n",
    "    ss = pd.DataFrame()\n",
    "    ss[\"num_sold\"] = test_preds\n",
    "    ss.to_csv(f\"{key}_test_preds.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f564",
   "metadata": {},
   "source": [
    "## Combine oof_preds and test_preds into a single DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T02:55:40.657107Z",
     "iopub.status.busy": "2025-01-31T02:55:40.656775Z",
     "iopub.status.idle": "2025-01-31T02:55:40.700198Z",
     "shell.execute_reply": "2025-01-31T02:55:40.6991Z",
     "shell.execute_reply.started": "2025-01-31T02:55:40.657073Z"
    }
   },
   "outputs": [],
   "source": [
    "oof_df = pd.DataFrame(index=np.arange(len(y)))\n",
    "for i in regressors.keys():\n",
    "    df = pd.read_csv(f\"/kaggle/working/{i}_oof_preds.csv\")\n",
    "    df.rename(columns={\"num_sold\": i}, inplace=True)\n",
    "    oof_df = pd.concat([oof_df,df], axis=1)\n",
    "    \n",
    "# Join test preds\n",
    "test_preds = pd.DataFrame(index=np.arange(len(X_test)))\n",
    "for i in regressors.keys():\n",
    "    df = pd.read_csv(f\"/kaggle/working/{i}_test_preds.csv\")\n",
    "    df.rename(columns={\"num_sold\": i}, inplace=True)\n",
    "    test_preds = pd.concat([test_preds,df], axis=1)\n",
    "    \n",
    "oof_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c903",
   "metadata": {},
   "source": [
    "## Calculation model score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T02:55:40.70212Z",
     "iopub.status.busy": "2025-01-31T02:55:40.701695Z",
     "iopub.status.idle": "2025-01-31T02:55:40.719682Z",
     "shell.execute_reply": "2025-01-31T02:55:40.718344Z",
     "shell.execute_reply.started": "2025-01-31T02:55:40.702078Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate oof preds\n",
    "scores = {}\n",
    "for col in oof_df.columns:\n",
    "    scores[col] = mean_absolute_percentage_error(y, oof_df[col])\n",
    "\n",
    "# Sort scores\n",
    "    scores = {k: v for k, v in sorted(scores.items(), key=lambda item: item[1], reverse=False)}\n",
    "\n",
    "# Sort oof_df and test_preds\n",
    "oof_df = oof_df[list(scores.keys())]\n",
    "test_preds = test_preds[list(scores.keys())]\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f55",
   "metadata": {},
   "source": [
    "# Optimization treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b887",
   "metadata": {},
   "source": [
    "## Model weight optimization based on Hill Climbing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2417",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T02:55:40.721795Z",
     "iopub.status.busy": "2025-01-31T02:55:40.721331Z",
     "iopub.status.idle": "2025-01-31T02:55:56.532233Z",
     "shell.execute_reply": "2025-01-31T02:55:56.531097Z",
     "shell.execute_reply.started": "2025-01-31T02:55:40.721765Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialise\n",
    "STOP = False\n",
    "current_best_ensemble = oof_df.iloc[:,0]\n",
    "current_best_test_preds = test_preds.iloc[:,0]\n",
    "MODELS = oof_df.iloc[:,1:]\n",
    "weight_range = np.arange(-1,1,0.001)  \n",
    "history = [mean_absolute_percentage_error(y, current_best_ensemble)]\n",
    "i=0\n",
    "\n",
    "# Hill climbing\n",
    "while not STOP:\n",
    "    i+=1\n",
    "    potential_new_best_cv_score = mean_absolute_percentage_error(y, current_best_ensemble)\n",
    "    k_best, wgt_best = None, None\n",
    "    for k in MODELS:\n",
    "        for wgt in weight_range:\n",
    "            potential_ensemble = (1-wgt) * current_best_ensemble + wgt * MODELS[k]\n",
    "            cv_score = mean_absolute_percentage_error(y, potential_ensemble)\n",
    "            if cv_score < potential_new_best_cv_score:\n",
    "                potential_new_best_cv_score = cv_score\n",
    "                k_best, wgt_best = k, wgt\n",
    "            \n",
    "    if k_best is not None:\n",
    "        current_best_ensemble = (1-wgt_best) * current_best_ensemble + wgt_best * MODELS[k_best]\n",
    "        current_best_test_preds = (1-wgt_best) * current_best_test_preds + wgt_best * test_preds[k_best]\n",
    "        MODELS.drop(k_best, axis=1, inplace=True)\n",
    "        if MODELS.shape[1]==0:\n",
    "            STOP = True\n",
    "        print(f'Iteration: {i}, Model added: {k_best}, Best weight: {wgt_best:}, Best MAPE: {potential_new_best_cv_score:.5f}')\n",
    "        history.append(potential_new_best_cv_score)\n",
    "    else:\n",
    "        STOP = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9421",
   "metadata": {},
   "source": [
    "## Weighted average of predicted results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T02:55:56.534145Z",
     "iopub.status.busy": "2025-01-31T02:55:56.53384Z",
     "iopub.status.idle": "2025-01-31T02:55:56.543603Z",
     "shell.execute_reply": "2025-01-31T02:55:56.54254Z",
     "shell.execute_reply.started": "2025-01-31T02:55:56.534117Z"
    }
   },
   "outputs": [],
   "source": [
    "test_total_sales_dates[\"num_sold\"] = current_best_test_preds*0.5+LinearModel2().predict(X_test)*0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49f",
   "metadata": {},
   "source": [
    "## The date offset method generates the predicted product proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d768",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T02:55:56.545336Z",
     "iopub.status.busy": "2025-01-31T02:55:56.544973Z",
     "iopub.status.idle": "2025-01-31T02:55:56.569069Z",
     "shell.execute_reply": "2025-01-31T02:55:56.567895Z",
     "shell.execute_reply.started": "2025-01-31T02:55:56.545306Z"
    }
   },
   "outputs": [],
   "source": [
    "product_ratio_2017_df = product_ratio_df.loc[product_ratio_df[\"date\"].dt.year == 2015].copy()\n",
    "product_ratio_2018_df = product_ratio_df.loc[product_ratio_df[\"date\"].dt.year == 2016].copy()\n",
    "product_ratio_2019_df = product_ratio_df.loc[product_ratio_df[\"date\"].dt.year == 2015].copy()\n",
    "\n",
    "product_ratio_2017_df[\"date\"] = product_ratio_2017_df[\"date\"] + pd.DateOffset(years=2)\n",
    "product_ratio_2018_df[\"date\"] = product_ratio_2018_df[\"date\"] + pd.DateOffset(years=2)\n",
    "product_ratio_2019_df[\"date\"] =  product_ratio_2019_df[\"date\"] + pd.DateOffset(years=4)\n",
    "\n",
    "forecasted_ratios_df = pd.concat([product_ratio_2017_df, product_ratio_2018_df, product_ratio_2019_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9f",
   "metadata": {},
   "source": [
    "## The data of the test set is decomposed and predicted based on multiple weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T02:55:56.570722Z",
     "iopub.status.busy": "2025-01-31T02:55:56.570309Z",
     "iopub.status.idle": "2025-01-31T02:55:56.729607Z",
     "shell.execute_reply": "2025-01-31T02:55:56.728443Z",
     "shell.execute_reply.started": "2025-01-31T02:55:56.570684Z"
    }
   },
   "outputs": [],
   "source": [
    "store_weights_df = store_weights.reset_index()\n",
    "test_sub_df = pd.merge(test_df, test_total_sales_dates, how=\"left\", on=\"date\")\n",
    "test_sub_df = test_sub_df.rename(columns = {\"num_sold\":\"day_num_sold\"})\n",
    "# Adding in the product ratios\n",
    "test_sub_df = pd.merge(test_sub_df, store_weights_df, how=\"left\", on=\"store\")\n",
    "test_sub_df = test_sub_df.rename(columns = {\"num_sold\":\"store_ratio\"})\n",
    "# Adding in the country ratios\n",
    "test_sub_df[\"year\"] = test_sub_df[\"date\"].dt.year\n",
    "test_sub_df = pd.merge(test_sub_df, gdp_per_capita_filtered_ratios_df_2, how=\"left\", on=[\"year\", \"country\"])\n",
    "test_sub_df = test_sub_df.rename(columns = {\"ratio\":\"country_ratio\"})\n",
    "# Adding in the product ratio\n",
    "test_sub_df = pd.merge(test_sub_df, forecasted_ratios_df, how=\"left\", on=[\"date\", \"product\"])\n",
    "test_sub_df = test_sub_df.rename(columns = {\"ratios\":\"product_ratio\"})\n",
    "\n",
    "# Adding in the week ratio\n",
    "test_sub_df[\"day_of_week\"] = test_sub_df[\"date\"].dt.dayofweek\n",
    "test_sub_df = pd.merge(test_sub_df, day_of_week_ratio.reset_index(), how=\"left\", on=\"day_of_week\")\n",
    "\n",
    "\n",
    "# Disaggregating the forecast\n",
    "test_sub_df.loc[test_sub_df['country'] == 'Kenya', 'country_ratio'] -= 0.0007/2\n",
    "\n",
    "test_sub_df[\"num_sold\"] = test_sub_df[\"day_num_sold\"] *test_sub_df[\"day_of_week_ratios\"]* test_sub_df[\"store_ratio\"] * test_sub_df[\"country_ratio\"] * test_sub_df[\"product_ratio\"]\n",
    "#test_sub_df[\"num_sold\"] = test_sub_df[\"num_sold\"].round()\n",
    "display(test_sub_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1724",
   "metadata": {},
   "source": [
    "# Blending with other solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T02:55:56.735117Z",
     "iopub.status.busy": "2025-01-31T02:55:56.734802Z",
     "iopub.status.idle": "2025-01-31T02:55:57.235457Z",
     "shell.execute_reply": "2025-01-31T02:55:57.234353Z",
     "shell.execute_reply.started": "2025-01-31T02:55:56.73509Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "submission = pd.read_csv(\"./input/playground-series-s5e1/sample_submission.csv\")\n",
    "lb_best = pd.read_csv(\"./input/pgs501-model-2-additional-country-doy-factor/submission.csv\")\n",
    "tf_best = pd.read_csv(\"./input/transformer-starter-lb-0-052/submission_v1.csv\")\n",
    "lb_blend = pd.read_csv(\"./input/pgs501-model-2-additional-country-doy-factor/blend.csv\")\n",
    "lb1_best = pd.read_csv(\"./input/pgs501-model-1-time-series-decomposition/submission.csv\")\n",
    "lb1_blend = pd.read_csv(\"./input/pgs501-model-1-time-series-decomposition/blend.csv\")\n",
    "hc_best = pd.read_csv(\"./input/stricker-sales-hill-climbing-for-ensembling/submission.csv\")\n",
    "pg_best = pd.read_csv(\"./input/stricker-sales-pgs501-model-1/submission.csv\")\n",
    "so_best = pd.read_csv(\"./input/stricker-sales-solution/submission.csv\")\n",
    "gc.collect()\n",
    "\n",
    "#submission[\"num_sold\"] = (test_sub_df[\"num_sold\"]*0.2403 + (tf_best.num_sold*0.5+lb_blend.num_sold*0.5)*0.2587 + hc_best.num_sold*0.5102).round()\n",
    "submission[\"num_sold\"] = so_best.num_sold.round()\n",
    "display(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T02:55:57.237462Z",
     "iopub.status.busy": "2025-01-31T02:55:57.237019Z",
     "iopub.status.idle": "2025-01-31T02:55:57.434318Z",
     "shell.execute_reply": "2025-01-31T02:55:57.4331Z",
     "shell.execute_reply.started": "2025-01-31T02:55:57.237422Z"
    }
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10652996,
     "sourceId": 85723,
     "sourceType": "competition"
    },
    {
     "datasetId": 2007861,
     "sourceId": 3325325,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6571992,
     "sourceId": 10615200,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6572038,
     "sourceId": 10615260,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6578043,
     "sourceId": 10624059,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 218891836,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 219013464,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 219999314,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30235,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
