{"metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "language_info": {"name": "python", "version": "3.7.12", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}, "kaggle": {"accelerator": "none", "dataSources": [{"sourceId": 85723, "databundleVersionId": 10652996, "sourceType": "competition"}, {"sourceId": 3325325, "sourceType": "datasetVersion", "datasetId": 2007861}, {"sourceId": 10615200, "sourceType": "datasetVersion", "datasetId": 6571992}, {"sourceId": 10615260, "sourceType": "datasetVersion", "datasetId": 6572038}, {"sourceId": 10624059, "sourceType": "datasetVersion", "datasetId": 6578043}, {"sourceId": 218891836, "sourceType": "kernelVersion"}, {"sourceId": 219013464, "sourceType": "kernelVersion"}, {"sourceId": 219999314, "sourceType": "kernelVersion"}], "dockerImageVersionId": 30235, "isInternetEnabled": true, "language": "python", "sourceType": "notebook", "isGpuEnabled": false}}, "nbformat_minor": 4, "nbformat": 4, "cells": [{"cell_type": "markdown", "source": "<p style=\"font-size: 1.2em;\">\n    I used some techniques borrowed from these excellent notebooks, and they do a great job.\n</p>\n\n<p style=\"font-size: 1.1em;\">\n    @Chris Deotte <a href=\"https://www.kaggle.com/code/cdeotte/transformer-starter-lb-0-052\">Transformer Starter notebook</a>\n</p>\n\n<p style=\"font-size: 1.1em;\">\n    @Konstantin Dmitriev <a href=\"https://www.kaggle.com/code/kdmitrie/pg501-model-2-decomposition-country-doy-factor#4.-Blending-with-other-solutions\">this notebook PG501: Model 2. Decomposition + country-doy factor</a> \n</p>\n\n<p style=\"font-size: 1.1em;\">\n    @Spiritmilk <a href=\"https://www.kaggle.com/code/act18l/hill-climbing-for-ensembling-baseline\">Hill Climbing for ensembling baseline notebook</a>\n</p>\n\n<p style=\"font-size: 1.2em;\">\n    Thank you so much for everyone's contribution.\n</p>", "metadata": {}}, {"cell_type": "markdown", "source": "# Import library", "metadata": {}}, {"cell_type": "code", "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.linear_model import Ridge,Lasso\nfrom sklearn.metrics import mean_absolute_percentage_error\nfrom lightgbm import log_evaluation, early_stopping, LGBMRegressor\n\nsns.set_style('darkgrid')", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-31T02:55:18.25413Z", "iopub.execute_input": "2025-01-31T02:55:18.254627Z", "iopub.status.idle": "2025-01-31T02:55:19.940998Z", "shell.execute_reply.started": "2025-01-31T02:55:18.254528Z", "shell.execute_reply": "2025-01-31T02:55:19.939849Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "# Load data", "metadata": {}}, {"cell_type": "code", "source": "train_df = pd.read_csv(\"./input/playground-series-s5e1/train.csv\", parse_dates=[\"date\"])\noriginal_train_df = train_df.copy()\ntest_df = pd.read_csv(\"./input/playground-series-s5e1/test.csv\", parse_dates=[\"date\"])\nsub = pd.read_csv(\"./input/playground-series-s5e1/sample_submission.csv\")\n", "metadata": {"execution": {"iopub.status.busy": "2025-01-31T02:55:19.943462Z", "iopub.execute_input": "2025-01-31T02:55:19.943811Z", "iopub.status.idle": "2025-01-31T02:55:20.565825Z", "shell.execute_reply.started": "2025-01-31T02:55:19.943778Z", "shell.execute_reply": "2025-01-31T02:55:20.564609Z"}, "trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "# Data preprocessing", "metadata": {}}, {"cell_type": "markdown", "source": "## Import GDP data", "metadata": {}}, {"cell_type": "code", "source": "gdp_per_capita_df = pd.read_csv(\"./input/world-gdpgdp-gdp-per-capita-and-annual-growths/gdp_per_capita.csv\")\n\nyears =  [\"2010\", \"2011\", \"2012\", \"2013\", \"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020\"]\ngdp_per_capita_filtered_df = gdp_per_capita_df.loc[gdp_per_capita_df[\"Country Name\"].isin(train_df[\"country\"].unique()), [\"Country Name\"] + years].set_index(\"Country Name\")\nfor year in years:\n    gdp_per_capita_filtered_df[f\"{year}_ratio\"] = gdp_per_capita_filtered_df[year] / gdp_per_capita_filtered_df.sum()[year]\ngdp_per_capita_filtered_ratios_df = gdp_per_capita_filtered_df[[i+\"_ratio\" for i in years]]\ngdp_per_capita_filtered_ratios_df.columns = [int(i) for i in years]\ngdp_per_capita_filtered_ratios_df = gdp_per_capita_filtered_ratios_df.unstack().reset_index().rename(columns = {\"level_0\": \"year\", 0: \"ratio\", \"Country Name\": \"country\"})\ngdp_per_capita_filtered_ratios_df['year'] = pd.to_datetime(gdp_per_capita_filtered_ratios_df['year'], format='%Y')\n\n# For plotting purposes\ngdp_per_capita_filtered_ratios_df_2 = gdp_per_capita_filtered_ratios_df.copy()\ngdp_per_capita_filtered_ratios_df_2[\"year\"] = pd.to_datetime(gdp_per_capita_filtered_ratios_df_2['year'].astype(str)) + pd.offsets.YearEnd(1)\ngdp_per_capita_filtered_ratios_df = pd.concat([gdp_per_capita_filtered_ratios_df, gdp_per_capita_filtered_ratios_df_2]).reset_index()", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-31T02:55:20.567363Z", "iopub.execute_input": "2025-01-31T02:55:20.567918Z", "iopub.status.idle": "2025-01-31T02:55:20.640586Z", "shell.execute_reply.started": "2025-01-31T02:55:20.567869Z", "shell.execute_reply": "2025-01-31T02:55:20.639517Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "gdp_per_capita_filtered_ratios_df_2[\"year\"] = gdp_per_capita_filtered_ratios_df_2[\"year\"].dt.year", "metadata": {"execution": {"iopub.status.busy": "2025-01-31T02:55:20.643447Z", "iopub.execute_input": "2025-01-31T02:55:20.644339Z", "iopub.status.idle": "2025-01-31T02:55:20.650321Z", "shell.execute_reply.started": "2025-01-31T02:55:20.644293Z", "shell.execute_reply": "2025-01-31T02:55:20.649455Z"}, "trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "## Process missing values", "metadata": {}}, {"cell_type": "code", "source": "train_df_imputed = train_df.copy()\nmissing_value_ids = train_df.loc[train_df[\"num_sold\"].isna(), \"id\"].values\nprint(f\"Missing values remaining: {train_df_imputed['num_sold'].isna().sum()}\")\n\ntrain_df_imputed[\"year\"] = train_df_imputed[\"date\"].dt.year\nfor year in train_df_imputed[\"year\"].unique():\n    # Impute Time Series 1 (Canada, Discount Stickers, Holographic Goose)\n    target_ratio = gdp_per_capita_filtered_ratios_df_2.loc[(gdp_per_capita_filtered_ratios_df_2[\"year\"] == year) & (gdp_per_capita_filtered_ratios_df_2[\"country\"] == \"Norway\"), \"ratio\"].values[0] # Using Norway as should have the best precision\n    current_raito = gdp_per_capita_filtered_ratios_df_2.loc[(gdp_per_capita_filtered_ratios_df_2[\"year\"] == year) & (gdp_per_capita_filtered_ratios_df_2[\"country\"] == \"Canada\"), \"ratio\"].values[0]\n    ratio_can = current_raito / target_ratio\n    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Canada\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year), \"num_sold\"] * ratio_can).values\n    \n    # Impute Time Series 2 (Only Missing Values)\n    current_ts =  train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Canada\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year)]\n    missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Canada\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] * ratio_can).values\n\n    # Impute Time Series 3 (Only Missing Values)\n    current_ts =  train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Canada\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year)]\n    missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Canada\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] * ratio_can).values\n    \n    # Impute Time Series 4 (Kenya, Discount Stickers, Holographic Goose)\n    current_raito = gdp_per_capita_filtered_ratios_df_2.loc[(gdp_per_capita_filtered_ratios_df_2[\"year\"] == year) & (gdp_per_capita_filtered_ratios_df_2[\"country\"] == \"Kenya\"), \"ratio\"].values[0]\n    ratio_ken = current_raito / target_ratio\n    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Holographic Goose\")& (train_df_imputed[\"year\"] == year), \"num_sold\"] * ratio_ken).values\n\n    # Impute Time Series 5 (Only Missing Values)\n    current_ts = train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year)]\n    missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] * ratio_ken).values\n\n    # Impute Time Series 6 (Only Missing Values)\n    current_ts = train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year)]\n    missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] * ratio_ken).values\n\n    # Impute Time Series 7 (Only Missing Values)\n    current_ts = train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Kerneler\") & (train_df_imputed[\"year\"] == year)]\n    missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Kerneler\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Kerneler\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] * ratio_ken).values\n    \nprint(f\"Missing values remaining: {train_df_imputed['num_sold'].isna().sum()}\")", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-31T02:55:20.653056Z", "iopub.execute_input": "2025-01-31T02:55:20.653505Z", "iopub.status.idle": "2025-01-31T02:55:27.703345Z", "shell.execute_reply.started": "2025-01-31T02:55:20.653464Z", "shell.execute_reply": "2025-01-31T02:55:27.702206Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "missing_rows = train_df_imputed.loc[train_df_imputed[\"num_sold\"].isna()]\ndisplay(missing_rows)\ntrain_df_imputed.loc[train_df_imputed[\"id\"] == 23719, \"num_sold\"] = 4\ntrain_df_imputed.loc[train_df_imputed[\"id\"] == 207003, \"num_sold\"] = 195\n\nprint(f\"Missing values remaining: {train_df_imputed['num_sold'].isna().sum()}\")", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-31T02:55:27.705082Z", "iopub.execute_input": "2025-01-31T02:55:27.705433Z", "iopub.status.idle": "2025-01-31T02:55:27.729238Z", "shell.execute_reply.started": "2025-01-31T02:55:27.705376Z", "shell.execute_reply": "2025-01-31T02:55:27.728062Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "## store weights", "metadata": {}}, {"cell_type": "code", "source": "store_weights = train_df_imputed.groupby(\"store\")[\"num_sold\"].sum()/train_df_imputed[\"num_sold\"].sum()\nstore_weights", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-31T02:55:27.730807Z", "iopub.execute_input": "2025-01-31T02:55:27.731218Z", "iopub.status.idle": "2025-01-31T02:55:27.767537Z", "shell.execute_reply.started": "2025-01-31T02:55:27.731174Z", "shell.execute_reply": "2025-01-31T02:55:27.76638Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "## The proportion of product in daily sales", "metadata": {}}, {"cell_type": "code", "source": "product_df = train_df_imputed.groupby([\"date\",\"product\"])[\"num_sold\"].sum().reset_index()\nproduct_ratio_df = product_df.pivot(index=\"date\", columns=\"product\", values=\"num_sold\")\nproduct_ratio_df = product_ratio_df.apply(lambda x: x/x.sum(),axis=1)\nproduct_ratio_df = product_ratio_df.stack().rename(\"ratios\").reset_index()\nproduct_ratio_df.head(4)", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-31T02:55:27.768864Z", "iopub.execute_input": "2025-01-31T02:55:27.769201Z", "iopub.status.idle": "2025-01-31T02:55:28.494695Z", "shell.execute_reply.started": "2025-01-31T02:55:27.769173Z", "shell.execute_reply": "2025-01-31T02:55:28.493489Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "train_df_imputed.head()", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-31T02:55:28.496052Z", "iopub.execute_input": "2025-01-31T02:55:28.496353Z", "iopub.status.idle": "2025-01-31T02:55:28.510305Z", "shell.execute_reply.started": "2025-01-31T02:55:28.496325Z", "shell.execute_reply": "2025-01-31T02:55:28.509144Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "## Group the dates and calculate the total sales num_sold per day", "metadata": {}}, {"cell_type": "code", "source": "original_train_df_imputed = train_df_imputed.copy()\ntrain_df_imputed = train_df_imputed.groupby([\"date\"])[\"num_sold\"].sum().reset_index()", "metadata": {"execution": {"iopub.status.busy": "2025-01-31T02:55:28.51495Z", "iopub.execute_input": "2025-01-31T02:55:28.515309Z", "iopub.status.idle": "2025-01-31T02:55:28.553886Z", "shell.execute_reply.started": "2025-01-31T02:55:28.515272Z", "shell.execute_reply": "2025-01-31T02:55:28.552904Z"}, "trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "train_df_imputed.head()", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-31T02:55:28.55511Z", "iopub.execute_input": "2025-01-31T02:55:28.555526Z", "iopub.status.idle": "2025-01-31T02:55:28.566833Z", "shell.execute_reply.started": "2025-01-31T02:55:28.555379Z", "shell.execute_reply": "2025-01-31T02:55:28.565473Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "## The weekly effect was normalized", "metadata": {}}, {"cell_type": "code", "source": "train_df_imputed[\"day_of_week\"] = train_df_imputed[\"date\"].dt.dayofweek\nday_of_week_ratio = (train_df_imputed.groupby(\"day_of_week\")[\"num_sold\"].mean() / train_df_imputed.groupby(\"day_of_week\")[\"num_sold\"].mean().mean()).rename(\"day_of_week_ratios\")\ndisplay(day_of_week_ratio)\ntrain_df_imputed = pd.merge(train_df_imputed, day_of_week_ratio, how=\"left\", on=\"day_of_week\")\ntrain_df_imputed[\"num_sold\"] = train_df_imputed[\"num_sold\"] / train_df_imputed[\"day_of_week_ratios\"]\ntrain_df_imputed = train_df_imputed.drop(\"day_of_week_ratios\",axis=1)# we don't need it in training.", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-31T02:55:28.568511Z", "iopub.execute_input": "2025-01-31T02:55:28.568845Z", "iopub.status.idle": "2025-01-31T02:55:28.594536Z", "shell.execute_reply.started": "2025-01-31T02:55:28.568816Z", "shell.execute_reply": "2025-01-31T02:55:28.593443Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "## Fetch date column", "metadata": {}}, {"cell_type": "code", "source": "test_total_sales_df = test_df.groupby([\"date\"])[\"id\"].first().reset_index().drop(columns=\"id\")\ntest_total_sales_dates = test_total_sales_df[[\"date\"]]", "metadata": {"execution": {"iopub.status.busy": "2025-01-31T02:55:28.596243Z", "iopub.execute_input": "2025-01-31T02:55:28.596594Z", "iopub.status.idle": "2025-01-31T02:55:28.60747Z", "shell.execute_reply.started": "2025-01-31T02:55:28.596565Z", "shell.execute_reply": "2025-01-31T02:55:28.606475Z"}, "trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "# Feature engineering", "metadata": {}}, {"cell_type": "code", "source": "def feature_engineer(df):\n    new_df = df.copy()\n    new_df[\"month\"] = df[\"date\"].dt.month\n    new_df[\"month_sin\"] = np.sin(new_df['month'] * (2 * np.pi / 12))\n    new_df[\"month_cos\"] = np.cos(new_df['month'] * (2 * np.pi / 12))\n    new_df[\"day_of_week\"] = df[\"date\"].dt.dayofweek\n    new_df[\"day_of_week\"] = new_df[\"day_of_week\"].apply(lambda x: 0 if x<=3 else(1 if x==4 else (2 if x==5 else (3))))    \n    new_df[\"day_of_year\"] = df['date'].apply(\n        lambda x: x.timetuple().tm_yday if not (x.is_leap_year and x.month > 2) else x.timetuple().tm_yday - 1\n    )\n\n    new_df['day_sin4'] = np.sin(new_df['day_of_year'] * (8 * np.pi /  365.0))\n    new_df['day_cos4'] = np.cos(new_df['day_of_year'] * (8 * np.pi /  365.0))\n    new_df['day_sin3'] = np.sin(new_df['day_of_year'] * (6 * np.pi /  365.0))\n    new_df['day_cos3'] = np.cos(new_df['day_of_year'] * (6 * np.pi /  365.0))\n    new_df['day_sin2'] = np.sin(new_df['day_of_year'] * (4 * np.pi /  365.0))\n    new_df['day_cos2'] = np.cos(new_df['day_of_year'] * (4 * np.pi /  365.0))\n    new_df['day_sin'] = np.sin(new_df['day_of_year'] * (2 * np.pi /  365.0))\n    new_df['day_cos'] = np.cos(new_df['day_of_year'] * (2 * np.pi /  365.0)) \n    new_df['day_sin_0.5'] = np.sin(new_df['day_of_year'] * (1 * np.pi /  365.0))\n    new_df['day_cos_0.5'] = np.cos(new_df['day_of_year'] * (1 * np.pi /  365.0))    \n    new_df[\"important_dates\"] = new_df[\"day_of_year\"].apply(lambda x: x if x in [1,2,3,4,5,6,7,8,9,10,99, 100, 101, 125,126,355,256,357,358,359,360,361,362,363,364,365] else 0)\n    \n    new_df = new_df.drop(columns=[\"date\",\"month\",\"day_of_year\"])\n    new_df = pd.get_dummies(new_df, columns = [\"important_dates\",\"day_of_week\"], drop_first=True)\n    \n    return new_df", "metadata": {"execution": {"iopub.status.busy": "2025-01-31T02:55:28.609022Z", "iopub.execute_input": "2025-01-31T02:55:28.60936Z", "iopub.status.idle": "2025-01-31T02:55:28.623695Z", "shell.execute_reply.started": "2025-01-31T02:55:28.609332Z", "shell.execute_reply": "2025-01-31T02:55:28.622369Z"}, "trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "train_total_sales_df = feature_engineer(train_df_imputed)\ntest_total_sales_df = feature_engineer(test_total_sales_df)", "metadata": {"execution": {"iopub.status.busy": "2025-01-31T02:55:28.62534Z", "iopub.execute_input": "2025-01-31T02:55:28.62587Z", "iopub.status.idle": "2025-01-31T02:55:28.694808Z", "shell.execute_reply.started": "2025-01-31T02:55:28.625828Z", "shell.execute_reply": "2025-01-31T02:55:28.693866Z"}, "trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "## import holidays", "metadata": {}}, {"cell_type": "code", "source": "import holidays\ntrain_df_tmp = train_df.copy()\ntest_df_tmp = test_df.copy()\nalpha2 = dict(zip(np.sort(train_df.country.unique()), ['CA', 'FI', 'IT', 'KE', 'NO', 'SG']))\nh = {c: holidays.country_holidays(a, years=range(2010, 2020)) for c, a in alpha2.items()}\ntrain_df_tmp['is_holiday'] = 0\ntest_df_tmp['is_holiday'] = 0\nfor c in alpha2:\n    train_df_tmp.loc[train_df_tmp.country==c, 'is_holiday'] = train_df_tmp.date.isin(h[c]).astype(int)\n    test_df_tmp.loc[test_df_tmp.country==c, 'is_holiday'] = test_df_tmp.date.isin(h[c]).astype(int)\n\n\ntrain_total_sales_df['is_holiday'] = (train_df_tmp.groupby([\"date\"])[\"is_holiday\"].sum().reset_index())[\"is_holiday\"]\ntest_total_sales_df['is_holiday'] = (test_df_tmp.groupby([\"date\"])[\"is_holiday\"].sum().reset_index())[\"is_holiday\"]", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-31T02:55:28.696255Z", "iopub.execute_input": "2025-01-31T02:55:28.696733Z", "iopub.status.idle": "2025-01-31T02:55:29.11598Z", "shell.execute_reply.started": "2025-01-31T02:55:28.696692Z", "shell.execute_reply": "2025-01-31T02:55:29.114898Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "y = train_total_sales_df[\"num_sold\"]\nX = train_total_sales_df.drop(columns=\"num_sold\")\nX_test = test_total_sales_df", "metadata": {"execution": {"iopub.status.busy": "2025-01-31T02:55:29.117415Z", "iopub.execute_input": "2025-01-31T02:55:29.117846Z", "iopub.status.idle": "2025-01-31T02:55:29.125161Z", "shell.execute_reply.started": "2025-01-31T02:55:29.117804Z", "shell.execute_reply": "2025-01-31T02:55:29.123946Z"}, "trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "X.head()", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-31T02:55:29.12677Z", "iopub.execute_input": "2025-01-31T02:55:29.12719Z", "iopub.status.idle": "2025-01-31T02:55:29.155382Z", "shell.execute_reply.started": "2025-01-31T02:55:29.127152Z", "shell.execute_reply": "2025-01-31T02:55:29.154007Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "## hill climbing", "metadata": {}}, {"cell_type": "code", "source": "import numpy as np\n\nclass LinearModel1:\n    def fit(self, X, y=None):\n        pass\n\n    def predict(self, X):\n        return self.nonlinear(X)\n\n    def nonlinear(self, X):\n        y = (\n            X.important_dates_1 * 14514.153142903406 +\n            X.important_dates_6 +\n            X.important_dates_99 -\n            X.important_dates_359 +\n            X.important_dates_363 * 23245.947607750375 +\n            64795.970182750174 -\n            ((X.important_dates_362 + np.sqrt(X.important_dates_364 + X.important_dates_365)) *\n             ((X.important_dates_2 - X.important_dates_10) * (X.important_dates_101 + X.month_sin) +\n              X.important_dates_362 - 19142.637752625015))\n        )\n        return y\n\nclass LinearModel2:\n    def fit(self, X, y=None):\n        pass\n\n    def predict(self, X):\n        return self.nonlinear(X)\n\n    def nonlinear(self, X):\n        y = X.month_sin*210.148+\\\n            (X.important_dates_1+X.important_dates_362+X.important_dates_363+X.important_dates_364+X.important_dates_365)*19472.7071+\\\n            X.important_dates_4*X.month_sin+\\\n            65141.2073\n            \n        return y\n", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-31T02:55:29.157111Z", "iopub.execute_input": "2025-01-31T02:55:29.157633Z", "iopub.status.idle": "2025-01-31T02:55:29.169286Z", "shell.execute_reply.started": "2025-01-31T02:55:29.157582Z", "shell.execute_reply": "2025-01-31T02:55:29.168219Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "# Model", "metadata": {}}, {"cell_type": "markdown", "source": "## Multi-model", "metadata": {}}, {"cell_type": "code", "source": "regressors = {\n    \"Ridge\" : Ridge(tol=1e-2, max_iter=1000000, random_state=0),\n    \"Lasso\" : Lasso(tol=1e-2, max_iter=1000000, random_state=0),\n    #\"nonlinear1\" : LinearModel1(),\n    #\"nonlinear2\" : LinearModel2(),\n    \"LGBM1\": LGBMRegressor(**{'objective': 'regression_l2',\n                               'metric': 'mape', \n                               'max_depth': 7,\n                               'num_leaves': 123, \n                               'min_child_samples': 21,\n                               'min_child_weight': 24,\n                               'colsample_bytree': 0.3641261996760593, \n                               'reg_alpha': 0.03632800166349373, \n                               'reg_lambda': 0.5287861861476272,\n                               'random_state': 42,\n                               'early_stopping_round':200,\n                               'verbose': -1,\n                               'boosting_type': 'gbdt',\n                               'n_estimators': 3000,\n                               'learning_rate': 0.01,\n                               }),\n    \"LGBM2\": LGBMRegressor(**{'objective': 'regression_l2',\n                               'metric': 'mape',\n                               'max_depth': 6,\n                               'num_leaves': 502,\n                               'min_child_samples': 23,\n                               'min_child_weight': 18, \n                               'colsample_bytree': 0.4714820876493163, \n                               'reg_alpha': 0.054972003081022576, \n                               'reg_lambda': 0.5774608955362155,\n                               'random_state': 42,\n                               'early_stopping_round': 200,\n                               'verbose': -1,\n                               'boosting_type': 'goss',\n                               'n_estimators': 3000,\n                               'learning_rate': 0.01,\n                              }),\n    \"LGBM3\": LGBMRegressor(**{'objective': 'regression_l2', \n                               'metric': 'mape',\n                               'max_depth': 14,\n                               'num_leaves': 279,\n                               'min_child_samples': 7,\n                               'min_child_weight': 24, \n                               'colsample_bytree': 0.43218993309765835,\n                               'reg_alpha': 0.42757392987472964,\n                               'reg_lambda': 0.9039762787446107,\n                               'random_state': 42,\n                               'early_stopping_round': 200,\n                               'verbose': -1,\n                               'boosting_type': 'goss',\n                               'n_estimators': 3000,\n                               'learning_rate': 0.01,\n                               }),\n   \n}", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-31T02:55:29.171415Z", "iopub.execute_input": "2025-01-31T02:55:29.171873Z", "iopub.status.idle": "2025-01-31T02:55:29.184991Z", "shell.execute_reply.started": "2025-01-31T02:55:29.17183Z", "shell.execute_reply": "2025-01-31T02:55:29.183857Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "## Cross-validation trains, validates, and generates predictions", "metadata": {}}, {"cell_type": "code", "source": "import time\nfrom sklearn.model_selection import KFold\nn_folds = 5\n\nfor key, reg in regressors.items():\n    test_preds = np.zeros(len(X_test))\n    oof_full = y.copy()\n\n    start = time.time()\n\n    cv = KFold(n_splits=n_folds, shuffle=False)\n    score=0\n    for fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n        X_train, X_valid = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_valid = y.iloc[train_idx], y.iloc[val_idx]\n        start = time.time()\n        if \"LGBM\" in key:\n            reg.fit(X_train, y_train,\n                   eval_set = [(X_valid, y_valid)], \n                                       callbacks = [log_evaluation(0),\n                                                    early_stopping(200, verbose = False)\n                                                   ])\n            \n        else:\n            reg.fit(X_train, y_train)\n        oof_preds = reg.predict(X_valid)\n        score += mean_absolute_percentage_error(y_valid, oof_preds)/n_folds\n        oof_full[val_idx] = oof_preds\n        \n        test_preds += reg.predict(X_test)/n_folds\n    \n    # Stop timer\n    stop = time.time()\n    \n    print('Model:', key)\n    print('Average validation MAPE:', score)\n    print('Training time (mins):', np.round((stop - start)/60,2))\n    print('')\n    \n\n    oof_full.to_csv(f\"{key}_oof_preds.csv\", index=False)\n    ss = pd.DataFrame()\n    ss[\"num_sold\"] = test_preds\n    ss.to_csv(f\"{key}_test_preds.csv\", index=False)\n", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-31T02:55:29.186558Z", "iopub.execute_input": "2025-01-31T02:55:29.186994Z", "iopub.status.idle": "2025-01-31T02:55:40.655834Z", "shell.execute_reply.started": "2025-01-31T02:55:29.186952Z", "shell.execute_reply": "2025-01-31T02:55:40.654995Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "## Combine oof_preds and test_preds into a single DataFrame", "metadata": {}}, {"cell_type": "code", "source": "oof_df = pd.DataFrame(index=np.arange(len(y)))\nfor i in regressors.keys():\n    df = pd.read_csv(f\"/kaggle/working/{i}_oof_preds.csv\")\n    df.rename(columns={\"num_sold\": i}, inplace=True)\n    oof_df = pd.concat([oof_df,df], axis=1)\n    \n# Join test preds\ntest_preds = pd.DataFrame(index=np.arange(len(X_test)))\nfor i in regressors.keys():\n    df = pd.read_csv(f\"/kaggle/working/{i}_test_preds.csv\")\n    df.rename(columns={\"num_sold\": i}, inplace=True)\n    test_preds = pd.concat([test_preds,df], axis=1)\n    \noof_df.head(3)", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-31T02:55:40.656775Z", "iopub.execute_input": "2025-01-31T02:55:40.657107Z", "iopub.status.idle": "2025-01-31T02:55:40.700198Z", "shell.execute_reply.started": "2025-01-31T02:55:40.657073Z", "shell.execute_reply": "2025-01-31T02:55:40.6991Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "## Calculation model score", "metadata": {}}, {"cell_type": "code", "source": "# Evaluate oof preds\nscores = {}\nfor col in oof_df.columns:\n    scores[col] = mean_absolute_percentage_error(y, oof_df[col])\n\n# Sort scores\n    scores = {k: v for k, v in sorted(scores.items(), key=lambda item: item[1], reverse=False)}\n\n# Sort oof_df and test_preds\noof_df = oof_df[list(scores.keys())]\ntest_preds = test_preds[list(scores.keys())]\n\nscores", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-31T02:55:40.701695Z", "iopub.execute_input": "2025-01-31T02:55:40.70212Z", "iopub.status.idle": "2025-01-31T02:55:40.719682Z", "shell.execute_reply.started": "2025-01-31T02:55:40.702078Z", "shell.execute_reply": "2025-01-31T02:55:40.718344Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "# Optimization treatment", "metadata": {}}, {"cell_type": "markdown", "source": "## Model weight optimization based on Hill Climbing", "metadata": {}}, {"cell_type": "code", "source": "# Initialise\nSTOP = False\ncurrent_best_ensemble = oof_df.iloc[:,0]\ncurrent_best_test_preds = test_preds.iloc[:,0]\nMODELS = oof_df.iloc[:,1:]\nweight_range = np.arange(-1,1,0.001)  \nhistory = [mean_absolute_percentage_error(y, current_best_ensemble)]\ni=0\n\n# Hill climbing\nwhile not STOP:\n    i+=1\n    potential_new_best_cv_score = mean_absolute_percentage_error(y, current_best_ensemble)\n    k_best, wgt_best = None, None\n    for k in MODELS:\n        for wgt in weight_range:\n            potential_ensemble = (1-wgt) * current_best_ensemble + wgt * MODELS[k]\n            cv_score = mean_absolute_percentage_error(y, potential_ensemble)\n            if cv_score < potential_new_best_cv_score:\n                potential_new_best_cv_score = cv_score\n                k_best, wgt_best = k, wgt\n            \n    if k_best is not None:\n        current_best_ensemble = (1-wgt_best) * current_best_ensemble + wgt_best * MODELS[k_best]\n        current_best_test_preds = (1-wgt_best) * current_best_test_preds + wgt_best * test_preds[k_best]\n        MODELS.drop(k_best, axis=1, inplace=True)\n        if MODELS.shape[1]==0:\n            STOP = True\n        print(f'Iteration: {i}, Model added: {k_best}, Best weight: {wgt_best:}, Best MAPE: {potential_new_best_cv_score:.5f}')\n        history.append(potential_new_best_cv_score)\n    else:\n        STOP = True", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-31T02:55:40.721331Z", "iopub.execute_input": "2025-01-31T02:55:40.721795Z", "iopub.status.idle": "2025-01-31T02:55:56.532233Z", "shell.execute_reply.started": "2025-01-31T02:55:40.721765Z", "shell.execute_reply": "2025-01-31T02:55:56.531097Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "## Weighted average of predicted results", "metadata": {}}, {"cell_type": "code", "source": "test_total_sales_dates[\"num_sold\"] = current_best_test_preds*0.5+LinearModel2().predict(X_test)*0.5", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-31T02:55:56.53384Z", "iopub.execute_input": "2025-01-31T02:55:56.534145Z", "iopub.status.idle": "2025-01-31T02:55:56.543603Z", "shell.execute_reply.started": "2025-01-31T02:55:56.534117Z", "shell.execute_reply": "2025-01-31T02:55:56.54254Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "## The date offset method generates the predicted product proportions", "metadata": {}}, {"cell_type": "code", "source": "product_ratio_2017_df = product_ratio_df.loc[product_ratio_df[\"date\"].dt.year == 2015].copy()\nproduct_ratio_2018_df = product_ratio_df.loc[product_ratio_df[\"date\"].dt.year == 2016].copy()\nproduct_ratio_2019_df = product_ratio_df.loc[product_ratio_df[\"date\"].dt.year == 2015].copy()\n\nproduct_ratio_2017_df[\"date\"] = product_ratio_2017_df[\"date\"] + pd.DateOffset(years=2)\nproduct_ratio_2018_df[\"date\"] = product_ratio_2018_df[\"date\"] + pd.DateOffset(years=2)\nproduct_ratio_2019_df[\"date\"] =  product_ratio_2019_df[\"date\"] + pd.DateOffset(years=4)\n\nforecasted_ratios_df = pd.concat([product_ratio_2017_df, product_ratio_2018_df, product_ratio_2019_df])", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-31T02:55:56.544973Z", "iopub.execute_input": "2025-01-31T02:55:56.545336Z", "iopub.status.idle": "2025-01-31T02:55:56.569069Z", "shell.execute_reply.started": "2025-01-31T02:55:56.545306Z", "shell.execute_reply": "2025-01-31T02:55:56.567895Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "## The data of the test set is decomposed and predicted based on multiple weights", "metadata": {}}, {"cell_type": "code", "source": "store_weights_df = store_weights.reset_index()\ntest_sub_df = pd.merge(test_df, test_total_sales_dates, how=\"left\", on=\"date\")\ntest_sub_df = test_sub_df.rename(columns = {\"num_sold\":\"day_num_sold\"})\n# Adding in the product ratios\ntest_sub_df = pd.merge(test_sub_df, store_weights_df, how=\"left\", on=\"store\")\ntest_sub_df = test_sub_df.rename(columns = {\"num_sold\":\"store_ratio\"})\n# Adding in the country ratios\ntest_sub_df[\"year\"] = test_sub_df[\"date\"].dt.year\ntest_sub_df = pd.merge(test_sub_df, gdp_per_capita_filtered_ratios_df_2, how=\"left\", on=[\"year\", \"country\"])\ntest_sub_df = test_sub_df.rename(columns = {\"ratio\":\"country_ratio\"})\n# Adding in the product ratio\ntest_sub_df = pd.merge(test_sub_df, forecasted_ratios_df, how=\"left\", on=[\"date\", \"product\"])\ntest_sub_df = test_sub_df.rename(columns = {\"ratios\":\"product_ratio\"})\n\n# Adding in the week ratio\ntest_sub_df[\"day_of_week\"] = test_sub_df[\"date\"].dt.dayofweek\ntest_sub_df = pd.merge(test_sub_df, day_of_week_ratio.reset_index(), how=\"left\", on=\"day_of_week\")\n\n\n# Disaggregating the forecast\ntest_sub_df.loc[test_sub_df['country'] == 'Kenya', 'country_ratio'] -= 0.0007/2\n\ntest_sub_df[\"num_sold\"] = test_sub_df[\"day_num_sold\"] *test_sub_df[\"day_of_week_ratios\"]* test_sub_df[\"store_ratio\"] * test_sub_df[\"country_ratio\"] * test_sub_df[\"product_ratio\"]\n#test_sub_df[\"num_sold\"] = test_sub_df[\"num_sold\"].round()\ndisplay(test_sub_df.head(2))", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-31T02:55:56.570309Z", "iopub.execute_input": "2025-01-31T02:55:56.570722Z", "iopub.status.idle": "2025-01-31T02:55:56.729607Z", "shell.execute_reply.started": "2025-01-31T02:55:56.570684Z", "shell.execute_reply": "2025-01-31T02:55:56.728443Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "# Blending with other solutions", "metadata": {}}, {"cell_type": "code", "source": "import gc\nsubmission = pd.read_csv(\"./input/playground-series-s5e1/sample_submission.csv\")\nlb_best = pd.read_csv(\"./input/pgs501-model-2-additional-country-doy-factor/submission.csv\")\ntf_best = pd.read_csv(\"./input/transformer-starter-lb-0-052/submission_v1.csv\")\nlb_blend = pd.read_csv(\"./input/pgs501-model-2-additional-country-doy-factor/blend.csv\")\nlb1_best = pd.read_csv(\"./input/pgs501-model-1-time-series-decomposition/submission.csv\")\nlb1_blend = pd.read_csv(\"./input/pgs501-model-1-time-series-decomposition/blend.csv\")\nhc_best = pd.read_csv(\"./input/stricker-sales-hill-climbing-for-ensembling/submission.csv\")\npg_best = pd.read_csv(\"./input/stricker-sales-pgs501-model-1/submission.csv\")\nso_best = pd.read_csv(\"./input/stricker-sales-solution/submission.csv\")\ngc.collect()\n\n#submission[\"num_sold\"] = (test_sub_df[\"num_sold\"]*0.2403 + (tf_best.num_sold*0.5+lb_blend.num_sold*0.5)*0.2587 + hc_best.num_sold*0.5102).round()\nsubmission[\"num_sold\"] = so_best.num_sold.round()\ndisplay(submission.head())", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-31T02:55:56.734802Z", "iopub.execute_input": "2025-01-31T02:55:56.735117Z", "iopub.status.idle": "2025-01-31T02:55:57.235457Z", "shell.execute_reply.started": "2025-01-31T02:55:56.73509Z", "shell.execute_reply": "2025-01-31T02:55:57.234353Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "submission.to_csv('submission.csv', index = False)", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-01-31T02:55:57.237019Z", "iopub.execute_input": "2025-01-31T02:55:57.237462Z", "iopub.status.idle": "2025-01-31T02:55:57.434318Z", "shell.execute_reply.started": "2025-01-31T02:55:57.237422Z", "shell.execute_reply": "2025-01-31T02:55:57.4331Z"}}, "outputs": [], "execution_count": null}]}